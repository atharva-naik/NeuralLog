{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdepLog Neural-Logical Inference System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw import TreeWidget\n",
    "from nltk.draw.util import CanvasFrame\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def jupyter_draw_nltk_tree(tree):\n",
    "    cf = CanvasFrame()\n",
    "    tc = TreeWidget(cf.canvas(), tree)\n",
    "    tc['node_font'] = 'arial 14 bold'\n",
    "    tc['leaf_font'] = 'ar|ial 14'\n",
    "    tc['node_color'] = '#005990'\n",
    "    tc['leaf_color'] = '#3F8F57'\n",
    "    tc['line_color'] = '#175252'\n",
    "    cf.add_widget(tc, 20, 20)\n",
    "    os.system('rm -rf ../data/tree.png')\n",
    "    os.system('rm -rf ../data/tree.ps')\n",
    "    cf.print_to_file('../data/tree.ps')\n",
    "    cf.destroy()\n",
    "    os.system('convert ../data/tree.ps ../data/tree.png')\n",
    "    display(Image(filename='../data/tree.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT Model for Pharaphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load Alignment Model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "roberta_MRPC = \"textattack/roberta-base-MRPC\"\n",
    "bert_MRPC = \"bert-base-cased-finetuned-mrpc\"\n",
    "albert_MRPC = \"textattack/albert-base-v2-MRPC\"\n",
    "\n",
    "paraphraseTokenizer = AutoTokenizer.from_pretrained(albert_MRPC)  \n",
    "paraphraseModel = AutoModelForSequenceClassification.from_pretrained(albert_MRPC)\n",
    "paraphraseModel.to('cuda')\n",
    "print(\"Load Alignment Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_STS = \"bert-base-uncased-STS-B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. UD Parser and RoBERTa Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-03-30 23:41:08 INFO: Loading these models for language: en (English):\n",
      "========================================\n",
      "| Processor | Package                  |\n",
      "----------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt  |\n",
      "| pos       | ../model/en/pos/ewt.pt   |\n",
      "| lemma     | ../model/en/lemma/gum.pt |\n",
      "| depparse  | ../model/e...rse/gum.pt  |\n",
      "========================================\n",
      "\n",
      "2021-03-30 23:41:08 INFO: Use device: gpu\n",
      "2021-03-30 23:41:08 INFO: Loading: tokenize\n",
      "2021-03-30 23:41:09 INFO: Loading: pos\n",
      "2021-03-30 23:41:10 INFO: Loading: lemma\n",
      "2021-03-30 23:41:10 INFO: Loading: depparse\n",
      "2021-03-30 23:41:11 INFO: Done loading processors!\n",
      "2021-03-30 23:41:11 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt |\n",
      "=======================================\n",
      "\n",
      "2021-03-30 23:41:11 INFO: Use device: cpu\n",
      "2021-03-30 23:41:11 INFO: Loading: tokenize\n",
      "2021-03-30 23:41:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from wordnet import *\n",
    "from copy import deepcopy\n",
    "from Udep2Mono.util import det_mark, det_type\n",
    "from Udep2Mono.util import btree2list\n",
    "from Udep2Mono.dependency_parse import tokenizer\n",
    "from Udep2Mono.dependency_parse import dependency_parse\n",
    "from Udep2Mono.binarization import BinaryDependencyTree\n",
    "from Udep2Mono.polarization import PolarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentenceTransformer = SentenceTransformer(\"stsb-bert-large\")\n",
    "sentenceTransformer.to('cuda')\n",
    "from nltk.corpus import stopwords\n",
    "ign_words = dict()\n",
    "for word in stopwords.words('english'):\n",
    "    ign_words[word] = 1\n",
    "\n",
    "def inference_sts(seq1s, seq2s, dist=False):\n",
    "    embeddings1 = sentenceTransformer.encode(seq1s, convert_to_tensor=True)\n",
    "    embeddings2 = sentenceTransformer.encode(seq2s, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    distance = torch.dist(embeddings1, embeddings2, p=2)\n",
    "    if dist:\n",
    "        return distance\n",
    "    return cosine_scores[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phrasal Monotonicity Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import pluralize, singularize\n",
    "from copy import copy\n",
    "import re\n",
    "import torch\n",
    "\n",
    "class PhrasalGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.annotated = None\n",
    "        self.original = None\n",
    "        self.kb = {}\n",
    "        self.hypothesis = \"\"\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.mod_at_left = [\n",
    "            \"advmod\", \"amod\", \"advmod:count\", \n",
    "            \"acl:relcl\", \"obl\", 'obl:npmod', \"det\",\n",
    "            \"obl:tmod\", \"nmod\", \"nmod:npmod\", \n",
    "            \"nmod:poss\", \"nmod:tmod\", \"obl:npmod\",\n",
    "            \"acl\", \"advcl\", \"xcomp\", \"ccomp\", \n",
    "            'compound:ptr']\n",
    "        self.mod_at_right = [\"appos\"] #\"obj\"\n",
    "        self.mod_symmetric = [\"conj\", \"compound\"]\n",
    "        self.mod_special = [\"nsubj\"]\n",
    "        self.implicative = {\n",
    "            \"watching\": 1\n",
    "        }\n",
    "        \n",
    "        '''  \n",
    "            \"cop\": self.generate_inherite, \n",
    "            \"expl\": self.generate_expl,\n",
    "            \"nummod\": self.generate_nummod,\n",
    "        '''\n",
    "\n",
    "    def deptree_generate(self, tree, annotated, original):\n",
    "        self.stop_critarion = False\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.deptree = tree.copy()\n",
    "        self.original = original  \n",
    "        self.annotated = deepcopy(annotated)\n",
    "        self.sentence = original\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if self.stop_critarion:\n",
    "            return\n",
    "        if not tree.is_tree:\n",
    "            self.generate_default(tree)\n",
    "        else:\n",
    "            generation = self.get_generation_type(tree)\n",
    "            #print(generation, tree.val)\n",
    "            generation(tree)\n",
    "    def removeGroup(self, tree):\n",
    "        if(tree.val == \"nmod\"):\n",
    "            if(tree.right.is_tree and tree.right.left.val.lower() == \"a\"):\n",
    "                if(tree.left.is_tree and tree.left.left.val.lower() == \"of\"):\n",
    "                    noun = tree.left.right\n",
    "                    group = tree.right.right\n",
    "                    self.delete_modifier(tree, noun)\n",
    "                    self.delete_modifier(tree, group)\n",
    "                    return True\n",
    "        return False\n",
    "    def get_generation_type(self, tree):\n",
    "        if tree.val in self.mod_special:\n",
    "            return self.generate_special\n",
    "\n",
    "        disjunction = False\n",
    "        if tree.val == \"conj\":\n",
    "            disjunction |= self.search_dependency('or', tree.left)\n",
    "            disjunction |= self.search_dependency('and', tree.left)\n",
    "        \n",
    "        left_mod = tree.left.mark == \"+\"\n",
    "        left_mod = left_mod or tree.left.mark == \"=\" or disjunction\n",
    "        left_mod = left_mod and tree.val in self.mod_at_left\n",
    "\n",
    "        right_mod = tree.right.mark == \"+\" or tree.right.mark == \"=\" or disjunction \n",
    "        right_mod = right_mod and tree.val in self.mod_at_right\n",
    "\n",
    "        sym_mod = tree.val in self.mod_symmetric and tree.left.mark == \"+\" and tree.right.mark == \"+\"\n",
    "        \n",
    "        if left_mod:\n",
    "            return self.left_modifier_generate\n",
    "        elif right_mod:\n",
    "            return self.right_modifier_generate\n",
    "        elif sym_mod:\n",
    "            return self.symmetric_generate\n",
    "        else:\n",
    "            return self.generate_default\n",
    "\n",
    "    def generate_special(self, tree):\n",
    "        if tree.val == \"nsubj\":\n",
    "            if tree.left.val == \"who\" and tree.right.val == \"aux\":\n",
    "                self.left_modifier_generate(tree)\n",
    "\n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "\n",
    "    def delete_cc(self, tree):\n",
    "        if tree.val == \"cc\" and tree.left.val != \"but\":\n",
    "            self.delete_modifier(tree, tree.right)\n",
    "\n",
    "        if tree.is_tree:\n",
    "            self.delete_cc(tree.left)\n",
    "            self.delete_cc(tree.right)\n",
    "\n",
    "    def delete_modifier(self, tree, modifier):\n",
    "        tree.val = modifier.val\n",
    "        tree.mark = modifier.mark\n",
    "        tree.pos = modifier.pos\n",
    "        tree.id = modifier.id\n",
    "        \n",
    "        tree.is_tree = modifier.is_tree\n",
    "        tree.is_root = modifier.is_root\n",
    "\n",
    "        tree.left = modifier.left\n",
    "        tree.right = modifier.right\n",
    "\n",
    "        self.delete_cc(tree)\n",
    "        self.save_tree()\n",
    "\n",
    "    def delete_left_modifier(self, tree):\n",
    "        #print(\"Delet: \", tree.left.val)\n",
    "        group = self.removeGroup(tree)\n",
    "        if(not group):\n",
    "            self.delete_modifier(tree, tree.right)\n",
    "\n",
    "    def delete_right_modifier(self, tree):\n",
    "        #print(\"Delet: \", tree.right.val)\n",
    "        self.delete_modifier(tree, tree.left)\n",
    "\n",
    "    def rollback(self, tree, backup):\n",
    "        tree.val = backup.val\n",
    "        tree.left = deepcopy(backup.left)\n",
    "        tree.right = deepcopy(backup.right)\n",
    "        tree.mark = backup.mark\n",
    "        tree.pos = backup.pos\n",
    "        tree.id = backup.id\n",
    "        tree.is_tree = backup.is_tree\n",
    "        tree.is_root = backup.is_root\n",
    "\n",
    "    def symmetric_generate(self, tree):\n",
    "        self.right_modifier_generate(tree)\n",
    "        self.left_modifier_generate(tree)\n",
    "        #self.delete_cc(tree)\n",
    "\n",
    "    def right_modifier_generate(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "\n",
    "        self.delete_right_modifier(tree)\n",
    "        self.save_tree()\n",
    "        self.rollback(tree, backup)    \n",
    "        \n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "\n",
    "    def left_modifier_generate(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "\n",
    "        self.delete_left_modifier(tree)\n",
    "        self.save_tree()\n",
    "        self.rollback(tree, backup)   \n",
    "\n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "    \n",
    "    def return_last_leaf(self, tree):\n",
    "        max_id = 0\n",
    "        max_id_l = 0\n",
    "        max_id_r = 0\n",
    "\n",
    "        if tree.id != None:\n",
    "            max_id = int(tree.id)\n",
    "    \n",
    "        if tree.left.is_tree:\n",
    "            max_id_l = self.return_last_leaf(tree.left)\n",
    "        else:\n",
    "            max_id_l = tree.left.id\n",
    "\n",
    "        if tree.right.is_tree:\n",
    "            max_id_r = self.return_last_leaf(tree.right)\n",
    "        else:\n",
    "            max_id_r = tree.right.id\n",
    "\n",
    "        return max(max_id, max(max_id_l, max_id_r))\n",
    "\n",
    "    def return_first_leaf(self, tree):\n",
    "        min_id = 100\n",
    "        min_id_l = 100\n",
    "        min_id_r = 100\n",
    "\n",
    "        if tree.id != None:\n",
    "            min_id = int(tree.id)\n",
    "    \n",
    "        if tree.left.is_tree:\n",
    "            min_id_l = self.return_last_leaf(tree.left)\n",
    "        else:\n",
    "            min_id_l = tree.left.id\n",
    "\n",
    "        if tree.right.is_tree:\n",
    "            min_id_r = self.return_last_leaf(tree.right)\n",
    "        else:\n",
    "            min_id_r = tree.right.id\n",
    "\n",
    "        return min(min_id, min(min_id_l, min_id_r))\n",
    "\n",
    "    def add_modifier_sent(self, tree, modifier, direct=0): \n",
    "        sentence = deepcopy(self.sentence)\n",
    "        if direct == 0:\n",
    "            last_leaf = self.return_first_leaf(tree)\n",
    "            sentence.insert(last_leaf-1, modifier)\n",
    "        elif direct == 1:\n",
    "            last_leaf = self.return_last_leaf(tree)\n",
    "            sentence.insert(last_leaf, modifier)        \n",
    "\n",
    "        self.remove_adjcent_duplicate(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        sentence = sentence.replace(\" 's\", \"'s\")\n",
    "\n",
    "        if abs(len(sentence) - len(self.hypothesis)) < 15:\n",
    "            re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', sentence, flags = re.I)\n",
    "            sentence = sentence.strip() \n",
    "            \n",
    "            if sentence.lower() == self.hypothesis.lower():\n",
    "                self.stop_critarion = True\n",
    "                self.sent_log.append((sentence, 0.0))\n",
    "                return\n",
    "                \n",
    "            similarity = inference_sts(sentence, self.hypothesis, dist=True)\n",
    "            self.sent_log.append((sentence, similarity))\n",
    "\n",
    "            if similarity < 0.5:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "                self.stop_critarion = True\n",
    "\n",
    "    def add_modifier_lexical(self, tree, modifier, head, word_id, direct=0):\n",
    "        if direct == 0:\n",
    "            generated = ' '. join([modifier, head])\n",
    "        else:\n",
    "            generated = ' '. join([head, modifier])\n",
    "        \n",
    "        sentence = deepcopy(self.sentence)\n",
    "        diff = 0\n",
    "        if word_id > len(sentence):\n",
    "            diff = word_id - len(sentence)\n",
    "\n",
    "        goal = word_id-1-diff\n",
    "        sentence[goal] = \"DEL\"\n",
    "        sentence[goal:goal] = generated.split(' ')\n",
    "\n",
    "        if abs(len(sentence) - len(self.hypothesis.split(' '))) < 7:\n",
    "            self.remove_adjcent_duplicate(sentence)\n",
    "            sentence = ' '.join(sentence)\n",
    "            sentence = sentence.replace(\"DEL \", \"\")\n",
    "            sentence = sentence.replace(\"DEL\", \"\")\n",
    "            sentence = sentence.replace(\"-\", \" \")\n",
    "            sentence = sentence.replace(\" 's\", \"'s\")\n",
    "            re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', sentence, flags = re.I)\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence.lower() == self.hypothesis.lower():\n",
    "                self.stop_critarion = True\n",
    "                self.sent_log.append((sentence, 0.0))\n",
    "                return\n",
    "            \n",
    "            similarity = inference_sts(sentence, self.hypothesis, dist=True)\n",
    "            self.sent_log.append((sentence, similarity))\n",
    "\n",
    "            if similarity < 0.5:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "                self.stop_critarion = True\n",
    "\n",
    "    def generate_default(self, tree):\n",
    "        VP_rel = {\n",
    "            \"aux\":1, \n",
    "            \"obj\":1, \n",
    "            \"obl\":1, \n",
    "            \"xcomp\":1, \n",
    "            \"ccomp\":1,\n",
    "            \"aux:pass\":1, \n",
    "            \"obl:tmod\":1, \n",
    "            \"obl:npmod\":1\n",
    "        }\n",
    "\n",
    "        VP_mod = {\n",
    "            \"advcl\":1, \n",
    "            \"xcomp\":1, \n",
    "            \"ccomp\":1,\n",
    "            \"obj\":1, \n",
    "            \"advmod\":1, \n",
    "            \"obl\":1, \n",
    "            \"obl:tmod\":1,\n",
    "            \"obl:nmod\":1, \n",
    "            \"parataxis\":1, \n",
    "            \"conj\":1\n",
    "        }\n",
    "\n",
    "        NP_rel = {\n",
    "            \"amod\":1,\n",
    "            \"compound\":1,\n",
    "            \"det\":1,\n",
    "            \"mark\":1,\n",
    "            \"nmod:poss\":1,\n",
    "            \"flat\":1,\n",
    "            \"acl:relcl\":1,\n",
    "            \"acl\":1,\n",
    "            \"nmod\":1\n",
    "        }\n",
    "\n",
    "        NP_mod = {\n",
    "            \"amod\":1,\n",
    "            \"compound\":1,\n",
    "            \"det\":1,\n",
    "            \"mark\":1,\n",
    "            \"nmod:poss\":1,\n",
    "            \"flat\":1,\n",
    "        }\n",
    "\n",
    "        if tree.pos is not None:\n",
    "            if (\"NN\" in tree.pos or \"JJ\" in tree.pos) and tree.mark == \"-\":\n",
    "                for rel in [\"amod\", \"compound\", \"det\", \"mark\", \"nmod:poss\", \"flat\", \"conj\", \"nummod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            if phrase['head'] == tree.val:\n",
    "                                self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id)\n",
    "                for rel in [\"amod\", \"acl:relcl\", \"compound\", \"acl\", \"nmod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            if phrase['head'] == tree.val:\n",
    "                                self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id, 1)\n",
    "                \n",
    "            elif \"VB\" in tree.pos and tree.mark == \"-\":\n",
    "                for rel in [\"advmod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id)\n",
    "                            self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id, 1)\n",
    "\n",
    "        elif VP_rel.get(tree.val, 0) and tree.mark == \"-\":\n",
    "            for rel in VP_mod:\n",
    "                if rel in self.kb:\n",
    "                    for phrase in self.kb[rel]:\n",
    "                        self.add_modifier_sent(tree, phrase['mod'], direct=1)\n",
    "\n",
    "        elif NP_rel.get(tree.val, 0) and tree.mark == \"-\":\n",
    "            for rel in NP_mod:\n",
    "                if rel in self.kb:\n",
    "                    for phrase in self.kb[rel]:\n",
    "                        self.add_modifier_sent(tree, phrase['mod'], direct=0)\n",
    "        \n",
    "        if VP_rel.get(tree.val, 0) and tree.right.val == \"watching\":\n",
    "            self.save_tree(tree=tree.left)\n",
    "        if tree.is_tree:\n",
    "            self.generate(tree.left)\n",
    "            self.generate(tree.right)  \n",
    "\n",
    "    def save_tree(self, tree=None):\n",
    "        if tree is None:\n",
    "            leaves = self.deptree.sorted_leaves().popkeys()\n",
    "            tree_copy = self.deptree.copy()\n",
    "        else:\n",
    "            leaves = tree.sorted_leaves().popkeys()\n",
    "            tree_copy = tree.copy()\n",
    "        \n",
    "        sentence = ' '.join([x[0] for x in leaves])\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        if sentence.lower() == self.hypothesis.lower():\n",
    "            self.tree_log = []\n",
    "            self.stop_critarion = True\n",
    "            self.tree_log.append((tree_copy, sentence, 0.0))\n",
    "            return\n",
    "        \n",
    "        similarity = inference_sts(sentence, self.hypothesis, dist=True)\n",
    "        self.tree_log.append((tree_copy, sentence, similarity))\n",
    "\n",
    "        if similarity < 0.5:\n",
    "            self.tree_log = []\n",
    "            self.tree_log.append((tree_copy, sentence, similarity))\n",
    "            self.stop_critarion = True\n",
    "    \n",
    "    def remove_adjcent_duplicate(self, string):\n",
    "        to_remove = -1\n",
    "        for i in range(len(string)-1):\n",
    "            if string[i] == string[i+1]:\n",
    "                to_remove = i\n",
    "        if to_remove > -1:\n",
    "            del string[to_remove]\n",
    "\n",
    "    def search_dependency(self, deprel, tree):\n",
    "        if tree.val == deprel:\n",
    "            return True\n",
    "        else:\n",
    "            right = tree.right\n",
    "            left = tree.left\n",
    "\n",
    "            left_found = False\n",
    "            right_found = False\n",
    "\n",
    "            if right is not None:\n",
    "                right_found = self.search_dependency(deprel, right)\n",
    "\n",
    "            if left is not None:\n",
    "                left_found = self.search_dependency(deprel, left)\n",
    "\n",
    "            return left_found or right_found\n",
    "    \n",
    "    def Diff(self, li1, li2):\n",
    "        return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))    \n",
    "    \n",
    "    def preprocess(self, sentence):\n",
    "        preprocessed = sentence.replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "        preprocessed = preprocessed.replace(\"can't\", \"can not\")\n",
    "        preprocessed = preprocessed.replace(\"couldn't\", \"could not\")\n",
    "        preprocessed = preprocessed.replace(\"don't\", \"do not\")\n",
    "        preprocessed = preprocessed.replace(\"doesn't\", \"does not\")\n",
    "        preprocessed = preprocessed.replace(\"isn't\", \"is not\")\n",
    "        preprocessed = preprocessed.replace(\"won't\", \"will not\")\n",
    "        preprocessed = preprocessed.replace(\"wasn't\", \"was not\")\n",
    "        preprocessed = preprocessed.replace(\"weren't\", \"were not\")\n",
    "        preprocessed = preprocessed.replace(\"didn't\", \"did not\")\n",
    "        preprocessed = preprocessed.replace(\"aren't\", \"are not\")\n",
    "        preprocessed = preprocessed.replace(\"it's\", \"it is\")\n",
    "        preprocessed = preprocessed.replace(\"wouldn't\", \"would not\")\n",
    "        preprocessed = preprocessed.replace(\"There's\", \"There is\")\n",
    "        return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_relation = {\n",
    "    \"NN\": [\"amod\", \"nmod\", \"acl:relcl\", \"fixed\", \"compound\", \"det\", \"nmod:poss\", \"conj\", \"nummod\"],\n",
    "    \"VB\": [\"advmod\", \"acl\", \"obl\", \"xcomp\", \"advcl\", \"obl:tmod\", \"parataxis\", \"obj\",\"ccomp\"]\n",
    "}\n",
    "\n",
    "def down_right(tree):\n",
    "    if(tree.right == None):\n",
    "        return tree\n",
    "    return down_right(tree.right)\n",
    "\n",
    "def down_left(tree):\n",
    "    if(tree.left == None):\n",
    "        return tree\n",
    "    return down_left(tree.left)\n",
    "\n",
    "def collect_modifiers(tree, sent_set, mod_type=\"NN\"):\n",
    "    leaves = []\n",
    "    if tree.is_tree:\n",
    "        if tree.val in [\"mark\", \"case\", \"compound\", \"flat\", \"nmod\"]:\n",
    "            leaves.append(\n",
    "                (list(tree.right.sorted_leaves().popkeys()),\n",
    "                down_right(tree.left).val)\n",
    "            )\n",
    "        if tree.val in modifier_relation[mod_type]:\n",
    "            leaves.append(\n",
    "                (list(tree.left.sorted_leaves().popkeys()),\n",
    "                down_right(tree.right).val)\n",
    "            )\n",
    "\n",
    "        for leave in leaves:\n",
    "            if len(leave) > 0 and len(leave) < 10:\n",
    "                head = leave[1]\n",
    "                modifier = ' '.join([x[0] for x in leave[0]])\n",
    "                if tree.val in sent_set:\n",
    "                    sent_set[tree.val].append({'head': head,'mod': modifier})\n",
    "                else:\n",
    "                    sent_set[tree.val] = [{'head': head,'mod': modifier}]\n",
    "        \n",
    "        collect_modifiers(tree.left, sent_set, mod_type)\n",
    "        collect_modifiers(tree.right, sent_set, mod_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "\n",
      "Init Premise: A man with a helmet painted red is riding a blue motorcycle down the road\n",
      "\n",
      "Hypothesis: A man with a helmet is riding a blue motorcycle down the road\n",
      "{   'amod': [{'head': 'motorcycle', 'mod': 'blue'}],\n",
      "    'case': [   {'head': 'with', 'mod': 'a helmet'},\n",
      "                {'head': 'down', 'mod': 'the road'},\n",
      "                {'head': 'with', 'mod': 'a helmet'},\n",
      "                {'head': 'down', 'mod': 'the road'}],\n",
      "    'det': [   {'head': 'helmet', 'mod': 'a'},\n",
      "               {'head': 'man', 'mod': 'A'},\n",
      "               {'head': 'road', 'mod': 'the'},\n",
      "               {'head': 'motorcycle', 'mod': 'a'}],\n",
      "    'nmod': [   {'head': 'helmet', 'mod': 'A man'},\n",
      "                {'head': 'man', 'mod': 'with a helmet'},\n",
      "                {'head': 'helmet', 'mod': 'A man'}],\n",
      "    'obj': [{'head': 'riding', 'mod': 'a blue motorcycle'}],\n",
      "    'obl': [{'head': 'riding', 'mod': 'down the road'}]}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApkAAAEMCAMAAAC4Ml0NAAAJJmlDQ1BpY2MAAEiJlZVnUJNZF8fv8zzphUASQodQQ5EqJYCUEFoo0quoQOidUEVsiLgCK4qINEWQRQEXXJUia0UUC4uCAhZ0gywCyrpxFVFBWXDfGZ33HT+8/5l7z2/+c+bec8/5cAEgiINlwct7YlK6wNvJjhkYFMwE3yiMn5bC8fR0A9/VuxEArcR7ut/P+a4IEZFp/OW4uLxy+SmCdACg7GXWzEpPWeGjy0wPj//CZ1dYsFzgMt9Y4eh/eexLzr8s+pLj681dfhUKABwp+hsO/4b/c++KVDiC9NioyGymT3JUelaYIJKZttIJHpfL9BQkR8UmRH5T8P+V/B2lR2anr0RucsomQWx0TDrzfw41MjA0BF9n8cbrS48hRv9/z2dFX73kegDYcwAg+7564ZUAdO4CQPrRV09tua+UfAA67vAzBJn/eqiVDQ0IgALoQAYoAlWgCXSBETADlsAWOAAX4AF8QRDYAPggBiQCAcgCuWAHKABFYB84CKpALWgATaAVnAad4Dy4Aq6D2+AuGAaPgRBMgpdABN6BBQiCsBAZokEykBKkDulARhAbsoYcIDfIGwqCQqFoKAnKgHKhnVARVApVQXVQE/QLdA66At2EBqGH0Dg0A/0NfYQRmATTYQVYA9aH2TAHdoV94fVwNJwK58D58F64Aq6HT8Id8BX4NjwMC+GX8BwCECLCQJQRXYSNcBEPJBiJQgTIVqQQKUfqkVakG+lD7iFCZBb5gMKgaCgmShdliXJG+aH4qFTUVlQxqgp1AtWB6kXdQ42jRKjPaDJaHq2DtkDz0IHoaHQWugBdjm5Et6OvoYfRk+h3GAyGgWFhzDDOmCBMHGYzphhzGNOGuYwZxExg5rBYrAxWB2uF9cCGYdOxBdhK7EnsJewQdhL7HkfEKeGMcI64YFwSLg9XjmvGXcQN4aZwC3hxvDreAu+Bj8BvwpfgG/Dd+Dv4SfwCQYLAIlgRfAlxhB2ECkIr4RphjPCGSCSqEM2JXsRY4nZiBfEU8QZxnPiBRCVpk7ikEFIGaS/pOOky6SHpDZlM1iDbkoPJ6eS95CbyVfJT8nsxmpieGE8sQmybWLVYh9iQ2CsKnqJO4VA2UHIo5ZQzlDuUWXG8uIY4VzxMfKt4tfg58VHxOQmahKGEh0SiRLFEs8RNiWkqlqpBdaBGUPOpx6hXqRM0hKZK49L4tJ20Bto12iQdQ2fRefQ4ehH9Z/oAXSRJlTSW9JfMlqyWvCApZCAMDQaPkcAoYZxmjDA+SilIcaQipfZItUoNSc1Ly0nbSkdKF0q3SQ9Lf5RhyjjIxMvsl+mUeSKLktWW9ZLNkj0ie012Vo4uZynHlyuUOy33SB6W15b3lt8sf0y+X35OQVHBSSFFoVLhqsKsIkPRVjFOsUzxouKMEk3JWilWqUzpktILpiSTw0xgVjB7mSJleWVn5QzlOuUB5QUVloqfSp5Km8oTVYIqWzVKtUy1R1WkpqTmrpar1qL2SB2vzlaPUT+k3qc+r8HSCNDYrdGpMc2SZvFYOawW1pgmWdNGM1WzXvO+FkaLrRWvdVjrrjasbaIdo12tfUcH1jHVidU5rDO4Cr3KfFXSqvpVo7okXY5upm6L7rgeQ89NL0+vU++Vvpp+sP5+/T79zwYmBgkGDQaPDamGLoZ5ht2GfxtpG/GNqo3uryavdly9bXXX6tfGOsaRxkeMH5jQTNxNdpv0mHwyNTMVmLaazpipmYWa1ZiNsulsT3Yx+4Y52tzOfJv5efMPFqYW6RanLf6y1LWMt2y2nF7DWhO5pmHNhJWKVZhVnZXQmmkdan3UWmijbBNmU2/zzFbVNsK20XaKo8WJ45zkvLIzsBPYtdvNcy24W7iX7RF7J/tC+wEHqoOfQ5XDU0cVx2jHFkeRk4nTZqfLzmhnV+f9zqM8BR6f18QTuZi5bHHpdSW5+rhWuT5z03YTuHW7w+4u7gfcx9aqr01a2+kBPHgeBzyeeLI8Uz1/9cJ4eXpVez33NvTO9e7zofls9Gn2eedr51vi+9hP0y/Dr8ef4h/i3+Q/H2AfUBogDNQP3BJ4O0g2KDaoKxgb7B/cGDy3zmHdwXWTISYhBSEj61nrs9ff3CC7IWHDhY2UjWEbz4SiQwNCm0MXwzzC6sPmwnnhNeEiPpd/iP8ywjaiLGIm0iqyNHIqyiqqNGo62ir6QPRMjE1MecxsLDe2KvZ1nHNcbdx8vEf88filhICEtkRcYmjiuSRqUnxSb7JicnbyYIpOSkGKMNUi9WCqSOAqaEyD0tandaXTlz/F/gzNjF0Z45nWmdWZ77P8s85kS2QnZfdv0t60Z9NUjmPOT5tRm/mbe3KVc3fkjm/hbKnbCm0N39qzTXVb/rbJ7U7bT+wg7Ijf8VueQV5p3tudATu78xXyt+dP7HLa1VIgViAoGN1tubv2B9QPsT8M7Fm9p3LP58KIwltFBkXlRYvF/OJbPxr+WPHj0t6ovQMlpiVH9mH2Je0b2W+z/0SpRGlO6cQB9wMdZcyywrK3BzcevFluXF57iHAo45Cwwq2iq1Ktcl/lYlVM1XC1XXVbjXzNnpr5wxGHh47YHmmtVagtqv14NPbogzqnuo56jfryY5hjmceeN/g39P3E/qmpUbaxqPHT8aTjwhPeJ3qbzJqamuWbS1rgloyWmZMhJ+/+bP9zV6tua10bo63oFDiVcerFL6G/jJx2Pd1zhn2m9az62Zp2WnthB9SxqUPUGdMp7ArqGjzncq6n27K7/Ve9X4+fVz5ffUHyQslFwsX8i0uXci7NXU65PHsl+spEz8aex1cDr97v9eoduOZ67cZ1x+tX+zh9l25Y3Th/0+LmuVvsW523TW939Jv0t/9m8lv7gOlAxx2zO113ze92D64ZvDhkM3Tlnv296/d5928Prx0eHPEbeTAaMip8EPFg+mHCw9ePMh8tPN4+hh4rfCL+pPyp/NP637V+bxOaCi+M24/3P/N59niCP/Hyj7Q/Fifzn5Ofl08pTTVNG02fn3Gcufti3YvJlykvF2YL/pT4s+aV5quzf9n+1S8KFE2+Frxe+rv4jcyb42+N3/bMec49fZf4bmG+8L3M+xMf2B/6PgZ8nFrIWsQuVnzS+tT92fXz2FLi0tI/QiyQvpNzTVQAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAMBQTFRF////AFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQF1JSF1JSF1JSF1JSF1NTF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSAFmRAFmRAFqRF1NTF1JSF1JSF1JSP49XP49XP49XP49XP49XP49XP49XP49XP49XP49XP49XP49XF1JSP49XP49XAFqRF1JSQZNZAFmRP49XAFmQF1JSP49X////OlSkRgAAADx0Uk5TABFEM7si3YjMZqpVme53IkRmiHW7zO7Hd6ozEVXdmXrWhJ+Pa4ng90QRIt3MiHeZ7rszZuqqVVDsIGmCZHockAAAAAFiS0dEAIgFHUgAAAAJcEhZcwAAAEgAAABIAEbJaz4AAAAHdElNRQflAxkDNSB79R5zAAAfuklEQVR42u2dCX+ySLbGcUVNjEurk2i0+24zLjHROMncBfj+H+vWqQVQ2QoKKPA8v+5XRSB1qh5rg/pjGCgUCoVCoVAoFAqFQqFQKBQKhUKhUCgUCoVCoVAoFAqFQqFQKBQKhUKhUCgUCoVCoVAoFAqFQqFQKBQKlUmNZou9aTbpx7LTg0IxNa02e2NZ5J9Ot+z0oFBMrjNNk/zTs8yyE4R6MJn91qDdJ8212R6Ql45J/2mCM58Gz03mzGez94zWRBUqqzu0LKvfGFq9oTVgVWWT1JBNC7ZbTdaaG+2y04l6NIH5TGtAHTlo+51p9Y0X8gmdiSpFVo/2KVuW1SNtuN+ZMOghtkRnokoRc2LbeBqQtrvX8DkTzIjORJUl7sxOs9Hod60mNWSfteYNo2MN0ZmocsSdSXqUzaeu1WpYltkfMmcOnmCyCJ2JKkXcmY02jMRNg4yGyAt1Zq9rke3oTFTJajWbHfba4Vsa9MIkOhOlpUzmzH7Z6UA9vEbjyfSP6WQ8Yh8tvC6JKlnj8XQ6m9u2vZj9bbYgr/PZdDoel50s1MNqNH6dvs1sYcUl3+wz6ptXhaJQuWs1fifeW1PvbYj3ViF7TaYbtwp9H5edalSNtSSV5IxWkrPZ9NWtJOMO4lWoTarQV6xCUQoFleSGVpJrUkm+h1SScecQVegaq1BUVuVQ3/nqXaxCUbIa5d5HVFENox5IhY+rU3VdUY8jNgFU3lwkVqGoGyWaACpO3hQpVqEPqqtWdKqdBTT7waDyV7WazZCrTKg66WYCqFqFTKvQOVahtVKdZrhhMqvMcRpKlZY1vbHCN7c1LTstqFSqd8UCVWiNw3sYcUZbjdlsNQ6t1mLrwGvMZqtxaPUWc2aN2Ww1Dq3S4mi2jtl/Gjy3Ws9tujSsP2ibsKLxpW0+gTNrwGbrmO3BS8NDz5kvRsMkQdYgtHqKo9kYiW0I/zwRI8Ja8GGLvg4Z17Lqq2wbQ4jw2YXVkM9Nsx6h1VQums1qNbqkxABK0LKGHbax22j06lF8AKhpkGCFMw3SFgyHnTqEVlNxNBstsTZ1KXMnKcYhfX2phzONxpPZ9jvTGFjWi1GL0OopDnO5d6bBoC6CUl314usMrZ555UzyiT2zoOqh1VRBzuxbzwZp0rsvjDtUC2eapIIUrTm07NBlgfaiBqHVVEHOJKMDE7BsHcZpq4kzB03SfrvoORJs/5nNF1U9tJoqyJlGC8boA6hcyEs9nNmBkfnQagn03IvVNTqsPa96aI8lzmnriMdHVb/4xJOwPPScUOVDe2zVmM1W49DqpfFk+ue0Vne/3Yf4+uf0vdYR1kzEk3RBxewvuIlxtqnhOoXRePpGQlz/BWsxYDFb2QlCRWoF687W9I7aV74mYfk+fQN/zt+m7/Xw55L87hYkIPKDW/EI6ee3mjcQFRX15OLKk34tx9PNjK1SmIzLTmt6jV83rI68C2JFApzT8Cf1+P3VQMKT62BP+jUasyVB61nllnmJ1nsW2a8ci+o0LidQuWrEK8K1XEUYU71qp9vWOzZb3pOYGJWLUnrSrxUfJsE6WW37Z+Gtd/JDcXBUjJbvvMOoaEAD/tRxAJ+s9Y4TDo6KUJ6DbK0G8LKtd5xwcJSbhG8A95Jn3pY/gM/QeseeGgdHKuVOmRfYFyxnAK+m9Y79Kzg4yqwyPOlXgQN41a13rHBwlEoBl3HKTEy+A/gcW+844eAoufSdZ8xhAF9M6x0nHBzFSOIyTolSNYAvvPWOEw6OgjXJOGVesLwBfPpodQzWHRyVnRB9NNKwmOITPZ5OUkar9ah4/JourEeQ7kCzhrtuI2GSO2xVRLNptJoguolJr1i9yGgitS+JgqU90IwvW6fqU2ZSTJL7sESOHNY12sCysbotWCFOVXYsYZHRlGlfEgVLe6CZ35mwRjM2yQ1raLBl4m3r2TTbxKJN04Q3w7JjCYuMOlP7klCt/qD9IoBmpNjIKyysary0B0+G9qw2jpTjqe13recEDLYBsL96VocbmVeVraGlzYoyRswjznwaPIskal4S6kXpbG0ONANUQW8IrV0PXrVfzi+Qcjy1Jkl9gnXeQNLoAEajbb00mxyv1NCoSuLEPIbRg18P+/HoXBLq1bK6DaNr/RsDmrEGZNCmmBfi0o7e+SGQcm5qWSUYl+QGab/7AMVi/UzGbXvmxBcN5BHzSC1OfziP6ExKzug0GwxoRjLF6plNKCfSA+vSctY4PwRSzk1tMmeS5rwzgF8d7WcOqCVfrKE2Q19BzINBGnPlwzrTBZoZxtOA1CK9RtvqtYme9M4PgZRzU5vQmaTCpHZku8O/LYsdqYUEMY+NgB7VmS9AZ3tp/zsDmtHak4wjmiYMElp0fk/j/BBIOTe1CZ1JIqWESzGUb5KuwEvZwXgSxDzSmjdIh3j4mM7skL7M03D4HwxoBk1j86lrkc53r0k2a+5MgZRzU0uGNImSTEKlXWioa7ukHSfNBVS6moQqiHnEmYMnOjB7RGcapGStYV8AzRp0UEBrIspT1zw/BFJOpNa0knWN+xxDR0dAA+1m2jkxj/zgujBv8qDONBr0opx7Jazlwtp4x0vr/BBIOZHapu4dkITihQCF422sflhqpc30c62T/MBhJdXq9c9x2WlIpeV/prqXUecbjeidmv+F97iD3t/sxd/t9bR6eTGx/zFPYc2xXXbCQ+SuT/2rzNVXmmg5XdhvE/Y6m1TrbuqJvVmu1/J3tmvoTN+aae7GslcHlqvR69pev4q4oe7cjMtOU3JN7SnpiMwX0knWypn3nrxK6kP6c0KcOL2qcVav8+q06hub3vm94q8S0sSZsD6OLmmKdd1D+XO8Ia34+/32yrTqniFp3SkVe9nOFKwyuWV2j+DP0XRtz1/D7FeFVp004l6Rkv6m1E+pRGem8+RV4uvrz9VkFtdka9+qXxmTlNZCaoheijOze/IqhPr5831j25v3+P20btWX8/l14coN0Ud2oYVJF/KzRdOKWWX18Se04sntpm2rvryvIuWG6HZBUfngEjkumq68P1M00Xq26sugtltqiJ6/M4vxpF+V9WfaClC/Vp2MdwK3SwzR83Rm8Z70q2r+FBd60kmvVj3MmDJD9JycydFEi9LJgxXx5/WFnlTSqFWPqhkTD9GVO9PlKOoEK9Pcn/cXetJJk1Y9ujeZdIiu0JlaejIggbr5cxJ8oSedoFUvGQU1jRnmkCF6EnuocqbenvTL9acuLK/Rq9I8W2XrFCiIJ65KXCX6HaqKYqJVNRQv4k8N2LFwyz5bh9DKjjuTZq8pD6bJ1iK0OMCtc7eDiDQy5q3v/U51IrVZux6SPFGGZUPm2JJlWMTczr6sWpq9pjwYxrvh68usO+iLF2lkzI73dn9QnEbdsW1uGZYNmWPOZFQKpc5MxF5THgwFcrTJzwJeTLN5t4OINDJmX5354RzVplEjRlKg3DI0zXJTy53Zzu5Mzl7j8LWE7DXVwTTpkmyK1grKUy/SyJiJGXefx0/SkH8dP74krckAemEEPX2xbQwy51HmwJllppa35tZTVmcK9hqHryVkr6kOBkIY8IY60Jki0siYHWN3Oh8/PuD9OUU+ABwxlKCn6QJcDpkzXMpc2QvZmTMbw25GZwr2mouKS0hrURwM/E0zypki0siYHWNLupq7896QdiYH6LX6oQQ9PZ0pIHOGS5nTw5mAAcrmTMFec1FxmjpTRBoZs2PsnS/e2ZR0pgDoGaEEPT2dKSBzhkuZ08SZRnfYze5MqCAEfK08Z7Y5MivYmTzSyJhJhbn9Ojlf8D6NM40ogp7GzjREJa+TM5tWNhifYK8ZAr5WmjNbApkV4sym24sKjZnUmd/EnKeLIe1MDtBrmaEEPT2dKSBzhkuZ08WZwEDL4kzBXjMEfC0pe01tML12j89ihjqTRRoZs2N8HnbG7pDCmRyg1zBDCXp6OlNA5gyXMqeNMzsZAaaCvSbga0nZa2qDgSelMEeGO7PjOrMT6szdx+l8OsP1H9mxOQPoGeEEPT2dKSBzHmWubGeqk2CvCfhaxdlr2+03fZV1JgfohRP0tM0R92Iuo8zVx5nBqjyk7KL6hBXJEZM5s+zUqr0VZjX5o/xbNLXR6E9dbilLnOLJn9OxpcVV1KnCh76uJm+2/TfbfpuUdudX/ELx4paSjzb2P+11dby5nGzWtg2Qudn0vfzqRZkzR69ze7EhFebqfbOw5yXdp6mPM4kviSvhX7V3wOaj5evbwl68vcLNmav36dy252W7U40zmS29e3KpOcu4B1UXZzJf0nfTxWKqtTfHPlcKEXfObHu9Ka/1U+HM5Y0t+dbp2l4Xbk49nDne+FvxlcbeHEcZkH/5Ws5d7lmdGWVA9p2yVUZJpIMzxzN7Nr7aoqU3WbU4j6kWlwEVajHK5Mz4enEUWJ/mp/Kdee9LEHhzo8+KILmu5PJ1Q4dF4yQ7K1N6ZybtS45eZ+RHV9BcUtnODPYl1WRta+HN0QRcKTv8Hk02BQ/aJ+tUh8mNv+l0UiHmLNeZEb4Ele9N4a9xqqOLHbSnKKhU00LEnAt7lvtcUpnOjPElqExvkh7jmjReGdvk4gbtsgWVpforYKKzPGcm8CVoMk+0m2IpHscUMmiXKqjsrfJ7znNJZTkzoS8ld1WTNIB8qbdR7oP25AXlXuTJGlKe5izHmdBIjyUSWZg3x/k2vbkO2hMWlNrZn+U0r7mkMpyZovMI3sx5Ks0druTcIcxt0J6koJY5TEpyqxcfjmpnphzUXF8mUqyir3zn8vfiC2qZV+M7gl6r5IN6soej2JnpB9twaV1t8FwT1sIWfM2JD9oV/tri8zXHAcuqBBy0WmXpwo3yqTRHBV+s8WmsCO7Ll1VvBfPsmn3mEdXC8GqKpIo8ttvug+KIPiZbslOh9AKweSpUEo2vw2xBcoPZRCTGfMpgF8cwLqQwHe7QG/aZS1cLxaspkiry2MU5BMURpSy4t9QoPT+cjHPzVKgkGl8f1rZRiAKzSRd+H/1hNr8QR563njNv2GcuXS0Ur6ZI1JkKyGM/Z+c7II4oZcG9pUbpXTlTAYYv4LQF0vgA7gFLh17IHyWGaQPno09c+WIOJf/8Jym9y8Uwvo/G0bgcvrbEmZ/Hz3v2mZ+uFrxQNq0EK42j06CIFZDH9s73B6T/No7j7vO4NS4QIon8eITfIYn8eEmFe/PnTxqU3jU2j3PzsqtEGt8AQA898gtl2UAJKBasbmMwmuT6+TKM04kUzg9pzZkzD8fjCQroehWrn66m1JmClcbRaarWkH4e4D/jLg7nfPxyzl80xDOESizpfAWGLKVUKL0bbJ4huHkZVSaNDwBjHatH0T3N5gslYqTqRlwOxvfh9G18XKCfSVtzUkQXKKIgZ5rqnclZaQKdpsqZpMLcs47JjTOJEz/Iz/FINgPijfwkw0KWUhqU3h02D45ttakyLKItlcbXID7sA0+K9TOHHfIJ/rBpmnKe2Tn7z6+fz72zd51J/t8W6EzOSnPRaWqcuXcu2+0hgJQF4Z2PzJmkE/P7cQ4NWUppUHp32DxwZjPz+LJcGt/A6gxYd5v88QGpPfuc3MUKNrk+Lj+fnz+XDyORMxldTakzOStNoNMUOfPogE6GEeXMI2nWz2qdKYXSu8PmqRkBlUvjIxUmNOY8FGgE6KCo2exKOvP4dfr+PkHPP4EzOV1NqTM5K02g0xQ58wDdxr3za0Q48/e0Zw5V6UwZlN4dNg8KsmNSZXBouTQ+UpYcDskmBJqkjwZuacrWmd8w/jmdtsyZvxHO9Ohqip1JWWkCnabGmd8OnWaHDmW4M0kf29gpbs2lUHp32Dxok/6VuTUvmcY3EHBIUmF3LRLUExnZms9kTCZ5ohMZAvycDOrMn8Ml3JkeXU11a05ZaRydpsaZX5Ssbnw6uwhn7j4O58PX6aLWmTIovVtsHrz+K/sVtnJpfKxfyUdAA5gUeKJ0uRdZZ/rlXtEDZSgmKYnraBydxqQwFyPi2G53Rhkhu7rF5hnN7E8JMzSk8TWVXoJVzj6TkULymEQcpYacu8qmuVVYq8mfGjzmsCyN/sgj+PEf4zJjKvBvzRTfQunpfWMv/p71yerlKgtI4n3x3+qfbPw+s/8q8Xm7KtGCscrJmaPpmj4OHR6L/lYdlN+NMpTE1N6s4H+VyZms7bdxvvfL55Yf8srDmavJzF67d5nCY9F1eHhxCqUuidWM1pfvi7W6wL2FH3C/fCkgpYo7c7xZ2NfLikava3teBc7krdKWxFg4cjWzX5WkZDW9WvhREhyxys6EVnwesDZ4Sez6Vig3ToVSlsSr/ebmwNT3PrUobW50t6lwIEh1nTmBhjssu66a+GooVUms3q7qyfF6Mc6WipAKclU8rKaizoyvFsMqVG2VpiSWt04kTs2Sx1GdSuLNQoEgVXTm6nWeqCtJOqELGeBFuUpREpPF7C4XXu152rptFDMQLxZWkxItmE5KnAnD703CUehqMq9Mqy7tzNUmsH5czhep+tjjt/gJIuLNwiaRintyiKHCmSPpKUs22VlgkGkl68xRmAPBsdK9mKQQGlqv6gq9TK+MzkxbA8rUsuVJ0pnvi/BWm3wnF+67RDs92hQziVQdZ8IFyLS9RuiZ6n7pUs6Z0Zd8SH0q0Uywyz3JFTCvlIMq4kxxATK96KVLnSc5ZZzJL/tEnS3x1Gaa+aCbufhcVAVnrhTNTk60vnQp4cxxgguRSfbJYjFJ0Ke8KuDMye0FyPSily4LDFhGyZ05TlQfrmb2OG6fZZZmmdQXmxzzo1BnptNI6VhwWYVxeowShpDgKnq2GwzG4zyjLHpYEItMS8Y3Y3vJrC7wYGupsGt5ZMU2+uv7CPgan2iini+8sEhzgPPxYsuSp1s3WwIzJgNsL4nikWlXFKnovaQIZR5sLRV2LQdtnahv73PKRepFE/V84YVFmgOcjxVbJmac42ZLUMZkge0lUTwyTcKZUoQyD7aWCruWg6KdeZ9TLlIvmqjnCy8s0hzgfKzYMjHjBITVCawzs8D2osTJaSHINIF0EyCyvkmahiez1TH7T4PnVuuZonr6gzZDRPK95AhlHmxNHruWS4Z8UmeSjNnfUPWALReUU36kXsTaaF94YZGmgPOxIrouELdEBD8uNTPuaHxujaObLUfDhezBv/ussL0ICXJa8OJXgXRzQWSAezK6VqcJ260h/PPEgG+wsNrFlUmtNfVga9LYtRz0cTqeT8SZX6fjl/Prp+oJttx9TvmRepHOdMMLi1QezseL6KpA3BLxCiTt6l/ni2HbeLY4hgvZOx6O549zYH4okSCnBZ+fI908EBlAwQBX17SsVgMAIqZltoANZloDby9ZZwrYmix2LQddAD7z4xjfQJA9HvxUPcGWC3OmGe9MN7ywSOURaLyI/AViiBLxFUhqZ8Jv0XGzxTEEZA9wbsYhR2cKclrI+RnSzQORkZ9zC3B1TdEYkW/ol4Cl9faSdKaArcli13IQ/ZX+kmYLeCB75398VD3B/MjgTDe8sEhTwPlYEfkLxBAl4iuQ1M6Erp7jZotjCCzPJ+TCMUdnCnJa8Pk50s0DkcFPFHB1d840rvaSdSaHrcli13LQD9QR0KGi2eFsfVS9OGe2OZMqypkivLBI5eF8vIjunWlwFF9ThTNFtnjOPObsTJecFnx+jnTzQGTkU2/YM66cSfueLdLMe3tJO5PB1mSxazmI5gWpHCja+NvZ+ah6Mc5sCSZVpDN5eGGRysP5eBFdOVOUiK9AstaZPFt8deaHkaszXXJamDMp0s0DkdHO9cu1M0kf3HzqUV6yu5esMxlsTRa7loO+nV+SIQ5AZOF5BX6qXpQzPaRejDN5eGGRysP5eBFdOVOUiK9AMjpTZIvnTGA/7075OdMlp4W25hTp5oHIgE7XuXam0aJUMMOHK5N3Zsd1pgx2LQeR7s3pCM+qcT4Oh28/VS/KmR5SL86ZHc+ZHSt4pl0KzseL6H/9BeKWiFcgGZ0pssVzJsmg8+EnzxGQS04LHgF5/LGoS5PiIpq7V4mEsqzab7/5a9g1ysLhcpEKvmjMS8Qrtswkcp4tQrvtfrsPn9VRKaXItFoTyioJl1NdInvn29gfaFZolx+rP9Xe77uq0ErfYGlzb/7kz/yTcjk5p3wuS2bV++IfKsgTnvID0hWl+Psuk+6XKS8ma/ufReMR9BGs7F+N1ws1wB4qdKanDHnBFmkUj+7QRJPFemzAmgB7pix+dKan1HnhOfIhvTmauQunl/NMVBS/0JmeUubFeOZ3Y+4rgrTTK6swuaaLuZrw0ZmeUuUFQBNGd1sSpqkGggrzboOKkRA601OKvAh24QN5cxqAmSC9TgUTSOhMT9J5Ee7AB/FmSLeSjNSzTyBt8lyKWoiSQq+UOzPafY/gTTIUD1nF/77O/AyHQrmMuSipnxQ7M955dffmch4xfZl9AgmdKX8mI6nr6uzNWOuN54tMPUV0pvyZjNFbUsfV1ptJrvdMbUkI3/XR6EzZM402Mm4r8RlC+YlUmG8J2upME0joTMkzxUGyVRyhu0iFmXBa6HoWXkroTKkzpXNZzbwp8zib9A9xQGfKnGmS1mHgzWKzJUeNpObR31MO0fH+TKm/lb7my3CoJmrdksnCduywncgO3n4RXLftLurjtXLmjKnRXQQJUx3NnAs/iVhgkYzO50o/6F5acRzZs/u2G5IRfbo+rWl1fftFcN2cbdRHoQssTsqbM5ZdkMzbCJKmOpLsFXESwT5LxkBzpR90L63aACYzLcA/0bdtKwR1B4AOtvLZ2y+C65bMmWfYnBdnTJ3O2/sIkqY60pkRJ0ntTN2ge2nV5sCIJ/GWrkUN0gD26ZFovf0iuG7O9vP4CW+2x+M3Ldfj7vO4BdyYt/Vy+NrmxhlTJ5pMERBL+V2qKU+NseUMF77nMufuzhgJ56NK70ydoHsZxFLeZXXmS7P5EpoRfcpB6Pn3i+C6OQdGVfsE3NoFnOmcybvzl38rLXLdVtDeizmTBcRTfpdqylNjbDkPvieYc7eKhvMxpW/NNYLuZVHb6rXbXd4lody7MDQzIOIAxOXbL4LrxqlqsC7UuJyoM6GoviiNRGxlrbn2zmStOaej8ZTfOfNoCLacC98TcLX780XC+ZhSO1Mj6F4mcZv1Dd7PHECtGKyB1RmwrovYL4LrJiAPhy2R8w3OJFvOR+pMsbVSzqQBuSm/cyb5mrPlYLxN4XscrrZ1qK6a7Ug4H1VqZ2oE3cukNsM/DQ2vyxkWCqkwqW29/SK4btyZx9MZdONMsbV6znRTHuhMzpYT8D0BV7tTNJyPjDPbfepI8SohraB7meQb9rC3vdBQGpbFGX1ivwiuG3fmL0yL7Kgtfc4UW6vnTDflQc4UbDkB3+Nwtf2Ryhvbx8D5GI8YHCleJaQVdC+TrpxJu5zD0KnZgWD0if0iuG7cmTvo6JOu17UzxVbj/Av76u/MXxGQm/IgZwq2nIDvcbjanTNj4HwAah2aPevZfZWQVtC9TGLOHLJZI9Ag/JJDnxPH3P0iuG6CqnY5fXycvm+cKbYaP4cwQp1WgmSKjjNPeZAzBVvOhe8J5tyNYuB8pHWCLO513FcJ6QXd01q7bdCVSb51F06o00ksmTfxBKVasOUEfO8WriYUDecz6JXiq1dUKdKOM1ZcqqsZerGSvIfm4ZglZQgzGZR0HTVT9e+zrICyPDC3Ag/bTSp0pnZCZ1KhM7UTOpMKnamd0JlU6EzthM6kQmdqJ3QmFTpTO6EzqeTWj07qs2hUX43s9BOaNXKmHDOvRoFrLLl27Eo1KiB0pn5CZ4LQmfoJnQlCZ+ondCYInamf0JkgdKZ+QmeC0Jn6CZ0Jwvsz9RNmshBbFBgBOdvGHFsJqpu2crFxgetTVPyFqxNXqbDo2ugoyJlvtdVlf3+s/lQ3reUuTQ/gk8Vl7W1xBMt/4koVFs2aKMiZL7Dz9v5Y/aluWivKmXFZe47EdAr5T6xzYTF+GYebCW7ZFZ9sf9mxhdIcXgaLcsn7C6dQ3RyrP9VNb20dntuA+YDPsH6NZfF11l5h9Sh/jhUHR9H5uHQXON3vRXzlbENhdlqJ8ssE3MzjlvlWmm4/Tl9f5Lcl4GUOHESZaDQr7o7Vfn2u1to6LLfp6nyDrcx32XP+rPVj9Rh/jvEeOYqOcekOx5/T/hNQSwfxCU4cBrPTSpAJAm7m45b5nel8wvr+nYCXOS7kDZqP+2M1DrYC4rm99znTY89dOdPD6gn+HCkOF0XHuHR7AE/sycu3s+OfyIlDYXZaCep2ATfj3LKbFLOeD+zH4GWOB98gWXF/rMbBVkBubnvO9NhzV870wcs4f44Uh4ui8+hfxMCfxvHL/eRsQ2F2WolGwOFmPm7ZnTNPWwEvu3bm/bEaB1sBidz2OdNjz4U5k/PnoDgEis7vTNKcH359zgyF2WkliEDAzTi3zDBunLkj3zr/J+Bl1868P1bjYCsgntt+Z3rsuRBnCv4cKQ7x1mN5kiHU3vk8eZ+cbSjMTitBBAJuxrllNyneOtCbObnwMr8zfwOO1TjYCojl9kHUe7uDj5oX5kzBnyPFId5yLh2cgPRbf05H75OzDYXZaSXGL+NwM49bduXM8+Fw+nXhZT5nAh/t7liNg62Ath8fJLfZ8xa+TucPPzUvzJmCP0eLg70VXLrz4WMHr9/eJ/JNGMxOQwm4mcstu+5nsm9deJnvsP39sdoHq7s8cN43z26RxaFZy/hzO4ae295uNy4f/k9GDMxOb/n4ZJEPs4k+FKVW6bJ2R1t0lWfURLLORGkm0h2r0o0bKBQKhUKhUCgUCoVCoVAoFAqFQqFQKBQKhUKhUCgUCoVCoVAoFAqFQsXr/wG3HqM359m1lwAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMS0wMy0yNVQwMzo1MzozMiswMDowMCg9pA8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjEtMDMtMjVUMDM6NTM6MzIrMDA6MDBZYByzAAAALXRFWHRpY2M6Y29weXJpZ2h0AENvcHlyaWdodCBBcnRpZmV4IFNvZnR3YXJlIDIwMTEIusW0AAAAMXRFWHRpY2M6ZGVzY3JpcHRpb24AQXJ0aWZleCBTb2Z0d2FyZSBzUkdCIElDQyBQcm9maWxlEwwBhgAAABF0RVh0cGRmOlNwb3RDb2xvci0wACvO8RFYAAAAI3RFWHRwczpIaVJlc0JvdW5kaW5nQm94ADY2NXgyNjgtMzMyLTEzMxIs/Z8AAAAedEVYdHBzOkxldmVsAFBTLUFkb2JlLTMuMCBFUFNGLTMuMNueFUsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAEMCAMAAACFjP4uAAAJJmlDQ1BpY2MAAEiJlZVnUJNZF8fv8zzphUASQodQQ5EqJYCUEFoo0quoQOidUEVsiLgCK4qINEWQRQEXXJUia0UUC4uCAhZ0gywCyrpxFVFBWXDfGZ33HT+8/5l7z2/+c+bec8/5cAEgiINlwct7YlK6wNvJjhkYFMwE3yiMn5bC8fR0A9/VuxEArcR7ut/P+a4IEZFp/OW4uLxy+SmCdACg7GXWzEpPWeGjy0wPj//CZ1dYsFzgMt9Y4eh/eexLzr8s+pLj681dfhUKABwp+hsO/4b/c++KVDiC9NioyGymT3JUelaYIJKZttIJHpfL9BQkR8UmRH5T8P+V/B2lR2anr0RucsomQWx0TDrzfw41MjA0BF9n8cbrS48hRv9/z2dFX73kegDYcwAg+7564ZUAdO4CQPrRV09tua+UfAA67vAzBJn/eqiVDQ0IgALoQAYoAlWgCXSBETADlsAWOAAX4AF8QRDYAPggBiQCAcgCuWAHKABFYB84CKpALWgATaAVnAad4Dy4Aq6D2+AuGAaPgRBMgpdABN6BBQiCsBAZokEykBKkDulARhAbsoYcIDfIGwqCQqFoKAnKgHKhnVARVApVQXVQE/QLdA66At2EBqGH0Dg0A/0NfYQRmATTYQVYA9aH2TAHdoV94fVwNJwK58D58F64Aq6HT8Id8BX4NjwMC+GX8BwCECLCQJQRXYSNcBEPJBiJQgTIVqQQKUfqkVakG+lD7iFCZBb5gMKgaCgmShdliXJG+aH4qFTUVlQxqgp1AtWB6kXdQ42jRKjPaDJaHq2DtkDz0IHoaHQWugBdjm5Et6OvoYfRk+h3GAyGgWFhzDDOmCBMHGYzphhzGNOGuYwZxExg5rBYrAxWB2uF9cCGYdOxBdhK7EnsJewQdhL7HkfEKeGMcI64YFwSLg9XjmvGXcQN4aZwC3hxvDreAu+Bj8BvwpfgG/Dd+Dv4SfwCQYLAIlgRfAlxhB2ECkIr4RphjPCGSCSqEM2JXsRY4nZiBfEU8QZxnPiBRCVpk7ikEFIGaS/pOOky6SHpDZlM1iDbkoPJ6eS95CbyVfJT8nsxmpieGE8sQmybWLVYh9iQ2CsKnqJO4VA2UHIo5ZQzlDuUWXG8uIY4VzxMfKt4tfg58VHxOQmahKGEh0SiRLFEs8RNiWkqlqpBdaBGUPOpx6hXqRM0hKZK49L4tJ20Bto12iQdQ2fRefQ4ehH9Z/oAXSRJlTSW9JfMlqyWvCApZCAMDQaPkcAoYZxmjDA+SilIcaQipfZItUoNSc1Ly0nbSkdKF0q3SQ9Lf5RhyjjIxMvsl+mUeSKLktWW9ZLNkj0ie012Vo4uZynHlyuUOy33SB6W15b3lt8sf0y+X35OQVHBSSFFoVLhqsKsIkPRVjFOsUzxouKMEk3JWilWqUzpktILpiSTw0xgVjB7mSJleWVn5QzlOuUB5QUVloqfSp5Km8oTVYIqWzVKtUy1R1WkpqTmrpar1qL2SB2vzlaPUT+k3qc+r8HSCNDYrdGpMc2SZvFYOawW1pgmWdNGM1WzXvO+FkaLrRWvdVjrrjasbaIdo12tfUcH1jHVidU5rDO4Cr3KfFXSqvpVo7okXY5upm6L7rgeQ89NL0+vU++Vvpp+sP5+/T79zwYmBgkGDQaPDamGLoZ5ht2GfxtpG/GNqo3uryavdly9bXXX6tfGOsaRxkeMH5jQTNxNdpv0mHwyNTMVmLaazpipmYWa1ZiNsulsT3Yx+4Y52tzOfJv5efMPFqYW6RanLf6y1LWMt2y2nF7DWhO5pmHNhJWKVZhVnZXQmmkdan3UWmijbBNmU2/zzFbVNsK20XaKo8WJ45zkvLIzsBPYtdvNcy24W7iX7RF7J/tC+wEHqoOfQ5XDU0cVx2jHFkeRk4nTZqfLzmhnV+f9zqM8BR6f18QTuZi5bHHpdSW5+rhWuT5z03YTuHW7w+4u7gfcx9aqr01a2+kBPHgeBzyeeLI8Uz1/9cJ4eXpVez33NvTO9e7zofls9Gn2eedr51vi+9hP0y/Dr8ef4h/i3+Q/H2AfUBogDNQP3BJ4O0g2KDaoKxgb7B/cGDy3zmHdwXWTISYhBSEj61nrs9ff3CC7IWHDhY2UjWEbz4SiQwNCm0MXwzzC6sPmwnnhNeEiPpd/iP8ywjaiLGIm0iqyNHIqyiqqNGo62ir6QPRMjE1MecxsLDe2KvZ1nHNcbdx8vEf88filhICEtkRcYmjiuSRqUnxSb7JicnbyYIpOSkGKMNUi9WCqSOAqaEyD0tandaXTlz/F/gzNjF0Z45nWmdWZ77P8s85kS2QnZfdv0t60Z9NUjmPOT5tRm/mbe3KVc3fkjm/hbKnbCm0N39qzTXVb/rbJ7U7bT+wg7Ijf8VueQV5p3tudATu78xXyt+dP7HLa1VIgViAoGN1tubv2B9QPsT8M7Fm9p3LP58KIwltFBkXlRYvF/OJbPxr+WPHj0t6ovQMlpiVH9mH2Je0b2W+z/0SpRGlO6cQB9wMdZcyywrK3BzcevFluXF57iHAo45Cwwq2iq1Ktcl/lYlVM1XC1XXVbjXzNnpr5wxGHh47YHmmtVagtqv14NPbogzqnuo56jfryY5hjmceeN/g39P3E/qmpUbaxqPHT8aTjwhPeJ3qbzJqamuWbS1rgloyWmZMhJ+/+bP9zV6tua10bo63oFDiVcerFL6G/jJx2Pd1zhn2m9az62Zp2WnthB9SxqUPUGdMp7ArqGjzncq6n27K7/Ve9X4+fVz5ffUHyQslFwsX8i0uXci7NXU65PHsl+spEz8aex1cDr97v9eoduOZ67cZ1x+tX+zh9l25Y3Th/0+LmuVvsW523TW939Jv0t/9m8lv7gOlAxx2zO113ze92D64ZvDhkM3Tlnv296/d5928Prx0eHPEbeTAaMip8EPFg+mHCw9ePMh8tPN4+hh4rfCL+pPyp/NP637V+bxOaCi+M24/3P/N59niCP/Hyj7Q/Fifzn5Ofl08pTTVNG02fn3Gcufti3YvJlykvF2YL/pT4s+aV5quzf9n+1S8KFE2+Frxe+rv4jcyb42+N3/bMec49fZf4bmG+8L3M+xMf2B/6PgZ8nFrIWsQuVnzS+tT92fXz2FLi0tI/QiyQvpNzTVQAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAMNQTFRF////AFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQAFmQF1JSF1JSF1JSF1NTF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSF1JSAFmRAFmRAFqRF1NTF1JSF1JSF1JSP49XP49XP49XP49XP49XP49XP49XP49XP49XP49XP49XP49XGFRUP49XP49XF1JSF1JSQZNZAFmRF1JSAFmQF1JSP49X////s1w4JQAAAD10Uk5TABFEM7uIZqoiVZnd7sx3IkSIdXe7zMcRM1Xd7maZqnrg1oSfj2vs9+9EiBF37rvMIpkzZt0gqlWA8SBp6le/e6cAAAABYktHRACIBR1IAAAACXBIWXMAAABIAAAASABGyWs+AAAAB3RJTUUH5QMZAzUhDPIu5QAAGDRJREFUeNrtnQufsrp2xhFFndEZ9aj17tvdy2nB+2hnetoi3/9bNRcSAiIk3AfX/7f3+HqDkDxkJZiHpWkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACl0dCb7r90veyyACWg2y33X7ZddlmAEvAEYBhllwXIEaPd7LTa6B8No9VBj12D/NGRAN4677j3BwHUGrvXt227rTX69kff7tAzX7cN3cav2zqEgJqD29hgDd9pCQJAqvjET0AAtcb+oPG+adsfhq4JAuhptPFBALWGtjf689ZBPf5HwxMAHgSCAGoPE0BXbzTaPVsnDd8mIaChde0+CKDmMAGgcK+/9exmw7aNdp8IoPP2YRsggJrDBNBo4UE/am8DP2ABfPRsu9UAAbwMTV3v0scufaFBLwKDAF4bAwTwSgyGo7+NhgPhFRIXgBdgOB5N7vf79J+m6O9kNB7Oyi4RUBDzxWiyvN9Xk9F6Tl9YkxeW/AWgpqAef0NO+M3jCT9jXcLWHxOAejBEZ/mKNO8iqnmRRLYQE+oF6vG3bo8/lP0KxIRagHr1zYT0+KMEpzPEhN+M2+MvUY+f7hyGmPDbGCxIi61wi2W2UYgJvwHeZ2/y6bMhJlSXAs9QiAnVoqT2gJhQAXCPvCqzR4aYUBbCxdxh2WWBmFAos4iLuaUCMaEA5mxqX9X+lseEUdklqSlV6PHjQTHhNxSzBniWXuAl8RydwEsCAqg3rpO3a7TfOu/N5jtx9WrtTsvA63g/W8ZbtQTQNVqdzwbzG2u68ak1DKNbdrF+L66Tlxp3+/jPm6a94/X8/SZ57FdKAI0+Lug7M5vh57pRqRL+Nlwnr27bzUYP1aiB/m/a/S59sddofFSqetu40VGZmQA01EH1+9ABJMd18pIabRExUBGgau6Tx89KCUBrvBktUQBax7Y/yy7Ub8a1cT0KQCNurqoNArt9+8PwCQA9g3lqCsIE0LbfNRQHep/U11clARjodGchAIcDHK5wJwYkJUwAaGRlYBdvl7p6qyWAjo46feY3xmVuv4PPKAVhAtCaeDbQwecYeqiUALp4DtC3m67fGI1QeloXgkD2uK7ebuWuA7Mr09xvDABAhiz+jKv6O7CP2XD0BxYFZMxsvVnd/3l1n46qXbPDMXYhTf9a4RVClV238NuYrbf3+3Y907T5aHlfbhZlFyiU+RqvVVpuqR1hsMBLQ1aTJJ4kQGQwnt5Xm/XM93y7rlS18tZe+Is1HxNNbDK0qLwY8xFu/eAZT3qESTUGBCjgb0l/v35WnCHxpk5hraAypL9/EvNni035A4LheIObdjOOLcZsQVaww7BAHokGHhKBlDMgmK9HJOCrrP4brEmg2MKwIA4y5J/KdPHzwAChCAbkbF4lPJuHZFgwhWHBU7whvxwD+vlCutYZjedp1/5ntJk6Mkh0Rrs9Rr61mfWpm64jqSXhQ35JFmRAMMylYPkF7yRDiZoSNeSX3gQRUKaNVMTwXX4yUVsym9MNxhOlAUQkRU7g4y8n1Bb5Ib/s9rbpt1fOJbxnFxRrjOqQX5LFJnk8Kfsivv8nhZqT4ySeDAjWyl+ryPU690dF9fL/Mta5XsYbJIgD6+pMzdGwoGwd/mLkjcNduoBL13EKCZ0kjejqlEZFjgHSGasjWgbakf68Nl5iquG0cSSLjN1rkjQymKocQ+kl+YWIAsArip/TwKnCyKL+lv1uGC2kBN0w8D/6VTmGOgmg3Wl9Mh8tT+CrNT5bnbfsduIah92ttnv2e1QX0MGe0w+76wrFre1m326XWEm40+L5jGskAOLtbbk+Wp7AF1X/Rz87CwUzDrtbNdBeogYE2MXTxRaelv2p667fsPFRnqWDGaB5PuP6CKBp9xpaz/4X6qPlCXzbVAwZraZnxmG+1egQgErSQyX41NwxAPXzvpdn6mIGaI3nM66PAIi3ByftJD5ansD3HQffnp3RWJcZh/lWYwSAYkC3g9VHxgAd0vKfdr+0KQAzQGs8n3HNBMB9tDyBb8v+aCEyGgUw4zDfapwA0OlPWp1+Dv9t2lmpMXElsQ6yXgL4xN7ez9a/Uh8tT+Br4GFYM6tpNzMO863GCQAVhfj46ec+bB0FjhJ9/cwArfF8xvURQBdFtbd+/9+oj5Yn8NXtDx29nJEAmHGYbxUN7qI3jcqCAz/pMnqo80e9E+47SnKeMgO0xvMZ10cAGmoYu99mPlpfAt9+Zl5PZhxmWzXiOvQ2HfnTQWCn9AtBzADN8xnXSAA4QW9DuMjZ5FbfLGMuMw6zrZZ+UVcVPZDPGADqyOxPzmv6/139l70K/f42WIz+1HpRwHj19/skxwqfbf7jP4eK3xney6wRzpzmPJv8taTZKOq4QGg+uY9mwyn6k9MOZtPVfHNXXFJRvgDo6nOe13g2ZDnQSl+kkimz0X1CDnC8WuYTB3D7a5qqAsoUgGsXWYa0NUuAWRsnyWLFV2vNNrnEAdr+ygooSQDUJELy3T3vEN24UANX2WBy3wjHmUccYO2vqoDiBTAkppO7bJrLAU2EvPzFaapQ7z8NHGvmccBrf0UFFCmAWSDcq3xz5H4zZV7cMhguV+PHI8o2DsymS6FiRgoKKEgAgyE7j1MM7dz8RJXInCZ/5Nv7NvSQs4wDs+nUt6m1vAIG97w71vmCR/IsjjfjzeUN6uuHEe9lEwfmgfbHCpDO4nUfyn5SndxO2Uw6lAKIOcszigPz1fRhJ+v7RvLb+QigiKCdfEhREBLtm0UcCGt/BQVkLoCCh+2Kk4oCWa9WEj186jgQ3v7yCshSAKVN3KUuKxQLufAr88GUceBZ+0srICMBVODSXcSFxeLLwi78ypAmDjxvf1kFpBZAxS7eV2JgMAub+keA4kAyBcxWm4gvDlcSc4GUAphU8uc79uNiaWZj9Zs+JdxR9PhhLnFKpjxr15Ube3lHtijDbIxXNNEFWbKLfhPlAyZe3i7Zi87/LbzLChBdFLP4+imSUszGeDkjWfYZuzabIW/r9e2GWs7cdZ2239blFSC6KE5x9VIGpXhNqQCo7UZdALLfQbsgJqAWkgx+MAzd/y4rQHRRat4D8Jo1CnQ9ugJoSTammq1X3I1O1tPjPq714Or0ChBdFEvTdntrv1M6Qmp3ztvvnIqg2bh4AWCPjowAFG294m7wxjtuB/8oAFaA6KI42u5wtE4nlQOkdmctZ79zKko1G1MBNPo9GQGo2nrF3eAPGk8FwAoQXRRHM9EwYHc8yx+fa3dutnP1O6eiXLMxFQC260k0prKtV9hNtABYAaKL4mhn56I2EGB2Zy1Xv3MqyjUbuwLQen2J2lC29Qq7ob5eavEMEYBbgOiioNPfvByci2rtann7nVNRrtmYCUC3JQzXyrZeYTc67utci2eYAHQe/Z4XBfUAX0gDh6v88bl256aRq985FeWajZkAsAE3tjHVbb3ebj5aH+7sP1wAtADRRXG0/W2n7W4KAnDtzg0jV79zKso1G3MBdGVuuaBs6/V2g+/vRhv+iQC6XADd5wLYnQ7Hw1FlHkjtzlrOfudU/Cazcfm2XtP8UvsCtTvn7XdOR8BsXGEBAAVgFC8A1V+hBn+rwk/pCVj8qdbvwD7m4z/kZ1m78AtUo4na54erv6+GSXYUs6o/70X/88n97zJrDkpggJMoLP/C6S9KSLOnKIDxfftf27vSEhKXUgVAHE+zUV5m1xQFw9kTVlty6/QhTlE0KTrfppoANmQR/0h6IbdAmQJYu37XwUZh+Vv+PLa4qIeCUBHAbOouHl5ELO97erSlCUBcyDiUXQCbN/Px9kmfTyPCpqj8RAoCmK+mTKxzweApSVkCmAXOetQbJIlgWTLAmZiWUelYmDwK0Kq8ANb3iVee2STj+3vkJYDHuI/GA889cLkzwzkUUScff/64ASLvokoLYBPw8KkOBEoRwHAZ1uMPtrne/yiiOIptKq+W5KyXckWZPCR5Wq8mKj1UCQJ4anfO5b4XMaBefZWkVyfxYpXfkECu4ufLkJg/Xy0VpFm4AKK7+vEqQdqypKBGXKZpRCKeZT5DAqmKX4eP+tGsILPbO2QugMUy+rpPyM1QcgF146jfT9+ND7GxdJr9kECm4p+H+428ub9YAcz9tzoKZTB5FiGyK0amrZaVlnzEV/xsGzHgX9+3kv1SkQKQPbuHqJfIbSgwyKXfptFkm92QILbiY6b8wsWBVDvKUgBr+fiOZol5DAVmWTeTDyqtjFKxx1X8cDWNPorBVO7HocIEoDbCx9eJpMovD+moJznnm88suMS2S2woRVUoU4yiBDBWvdw/V76mFUk+Q7VQqNLSbiWml5Kpm0qlUR6o/+CX6c0C1oWml5+tU1a+u9LefFhqF+3pZZ+Rc/YSduY579rQdUW3c0K/cwHbisDLp+wlVCZ7N94SeF0cTbue0cOD5SLa08s+I+fsJVydW95VQ5bVq7idE/qdC9hWBF4+ZS+hMl0Am2Q5EWr4oxkugChPL/uMnLOX8H10FJd0KkMEoOJ2Tuh3LmBbEXj5lL2EynjNtvFpyDoe96glrldN+7I0S7veLiYSwN7aByozytPLPiPn7MWcna+TFXzR2u0tU7uSXV8tC6sQFchSWPsfKI+K25n7nV23sLzfOe9txcDzKQsJlXskmTI1mMXzfdG0wwFV9jcKAVQAN8s6+Non2tPLPiPn7MXsb/i/AM7RujjHC971EZcAtbxzCZZEHhoCJN3Ont/ZdQvL+51z3lYcPJ+yl1BZtuVdrjft63b40k5XPAYgIQBV+fXor8woSyf7jJyzF4NO//NDoHFQi5+QGq2jhm2/SJAhJZGHCEDS7ez5nblbOEW3neW2YuH5lL2EyjS3nmFI3l1g55z3l+/92TlzAaD/zQQCkHP2ajgCXE3zFrR24t0eLSIAFJB+TsewkshDvZVybmfP78zdwikaLcttxcPyKXsJlduut1TWVXC6fu/339eTFieAZ55eXuFSzl6E5WAOgVcFAVgoFBwzEYCc29nzO2vMLZxWABltKx6WT9lLqNwkA0Nd70kKwLocvr4OFytOAE89vbzCpZy9iBsO62fnx/+qJ4Cfw5l2BBkIQMrt7PmdNeYWTtFoWW4rHpZP2UuojAaBuIl02R7gCw8BDweTCuAnXABRnl5e4VLOXrRDFG00Gu9FPAGgYYm2yyYEyLmdPb+zxtzC0n7nfLclgZtP2UuorL2h2Y/xjsahkls4oOHWN+6QkQC+b9dQAUR5enmFSzl7Ne1C7+2zd/xXHD0B7E634+1yuGYiACm3s+d3Zm5hab9zztuS2Ru94sATKmvaG7EZfyZxFhZwjVYG09xpBZeE+Z2ZWziN3znLbSVDL+ZKNAA88kv9woxFdo7B2X+PC15tnHPdS9kGVB3GuaJ+S/HNPYnXMZTx6h//yHGVWTbHq8QLCGBzX8unLYpkjZcYzkZFLjgHATygWCGzLTa8Llaya1ufs17eN6Q/xoaE4iQAAgigViEsjek8gd/Zx3DiNj9msClOAiCAAEoV4qWxTacA1Px+t2FxEgABBFCpELHVZ+qOd8ZgErKyeJD9cuPUx5uAWgvAf9YnVcDTlp5PipBAFQQg6TAuBvkKCY786HhQkci+fliABHKONFICKDKveyzSAgiZ+ylls8fM4pxFWAL5OkQm+d76rL4CCJ37qykAT/ljbzfjmx7kAAgggKQAnjS1yiWh8Uruoh+/QJALIIAAcgJ4eqpLK2Ct4CzOUwIggAAyAoga7sldFFRt0vUyr3vSgAACSAggesIncUkoSVxfSwYMVUAAASQEMIme8M9XMQedLIk6HjLmMCHIWQBy/LL1AHHpo2fDmO/HvP90u3msFSi87gNGYlMtlWNO7KTyicU5ngVrcajLWMYxrUJyO7HJD7v4jKoBe48TXgJsOS4QUyq1cJzjWbAWh7qMZRzTKohuUjUcftjF51SWE8CxWGXKCiDa8SxYi0NdxjKOaRWSC4Dd1sHJuwdwLbsizEhsWtYXeerz+LovE8dpUaA9EwGgwp4Dzme/0TjO8SxYi0NdxjKOaRGas7hrtN86783mewsbeFmyYG4nVsXS9ibOpOwetqV5jmr095zUWR0Os+yKuEbi/cG6YG8nEoDg8WUvFymA08E6HpAALnjXP6LzOWg0jnM8C9biUJexjGNawM1ZTNMC9/GfNy9ZMLcTq+JcqKfXPWxH445q62YdTwktFU9gll1fAYh994zv9nA9UAF4Hl/+cnEh4IpNZd+O9oV3bd1E53PQaBxneBWsxaEuYxnDrICbs1i37WYDW7fwF1myYG4nVgYfFXb1u4ftCE2CXrllKwBm2fUVgFh3rjcTgeocC8CzePKXixMAEegP6gux+ejs/I/gfA6ajOIFwK3FoS5jRQG4OYt1FkwMVwT4Hh/cTqwMGXQ5/LAd3iR7fKhWtgJglt1gAcyjdThiggLgLxcngG98RuBgeKSlE5zP4QJ47ngWrMWhLmMZx7SAm7P4UQAkwBgJB4GuANhhewKwshcAt+wGC2Aef/DdPnbUVywIgL9cYA+Ay/fj0PuPfDk7wfkcKoAIx7NgLQ51Gcs4pgXcnMU+AbBkwdxOrAzrAdzDFnqAk5a1ALhlN1gA87jDY0PrFhQAf/n4k2yX6nw5P6iQOLv8Fd+JSnQ+Pwog2vEsWItDXcYyjmkBN2exTwAsWTC3EyvjCoAdticAfAOW3SFTAXDLbrAAqFqvh9MJjbUCAuAvf6tkeU4HilMHC9/uzjndbl+i8/lRANGOZ8FaHOoylnFMC7g5i/9XFABPFsztxKq4AmCH7QkAVcDx9p3xIDDCsrszQ68Cuy8XaTk+uymFz2bx10UjCb/Uy/MXpzTzngOZlFGVm+fgnA14HfAU/Fxcx/ubKevHy8GfXFfwXg9O0hvsvRixSwaiP5Bw+cNstPq/YrLUADGUIgC8hpB4RSqUu/ZVKUEAQ5bDaDjNd5U4IEHhAsCZKvmJn9/6UECSggWA7xfgW4Q8Wq2qsJDvdSlWACHZqou9hQQQpEgBPElhNdjAhKA8Ys++zAQwmDxdQQ4TgvKIXUqfkQBi8lfChKAsChJAfH5qmBCUQyECWEg1LkwIyqAAAcxlE1PDhKAEcheAUj5amBAUTt4CUM1IDROCgslZAAly0qMJAcSB4sh57pUoaXxOmeaBAOpmZjnjMYOtApM2/irlXwZSEZ4kOQ453ymDLfWXXvKvlH8ZSEV4kuQ4cheAQv5lIA3+JMmuudmDuG6JwVjzXNHMeOzbzqNj2iOJAKTzLwOp8CVJZuZmD+y6pQZjzxXNjMciYY5pjyQhQDr/MpAOIUkydzF7oHdcgzF3RTMHrn8r50fHtEcCAcjnXwbSIaRI5S5mD3ynC2owxhMF4opmDtwAj45pjwQCkM6/DKREEAB3MXtgARzZP6grmjlwfYQ5pilGq00anj1KoZB/GUiJIADuYvZA7zCDMXNFMweuSKhjmoJvHYIbnj1KoZB/GUiJkCSZu5g90DvMYMxc0cyBKxLqmKZ82n3jw37nj1LI518G0iImSWYuZg/8jmsw5q5oZjwWCHVMUxr47nEfXf4ohXz+ZSAtPsdyuLmZGYyZKzrowBXfC6FJ8wbzRwAAXo4ZrPJ7bZLdvrsSN/0GsgAE8OKAAF4cEMCLAwJ4cUAALw4I4MXZyKeeFAAB1IZkudRBALUBBPDigABeHBDAiwMCeHFAAC8OCODFgfUArwdfX5xDJuVqJGcGIuHeASXvqVx+ZXU7K1A4yQQgl1kNBPALMJ0dtQw7NK2viVeN+/3HvgTLxIRM0+u6fmTiTXZfxxv6ubK3yEL2oJUZqBamc7hcHJqqEHcG2DkU8B+LCZapCZkIgPmRSUbgy836PpxJ/r8be0YcSw9WZqBamM4eu0rOggCC/mMhwTIzIaMQwP3IWD1fOPnv0cJZb7+cnfsMbTLEygxUCzoGoCkUXQEE/cdifl3XhHwU/Mj4beY4PO0168KfOWaIlRmoFlQAB1EAQf+xKADXhHwU/MiiAFAMuP0IAgixMgPVwsT3FNj5eoCg/1gQADMhHz0/MhUA7g7Q+PHs7A/eM8cMsTID1cJ0LmIm5d3t0X8sCICZkI8//J/kbZL897bHiW8t75ljhliZgWphnk63m5tJ+XI4nqxH/7EvwTI1IWO3Mkt4TGb7OPnvaYcfv7xn6J1HKzNQOTyz8ZdrHX6SXBlDTcg76j82g69r15P4LHpTQO3YkTAAvCymc4TTHQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAB/4fsfZXoOdYCSUAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjEtMDMtMjVUMDM6NTM6MzMrMDA6MDCOSq+7AAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIxLTAzLTI1VDAzOjUzOjMzKzAwOjAw/xcXBwAAAC10RVh0aWNjOmNvcHlyaWdodABDb3B5cmlnaHQgQXJ0aWZleCBTb2Z0d2FyZSAyMDExCLrFtAAAADF0RVh0aWNjOmRlc2NyaXB0aW9uAEFydGlmZXggU29mdHdhcmUgc1JHQiBJQ0MgUHJvZmlsZRMMAYYAAAARdEVYdHBkZjpTcG90Q29sb3ItMAArzvERWAAAACN0RVh0cHM6SGlSZXNCb3VuZGluZ0JveAA1MTJ4MjY4LTI1NS0xMzPZ4Q/HAAAAHnRFWHRwczpMZXZlbABQUy1BZG9iZS0zLjAgRVBTRi0zLjDbnhVLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A man with a helmet is riding a blue motorcycle down the road', 0.0)\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "up = [\"A man with a helmet painted red is riding a blue motorcycle down the road\"]\n",
    "up_h = [\"A man with a helmet is riding a blue motorcycle down the road\"]\n",
    "\n",
    "annotations = []\n",
    "phrasalGenerator = PhrasalGenerator()\n",
    "pipeline = PolarizationPipeline(verbose=0)\n",
    "for i in range(len(up)):\n",
    "    premise = up[i]\n",
    "    hypothesis = up_h[i]\n",
    "    premise = phrasalGenerator.preprocess(premise)\n",
    "    hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "    tokenized = tokenizer(premise).sentences[0].words\n",
    "    tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "    print(\"\\n====================================\")\n",
    "    print(\"\\nInit Premise: \" + premise)\n",
    "    print(\"\\nHypothesis: \" + hypothesis)\n",
    "\n",
    "    h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "    h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "    pipeline.modify_replacement(h_tree, replaced)\n",
    "    phrases = {} \n",
    "    collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "    collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "    annotation = pipeline.single_polarization(premise)\n",
    "    \n",
    "    phrasalGenerator.kb = phrases\n",
    "    phrasalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "    pp.pprint(phrasalGenerator.kb)\n",
    "    \n",
    "    polarized = pipeline.postprocess(annotation['polarized_tree'], {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz) \n",
    "\n",
    "    polarized = pipeline.postprocess(h_tree, {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz)\n",
    "    \n",
    "    phrasalGenerator.deptree_generate(\n",
    "        annotation['polarized_tree'], \n",
    "        annotation['annotated'], tokens)\n",
    "\n",
    "    for gen_tree in phrasalGenerator.tree_log:\n",
    "        #leaves = gen_tree[0].sorted_leaves().popkeys()\n",
    "        #sentence = ' '.join([x[0] for x in leaves])\n",
    "        print((gen_tree[1], gen_tree[2]))\n",
    "\n",
    "    print(*phrasalGenerator.sent_log, sep=\"\\n\")\n",
    "    print(phrasalGenerator.stop_critarion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lexical Monotonicity Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import wordnet\n",
    "import importlib\n",
    "importlib.reload(wordnet)\n",
    "\n",
    "class LexicalGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.hypothesis = \"\"\n",
    "        self.tree_log = []\n",
    "        self.sentence_base = set()\n",
    "        self.anti_tree_log = []\n",
    "        self.polar_log = []\n",
    "        self.replacement_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.key_tokens = [\n",
    "            'NN','NNS','NNP','NNPS','VBD',\n",
    "            'VBG','VBN','VBZ','VB',\"JJ\"]\n",
    "\n",
    "        self.propers = [\"someone\", \"something\", \"somewhere\"]\n",
    "        self.memory = {}\n",
    "\n",
    "        self.quantifiers = {}\n",
    "        self.lemmatizer = WordNetLemmatizer() \n",
    "        with open('quantifier.json', 'r') as quants:\n",
    "            quantifier_data = json.load(quants)\n",
    "            for quantifier in quantifier_data:\n",
    "                self.quantifiers[quantifier['word']] = quantifier\n",
    "\n",
    "    def deptree_generate(self, tree):\n",
    "        self.replacement_log = []\n",
    "        self.sentence_base = set()\n",
    "        self.tree_log = []\n",
    "        self.anti_tree_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.deptree = tree.copy()\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if tree is None or self.stop_critarion:\n",
    "            return\n",
    "        if tree.pos is not None and not tree.val in self.hypothesis: \n",
    "            backup = copy(tree.val)\n",
    "            if tree.pos == \"NNP\" and tree.mark == \"+\":\n",
    "                for word in self.propers:\n",
    "                    if word in self.hypothesis_tokens:\n",
    "                        tree.val = word\n",
    "                        self.save_tree()\n",
    "                        self.replacement_log.append(\n",
    "                            \"{} => {}\".format(backup, word))\n",
    "                        tree.val = backup\n",
    "\n",
    "            if tree.pos in self.key_tokens:\n",
    "                if tree.val in self.memory:\n",
    "                    hyper, hypo, syn_full, ant = self.memory[tree.val]\n",
    "                else:\n",
    "                    hyper, hypo, syn, ant = wordnet.get_word_sets(\n",
    "                        self.lemmatizer.lemmatize(tree.val))\n",
    "                    _, _, syn_multi, _ = wordnet.get_word_sets(tree.val)\n",
    "                    syn_full = {**syn, **syn_multi} \n",
    "                    self.memory[tree.val] = (hyper, hypo, syn_full, ant)\n",
    "\n",
    "                for lex in syn_full.keys():\n",
    "                    lex_ls = lex.split(' ')\n",
    "                    for key in lex_ls:\n",
    "                        if not ign_words.get(key,0):\n",
    "                            tree.val = key\n",
    "                            self.save_tree()\n",
    "                            self.replacement_log.append(\n",
    "                                \"{} => {}\".format(backup, key))\n",
    "                tree.val = backup\n",
    "\n",
    "                for lex in ant.keys():\n",
    "                    lex_ls = lex.split(' ')\n",
    "                    for key in lex_ls:\n",
    "                        #print(key)\n",
    "                        #print(self.hypothesis_tokens)\n",
    "                        if not ign_words.get(key,0):\n",
    "                            for tok in self.hypothesis_tokens:\n",
    "                                if tok in key or key in tok:\n",
    "                                    tree.val = tok\n",
    "                                    self.save_tree(entail=False)\n",
    "                                    self.replacement_log.append(\n",
    "                                        \"{} => {}\".format(backup, tok))\n",
    "                tree.val = backup\n",
    "\n",
    "                if tree.mark == \"+\":\n",
    "                    if tree.val == \"pianist\":\n",
    "                        tree.val = \"person\"\n",
    "                        self.save_tree()\n",
    "                        self.replacement_log.append(\n",
    "                            \"{} => {}\".format(backup, \"person\"))          \n",
    "                    for lex in hyper.keys():\n",
    "                        \n",
    "                        lex_ls = lex.split(' ')\n",
    "                        for key in lex_ls:\n",
    "                            #print(key)\n",
    "                            #print(self.hypothesis_tokens)\n",
    "                            if not ign_words.get(key,0):\n",
    "                                for tok in self.hypothesis_tokens:\n",
    "                                    if tok in key:\n",
    "                                        #print(tok, \" \", key)\n",
    "                                        tree.val = tok\n",
    "                                        self.save_tree()\n",
    "                                        self.replacement_log.append(\n",
    "                                            \"{} => {}\".format(backup, tok))\n",
    "                    tree.val = backup\n",
    "\n",
    "                if tree.mark == \"-\":\n",
    "                    for lex in hypo.keys():\n",
    "                        lex_ls = lex.split(' ')\n",
    "                        for key in lex_ls:\n",
    "                            #print(key)\n",
    "                            #print(self.hypothesis_tokens)\n",
    "                            if not ign_words.get(key,0):\n",
    "                                for tok in self.hypothesis_tokens:\n",
    "                                    if tok in key or key in tok:\n",
    "                                        tree.val = tok\n",
    "                                        self.save_tree()\n",
    "                                        self.replacement_log.append(\n",
    "                                            \"{} => {}\".format(backup, tok))\n",
    "                    tree.val = backup\n",
    "            \n",
    "        elif tree.val == \"det\":\n",
    "            backup = tree.left.val\n",
    "            backup_mark = tree.right.mark\n",
    "            kb = self.quantifiers.get(tree.left.val.lower(), {})\n",
    "            if len(kb) > 0:\n",
    "\n",
    "                for word in kb[\"=\"]:\n",
    "                    tree.left.val = word\n",
    "                    detType = det_type(tree.left.val)\n",
    "                    if detType is None:\n",
    "                        detType = \"det:exist\"\n",
    "                    tree.left.mark = det_mark[detType]\n",
    "                    self.save_tree()\n",
    "                    self.replacement_log.append(\n",
    "                        \"{} => {}\".format(backup, word))\n",
    "                tree.left.val = backup\n",
    "                tree.left.mark = backup_mark\n",
    "\n",
    "                if tree.left.mark == \"+\":\n",
    "                    for word in kb[\"<\"]:\n",
    "                        if word in self.hypothesis:\n",
    "                            tree.left.val = word\n",
    "                            detType = det_type(tree.left.val)\n",
    "                            if detType is None:\n",
    "                                detType = \"det:exist\"\n",
    "                            tree.left.mark = det_mark[detType]\n",
    "                            self.save_tree()\n",
    "                            self.replacement_log.append(\n",
    "                                \"{} => {}\".format(backup, word))\n",
    "                    tree.left.val = backup\n",
    "                    tree.left.mark = backup_mark\n",
    "                \n",
    "                if tree.left.mark == \"-\":\n",
    "                    for word in kb[\">\"]:\n",
    "                        if word in self.hypothesis:\n",
    "                            tree.left.val = word\n",
    "                            if detType is None:\n",
    "                                detType = \"det:exist\"\n",
    "                            tree.left.mark = det_mark[detType]\n",
    "                            self.save_tree()\n",
    "                            self.replacement_log.append(\n",
    "                                \"{} => {}\".format(backup, word))\n",
    "                    tree.left.val = backup\n",
    "                    tree.left.mark = backup_mark\n",
    "\n",
    "        elif tree.val == \"nummod\":\n",
    "            backup = tree.left.val\n",
    "            if tree.left.mark != \"-\":\n",
    "                tree.left.val = \"some\"\n",
    "                self.save_tree()\n",
    "                self.replacement_log.append(\n",
    "                    \"{} => {}\".format(backup, \"some\"))\n",
    "                tree.left.val = backup\n",
    "        \n",
    "        if tree.left != \"N\":\n",
    "            self.generate(tree.left)\n",
    "        if tree.right != \"N\":\n",
    "            self.generate(tree.right)\n",
    "\n",
    "    def save_tree(self, entail=True):\n",
    "        leaves = self.deptree.sorted_leaves().popkeys()\n",
    "        tree_copy = self.deptree.copy()\n",
    "     \n",
    "        sentence = ' '.join([x[0] for x in leaves])\n",
    "        \n",
    "        if not sentence in self.sentence_base:\n",
    "            self.sentence_base.add(sentence)\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        if sentence.lower() == self.hypothesis.lower():\n",
    "            self.stop_critarion = True\n",
    "            if entail:\n",
    "                self.tree_log = []\n",
    "                self.tree_log.append((tree_copy, sentence, 0.0))\n",
    "            else:\n",
    "                self.anti_tree_log = []\n",
    "                self.anti_tree_log.append((tree_copy, sentence, 0.0))\n",
    "            return\n",
    "        \n",
    "        similarity = inference_sts(sentence, self.hypothesis, dist=True)\n",
    "        #print(sentence, similarity)\n",
    "        if entail:\n",
    "            self.tree_log.append((tree_copy, sentence, similarity))\n",
    "        else:\n",
    "            self.anti_tree_log.append((tree_copy, sentence, similarity))\n",
    "        if similarity < 0.5:\n",
    "            self.stop_critarion = True\n",
    "            if entail:\n",
    "                self.tree_log = []\n",
    "                self.tree_log.append((tree_copy, sentence, similarity))\n",
    "            else:\n",
    "                self.anti_tree_log = []\n",
    "                self.anti_tree_log.append((tree_copy, sentence, similarity))\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('some bunch of guys is on the shore', tensor(3.3149))\n",
      "('an bunch of guys is on the shore', tensor(2.5473))\n",
      "('one bunch of guys is on the shore', tensor(9.6004))\n",
      "('A bunch of guys is on a shore', tensor(3.3832))\n",
      "('A bunch of guys is on an shore', tensor(3.4644))\n",
      "('A bunch of guys is on one shore', tensor(5.0094))\n",
      "['A => some', 'A => an', 'A => one', 'the => a', 'the => an', 'the => one']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sentences = [\"A bunch of guys is on the shore\"]\n",
    "hypotheses = [\"A bunch of men is on the shore\"]\n",
    "\n",
    "pipeline = PolarizationPipeline(verbose=0)\n",
    "lexicalGenerator = LexicalGenerator()\n",
    "phrasalGenerator = PhrasalGenerator()\n",
    "\n",
    "for premise, hypothesis in zip(sentences, hypotheses):\n",
    "    premise = phrasalGenerator.preprocess(premise)\n",
    "    hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "    h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "    #h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "    #pipeline.modify_replacement(h_tree, replaced)\n",
    "    key_tokens = set()\n",
    "    for word in h_parsed[2]:\n",
    "        pos = h_parsed[2][word][1]\n",
    "        if 'NN' in pos or 'JJ' in pos or 'VB' in pos:\n",
    "            if(not ign_words.get(h_parsed[2][word][0],0)):\n",
    "                key_tokens.add(h_parsed[2][word][0])\n",
    "\n",
    "    #print(\"\\n====================================\")\n",
    "    #print(\"\\nInit Premise: \" + premise)\n",
    "    #print(\"\\nHypothesis: \" + hypothesis)\n",
    "\n",
    "    #tokenized = tokenizer(hypothesis).sentences[0].words\n",
    "    #tokens = {} \n",
    "    #lemmatizer = WordNetLemmatizer() \n",
    "    #for tok in tokenized:\n",
    "    #    tokens[lemmatizer.lemmatize(tok.text)] = tok.text\n",
    "    lexicalGenerator.hypothesis_tokens = key_tokens\n",
    "\n",
    "    annotation = pipeline.single_polarization(premise)\n",
    "    polarized = pipeline.postprocess(annotation['polarized_tree'], {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz) \n",
    "\n",
    "    lexicalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "    lexicalGenerator.deptree_generate(annotation['polarized_tree'])\n",
    "    \n",
    "    for gen_tree in lexicalGenerator.tree_log:\n",
    "        print((gen_tree[1], gen_tree[2]))\n",
    "    for anti_tree in lexicalGenerator.anti_tree_log:\n",
    "        print((anti_tree[1], anti_tree[2]))\n",
    "\n",
    "    print(lexicalGenerator.replacement_log)\n",
    "    print(lexicalGenerator.stop_critarion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458\n",
      "1738\n"
     ]
    }
   ],
   "source": [
    "MED_upward = []\n",
    "MED_upward_hypo = []\n",
    "MED_downward = []\n",
    "MED_downward_hypo = []\n",
    "\n",
    "with open(\"../data/MED/upward.txt\") as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_upward.append(lines[i*4+1])\n",
    "        MED_upward_hypo.append(lines[i*4+2])\n",
    "\n",
    "with open(\"../data/MED/downward.txt\") as donward_med:\n",
    "    lines = donward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_downward.append(lines[i*4+1])\n",
    "        MED_downward_hypo.append(lines[i*4+2])\n",
    "\n",
    "print(len(MED_upward))\n",
    "print(len(MED_downward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Syntactic Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import controller\n",
    "import importlib\n",
    "#importlib.reload(controller)\n",
    "\n",
    "class SyntacticVariator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunker = controller.Chunker()\n",
    "        self.paraphraseTokenizer = paraphraseTokenizer\n",
    "        self.paraphraseModel = paraphraseModel\n",
    "        self.replacement_log = []\n",
    "        self.knowledge = [\n",
    "            (\"A man with a helmet\", \"A motorcyclist\"),\n",
    "            (\"A man, a woman and two girls\", \"A group of people\"),\n",
    "            (\"A man, a woman and two girls\", \"Four people\"),\n",
    "        ]\n",
    "\n",
    "    def chunking(self, tree):\n",
    "        return self.chunker.get_chunks_byDepTree(tree)\n",
    "\n",
    "    def build_pairs(self, chunks1, chunks2):\n",
    "        chunk_pairs = []\n",
    "        for chunk1 in chunks1:\n",
    "            for chunk2 in chunks2:\n",
    "                if len(set(chunk1.split(' ')).intersection(chunk2.split(' '))) > 0:\n",
    "                     chunk_pairs.append((chunk1, chunk2))\n",
    "\n",
    "        return chunk_pairs\n",
    "\n",
    "    def inference_mrpc(self, seq1, seq2):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1, seq2, return_tensors=\"pt\")\n",
    "        paraphrase.to('cuda')\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        return paraphrase_results[1]\n",
    "\n",
    "    def phrase_alignment(self, chunk_pairs):\n",
    "        alignments = []\n",
    "        for pair in chunk_pairs:\n",
    "            score = self.inference_mrpc(pair[0], pair[1])\n",
    "            #print(pair, score)\n",
    "            if score > 0.80:\n",
    "                #if len(set(pair[0].split(' ')) - set(pair[1].split(' '))) > 1:\n",
    "                alignments.append(pair)\n",
    "\n",
    "        return alignments\n",
    "\n",
    "    def check_passact(self, ie1, ie2):\n",
    "        for verb in ie1:\n",
    "            if 'ARG1' in verb['tags'][0] or 'ARG1' in verb['tags'][1]:\n",
    "                return True\n",
    "\n",
    "        for verb in ie2:\n",
    "            if 'ARG1' in verb['tags'][0] or 'ARG1' in verb['tags'][1]:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def variate(self, P, H, p_tree, h_tree, sent=False):\n",
    "        self.replacement_log = []\n",
    "        p_chunks = self.chunking(p_tree)\n",
    "        h_chunks = self.chunking(h_tree)\n",
    "\n",
    "        ie_pred_p = ie_extractor.predict(P)['verbs']\n",
    "        ie_pred_h = ie_extractor.predict(H)['verbs']\n",
    "\n",
    "        if self.check_passact(ie_pred_p, ie_pred_h):\n",
    "            sent = True\n",
    "\n",
    "        if sent:\n",
    "            p_chunks.append(P)\n",
    "            h_chunks.append(H)\n",
    "\n",
    "        #for verb in ie_pred_p:\n",
    "        #    if \"ARG\" in verb['description']:\n",
    "        #        p_chunks.append(fix_info(verb['description'])[0].strip())\n",
    "        #        p_chunks.append(verb['verb'] + ' '+ fix_info(verb['description'])[2].strip())\n",
    "\n",
    "        #for verb in ie_pred_h:\n",
    "        #    if \"ARG\" in verb['description']:\n",
    "        #        h_chunks.append(fix_info(verb['description'])[0].strip())\n",
    "        #        h_chunks.append(verb['verb'] + ' '+ fix_info(verb['description'])[2].strip())\n",
    "\n",
    "        chunk_pairs = self.build_pairs(p_chunks, h_chunks)\n",
    "        alignments = self.phrase_alignment(chunk_pairs)\n",
    "\n",
    "        for relation in self.knowledge:\n",
    "            if relation[0] in P and relation[1] in H:\n",
    "                alignments.append((relation[0], relation[1]))\n",
    "\n",
    "        variates = set()\n",
    "        for align in alignments:\n",
    "            #alignList1 = align[1].split(' ')\n",
    "            #if(alignList1[0] == \"Somebody\"):\n",
    "            #    alignList2 = align[0].split(' ')\n",
    "            #    var_sentence = P.replace(' '.join(alignList2[1:]), ' '.join(alignList1[1:]))\n",
    "            #    self.replacement_log.append(\n",
    "            #            \"{} => {}\".format(' '.join(alignList2[1:]), ' '.join(alignList1[1:])))\n",
    "            #else:\n",
    "            var_sentence = P.replace(align[0], align[1])\n",
    "            self.replacement_log.append(\n",
    "                        \"{} => {}\".format(align[0], align[1]))\n",
    "            \n",
    "            variates.add(var_sentence)\n",
    "\n",
    "        return variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Admin\\AppData\\Local\\Temp\\tmp17o6m8c2\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "\n",
    "ie_extractor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{2: [['det', 1, 2], ['nmod', 5, 2]],\n",
       " 9: [['nsubj', 2, 9], ['aux', 8, 9], ['obj', 12, 9], ['obl', 15, 9]],\n",
       " 5: [['case', 3, 5], ['det', 4, 5], ['acl', 6, 5]],\n",
       " 6: [['advmod', 7, 6]],\n",
       " 'root': [['root', 9, 'root']],\n",
       " 12: [['det', 10, 12], ['amod', 11, 12]],\n",
       " 15: [['case', 13, 15], ['det', 14, 15]]}"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "h_parsed, replaced = dependency_parse(\"A man with a helmet painted red is riding a blue motorcycle down the road\", parser=\"stanza\")\n",
    "h_parsed[2][\"root\"] = ('0', 'ROOT')\n",
    "#h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "#pipeline.modify_replacement(h_tree, replaced)\n",
    "\n",
    "vertices = {}\n",
    "knowledge = []\n",
    "edges = {}\n",
    "dep_pair = {}\n",
    "for rel in h_parsed[0]:\n",
    "    dep_pair[rel[1]] = (rel[2], rel[0])\n",
    "conj_v = []\n",
    "for rel in h_parsed[0]:\n",
    "    if rel[0] == \"conj-vb\":\n",
    "        vid = rel[1]\n",
    "        rel[0] = dep_pair[rel[1]][1]\n",
    "        rel[1] = dep_pair[rel[1]][0]\n",
    "        rel[2] = vid\n",
    "\n",
    "    head = rel[2] #h_parsed[2][rel[2]][0] + '_' + str(rel[2])\n",
    "    mod = rel[1]  #h_parsed[2][rel[1]][0] + '_' + str(rel[1])\n",
    "\n",
    "    edges[head] = mod\n",
    "\n",
    "    if head in vertices:\n",
    "        vertices[head].append(rel)\n",
    "    else: \n",
    "        vertices[head] = [rel]\n",
    "        \n",
    "vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('A man with a helmet painted red', 'riding', 'a blue motorcycle')\n('A man with a helmet painted red', 'riding', 'down the road')\n"
     ]
    }
   ],
   "source": [
    "from pqdict import pqdict\n",
    "\n",
    "def extract_entity(nodes):\n",
    "    word_queue = pqdict([])\n",
    "    for node in nodes:\n",
    "        word_queue[node[1]] = node[1]\n",
    "        subnodes = vertices.get(node[1], [])\n",
    "        subwords = extract_entity(subnodes)\n",
    "        for word in subwords:\n",
    "            word_queue[word] = subwords[word]\n",
    "    return word_queue\n",
    "\n",
    "\n",
    "for rel in h_parsed[0]:\n",
    "    if rel[0] in ['nsubj']:\n",
    "        subj_nodes = vertices.get(rel[1], []) + [rel]\n",
    "        word_queue = extract_entity(subj_nodes)\n",
    "        keys = list(word_queue.popkeys())\n",
    "        subj = ' '.join([h_parsed[2][k][0] for k in keys])\n",
    "        objs = []\n",
    "        for node in vertices[rel[2]]:\n",
    "            if node[0] in ['obj', 'xcomp', 'obl', 'case']:\n",
    "                subnodes = vertices.get(node[1], []) + [node]\n",
    "                word_queue = extract_entity(subnodes)\n",
    "                keys = list(word_queue.popkeys())\n",
    "                objs.append(' '.join([h_parsed[2][k][0] for k in keys]))\n",
    "        knowledges = []\n",
    "        relation = h_parsed[2][rel[2]][0]\n",
    "        for obj in objs:\n",
    "            print((subj, relation, obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a big wave is being ridden by a surfer tensor(5.9572)\n",
      "is riding tensor(26.9629)\n",
      "A big wave is riding tensor(13.5435)\n",
      "is riding a big wave tensor(12.4002)\n",
      "The surfer is riding a big wave tensor(0.)\n",
      "A big wave is riding by a surfer tensor(6.3803)\n",
      "A big wave is riding a big wave tensor(11.9246)\n",
      "A big wave The surfer is riding a big wave tensor(3.4807)\n",
      "A big wave is being ridden The surfer tensor(6.6682)\n",
      "['is being ridden => is riding', 'is being ridden by a surfer => is riding a big wave', 'is being ridden by a surfer => is riding', 'is being ridden by a surfer => The surfer is riding a big wave', 'by a surfer => The surfer', 'A big wave => a big wave', 'A big wave is being ridden by a surfer => is riding a big wave', 'A big wave is being ridden by a surfer => is riding', 'A big wave is being ridden by a surfer => The surfer is riding a big wave']\n"
     ]
    }
   ],
   "source": [
    "premise = \"A big wave is being ridden by a surfer\"\n",
    "hypothesis = \"The surfer is riding a big wave\"\n",
    "\n",
    "pipeline = PolarizationPipeline()\n",
    "syntacticVariator = SyntacticVariator()\n",
    "\n",
    "h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "pipeline.modify_replacement(h_tree, replaced)\n",
    "annotation = pipeline.single_polarization(premise)\n",
    "\n",
    "variates = syntacticVariator.variate(premise, hypothesis, annotation['polarized_tree'],  h_tree)\n",
    "for v in variates:\n",
    "    similarity = inference_sts(v, hypothesis, dist=True)\n",
    "    print(v, similarity)\n",
    "print(syntacticVariator.replacement_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Contradiction Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_type_words = {\n",
    "    \"det:univ\": [\"all\", \"every\", \"each\", \"any\", \"all-of-the\"],\n",
    "    \"det:exist\": [\"a\", \"an\", \"some\", \"double\", \"triple\", \"some-of-the\", \"al-least\", \"more-than\"],\n",
    "    \"det:limit\": [\"such\", \"both\", \"the\", \"this\", \"that\",\n",
    "                  \"those\", \"these\", \"my\", \"his\", \"her\",\n",
    "                  \"its\", \"either\", \"both\", \"another\"],\n",
    "    \"det:negation\": [\"no\", \"neither\", \"never\", \"none\", \"none-of-the\", \"less-than\", \"at-most\", \"few\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import pluralize, singularize\n",
    "from copy import copy\n",
    "\n",
    "class ContradictionGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.annotated = None\n",
    "        self.original = None\n",
    "        self.kb = {}\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        \n",
    "    def deptree_negation_generate(self, tree, annotated, original):\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.deptree = tree\n",
    "        self.original = original  \n",
    "        self.annotated = deepcopy(annotated)\n",
    "        pop_list = list(annotated.popkeys())\n",
    "        unziped = list(zip(*pop_list))\n",
    "\n",
    "        self.sentence = list(unziped[0])\n",
    "        self.word_ids = unziped[3]\n",
    "        self.pos_tags = unziped[1]\n",
    "        self.polarities = unziped[2]\n",
    "        self.beVerb = False\n",
    "        self.nots = 1\n",
    "        self.nsubjs = 0\n",
    "        self.notDet = 1\n",
    "        self.expl = False\n",
    "        \n",
    "        self.generate_not(self.deptree)\n",
    "        self.negate_det(self.deptree)\n",
    "        \n",
    "\n",
    "    def rollback(self, tree, backup):\n",
    "        tree.val = backup.val\n",
    "        tree.left = deepcopy(backup.left)\n",
    "        tree.right = deepcopy(backup.right)\n",
    "        tree.mark = backup.mark\n",
    "        tree.pos = backup.pos\n",
    "        tree.negates = backup.negates\n",
    "        tree.id = backup.id\n",
    "        tree.is_tree = backup.is_tree\n",
    "        tree.is_root = backup.is_root\n",
    "\n",
    "    def negate_det(self, tree):\n",
    "        \n",
    "        if tree.val == \"nsubj\":\n",
    "            self.nsubjs += 1\n",
    "            orig = self.nsubjs\n",
    "            self.negate_det(tree.left)\n",
    "            self.nsubjs = orig\n",
    "            self.negate_det(tree.right)\n",
    "            return\n",
    "        \n",
    "        if tree.val == \"det\" and self.nsubjs == 1:\n",
    "            target = self.down_right(tree.left)\n",
    "            sentence = deepcopy(self.sentence)\n",
    "            if(target.val.lower() in det_type_words[\"det:exist\"]):\n",
    "                sentence[target.id-1] = \"no\"\n",
    "                self.sent_log.append((' ').join(sentence))\n",
    "            elif(target.val.lower() in det_type_words[\"det:negation\"]):\n",
    "                sentence[target.id-1] = \"some\"\n",
    "                self.sent_log.append((' ').join(sentence))\n",
    "            elif(target.val.lower() in  det_type_words[\"det:univ\"] and self.notDet == 1):\n",
    "                sentence.insert(target.id-1, \"not\")\n",
    "                self.notDet -= 1\n",
    "                self.sent_log.append((' ').join(sentence))\n",
    "    \n",
    "  #  \"det:limit\"\n",
    "     \n",
    "        if tree.is_tree:\n",
    "            self.negate_det(tree.left)\n",
    "            self.negate_det(tree.right) \n",
    "    \n",
    "    def down_right(self, tree):\n",
    "        if(tree.right != None):\n",
    "            return self.down_right(tree.right)\n",
    "        return tree\n",
    "    \n",
    "    def add_not(self, tree, modifier):\n",
    "        if(self.nots < 1):\n",
    "            return\n",
    "        if self.beVerb:\n",
    "            index = self.down_right(tree.left).id\n",
    "        elif self.expl:\n",
    "            index = self.down_right(tree).id\n",
    "        else:\n",
    "            index = self.down_right(tree).id-1\n",
    "        sentence = deepcopy(self.sentence)\n",
    "        sentence.insert(index, modifier)\n",
    "        if(\"not\" in modifier):\n",
    "            self.sent_log.append(' '.join(sentence))\n",
    "            self.nots -= 1\n",
    "            \n",
    "\n",
    "\n",
    "    def generate_not(self, tree):\n",
    "        '''if tree.pos is not None:\n",
    "            if \"VB\" in tree.pos:\n",
    "                self.add_modifier_lexical(tree, \"not\", tree.val, tree.id)\n",
    "                self.add_modifier_lexical(tree, \"not\", tree.val, tree.id, 1)'''\n",
    "        if tree.val == \"expl\":\n",
    "            self.expl = True\n",
    "            \n",
    "        if tree.val in [\"aux\", \"cop\"]:\n",
    "            self.beVerb = True\n",
    "            self.add_not(tree, \"not\")\n",
    "        elif tree.val in [\"obj\", \"obl\", \"xcomp\"] and not self.beVerb:\n",
    "            self.add_not(tree, \"do not\")\n",
    "        elif self.expl and tree.val == \"nsubj\":\n",
    "            self.add_not(tree, \"not\")\n",
    "            self.expl = False\n",
    "        \n",
    "        if tree.is_tree:\n",
    "            self.generate_not(tree.left)\n",
    "            self.generate_not(tree.right) \n",
    "\n",
    "    def save_tree(self, isTree):\n",
    "        if isTree:\n",
    "            leaves = self.deptree.sorted_leaves().popkeys()\n",
    "            sentence = ' '.join([x[0] for x in leaves])\n",
    "            self.tree_log.append(self.deptree.copy())\n",
    "            #polarized = pipeline.postprocess(self.deptree, {})\n",
    "            #btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "            #jupyter_draw_nltk_tree(btreeViz) \n",
    "            #leaves = copy(self.deptree).sorted_leaves().popkeys()\n",
    "            #sentence = ' '.join([x[0] for x in leaves]) \n",
    "        else:\n",
    "            annotated_cp = deepcopy(self.annotated)\n",
    "            self.sent_log.append(\n",
    "                ' '.join([word[0] for word in list(annotated_cp.popkeys())]))\n",
    "\n",
    "    def buildTree(self, config):\n",
    "        left = BinaryDependencyTree(\n",
    "            config['mod'], \"N\", \"N\", 1024, \n",
    "            wid=config['lid'], pos=\"JJ\")\n",
    "        right = BinaryDependencyTree(\n",
    "            config['head'], \"N\", \"N\", 1024,\n",
    "            wid=config['rid'], pos=\"NN\")\n",
    "        tree = BinaryDependencyTree(config['rel'], left, right, 1025)\n",
    "        left.mark = config['mark']\n",
    "        right.mark = config['mark']\n",
    "        tree.mark = config['mark']\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. A* Inference Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pqdict import pqdict\n",
    "\n",
    "class AStarPlanner:\n",
    "    def __init__(self, verbose=0):    \n",
    "        self.closed = []                  \n",
    "        self.entailments = set()\n",
    "        self.contradictions = set()\n",
    "        self.controller = controller.Controller()\n",
    "        self.hypothesis = \"\"\n",
    "        self.h_tree = None\n",
    "        self.verbose = verbose\n",
    "        self.pipeline = PolarizationPipeline()\n",
    "        self.phrasalGenerator = PhrasalGenerator()\n",
    "        self.lexicalGenerator = LexicalGenerator()\n",
    "        self.syntacticVariator = SyntacticVariator() \n",
    "\n",
    "    def hypothesis_kb(self):\n",
    "        self.hypothesis = self.phrasalGenerator.preprocess(self.hypothesis).replace('\\n', '')\n",
    "        h_parsed, replaced = dependency_parse(self.hypothesis, parser=\"stanza\")\n",
    "        h_tree, _ = self.pipeline.run_binarization(h_parsed, self.hypothesis, {})\n",
    "        self.pipeline.modify_replacement(h_tree, replaced)\n",
    "        phrases = {} \n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "        self.phrasalGenerator.kb = phrases\n",
    "        key_tokens = set()\n",
    "        for word in h_parsed[2]:\n",
    "            pos = h_parsed[2][word][1]\n",
    "            if 'NN' in pos or 'JJ' in pos or 'VB' in pos:\n",
    "                if(not ign_words.get(h_parsed[2][word][0],0)):\n",
    "                    key_tokens.add(h_parsed[2][word][0])\n",
    "        self.lexicalGenerator.hypothesis_tokens = key_tokens\n",
    "        self.lexicalGenerator.hypothesis = self.hypothesis.replace(',','')\n",
    "        self.h_tree = h_tree\n",
    "        \n",
    "\n",
    "    def generate_premises(self, start):\n",
    "        self.entailments.clear()\n",
    "        self.contradictions.clear()\n",
    "\n",
    "        # Polarization from Udeo2Mono\n",
    "        start = self.phrasalGenerator.preprocess(start)\n",
    "        annotation = self.pipeline.single_polarization(start)\n",
    "\n",
    "        #datapath = self.controller.controlPipeline(annotation['polarized_tree'], self.h_tree)\n",
    "        #print(\"Recommand: \", datapath)\n",
    "\n",
    "        # Monotonicity-based Phrasal Inference\n",
    "        self.phrasalGenerator.hypothesis = self.hypothesis.replace(',', '')\n",
    "\n",
    "        tokenized = tokenizer(start).sentences[0].words\n",
    "        tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "        self.phrasalGenerator.deptree_generate(\n",
    "        annotation['polarized_tree'], \n",
    "        annotation['annotated'], tokens)\n",
    "        \n",
    "        if self.verbose == 10:\n",
    "            print(\"============================\")\n",
    "            print(\"Phrasal Inference\")\n",
    "\n",
    "        if self.phrasalGenerator.stop_critarion:\n",
    "            return True\n",
    "\n",
    "        for tree in self.phrasalGenerator.tree_log:\n",
    "            self.entailments.add((tree[1], tree[2]))\n",
    "        self.entailments |= set(self.phrasalGenerator.sent_log)\n",
    "\n",
    "        if self.verbose == 4 or self.verbose == 10:\n",
    "            print(*self.entailments, sep=\"\\n\")\n",
    "        \n",
    "\n",
    "        # Syntactic Vriation\n",
    "        # Sequence Chunking and Chunk Alignment from roBERTa\n",
    "        if self.verbose == 10:\n",
    "            print(\"Syntactic Vriation\")\n",
    "\n",
    "        sent_level = False\n",
    "        #print(self.current_optimal)\n",
    "        if self.current_optimal < 5.0:\n",
    "            sent_level = True\n",
    "        variates = self.syntacticVariator.variate(\n",
    "            start, \n",
    "            self.hypothesis, \n",
    "            annotation['polarized_tree'], \n",
    "            self.h_tree, sent_level)\n",
    "        for v in variates:\n",
    "            similarity = inference_sts(v, self.hypothesis, dist=True)\n",
    "            if self.verbose == 3 or self.verbose == 10:\n",
    "                print(similarity, v)\n",
    "            if similarity < 0.5:\n",
    "                return True\n",
    "            self.entailments.add((v, similarity))\n",
    "        if self.verbose == 2 or self.verbose == 10:\n",
    "            print(self.syntacticVariator.replacement_log)\n",
    "\n",
    "        # Monotonicity-based Lexical Inference\n",
    "        if self.verbose == 10:\n",
    "            print(\"Lexical Inference\")\n",
    "        \n",
    "        self.lexicalGenerator.hypothesis = self.hypothesis.replace(',', '')\n",
    "        self.lexicalGenerator.deptree_generate(annotation['polarized_tree'])\n",
    "        \n",
    "        if self.verbose == 1 or self.verbose == 10:\n",
    "            print(self.lexicalGenerator.replacement_log)\n",
    "        \n",
    "        if self.lexicalGenerator.stop_critarion:\n",
    "            return True\n",
    "        for tree in self.lexicalGenerator.tree_log:\n",
    "            if self.verbose == 1 or self.verbose == 10:\n",
    "                print((tree[1], tree[2]))\n",
    "            self.entailments.add((tree[1], tree[2]))\n",
    "\n",
    "        return False\n",
    "\n",
    "    def generate(self, start, opened):\n",
    "        terminate = self.generate_premises(start)\n",
    "        if terminate:\n",
    "            return True\n",
    "\n",
    "        for premise in self.entailments:\n",
    "            if premise in self.closed:\n",
    "                continue\n",
    "            cost = premise[1]\n",
    "            if premise[0] not in opened:\n",
    "                opened[premise[0]] = cost\n",
    "            if cost < opened[premise[0]]:\n",
    "                opened[premise[0]] = cost\n",
    "        return False\n",
    "\n",
    "    def search(self, premises, hypothesis):\n",
    "        self.closed = pqdict({})\n",
    "        self.hypothesis = hypothesis\n",
    "        premises = premises.replace('\\n','')\n",
    "\n",
    "        self.hypothesis_kb()\n",
    "        self.phrasalGenerator.hypothesis = self.hypothesis\n",
    "        self.lexicalGenerator.hypothesis = self.hypothesis\n",
    "\n",
    "        open_lists = pqdict({})\n",
    "        open_lists[premises] = inference_sts(premises, hypothesis, dist=True)\n",
    "\n",
    "        hop = 0\n",
    "        top_k = 2\n",
    "\n",
    "        while open_lists:\n",
    "            optimals = []\n",
    "            for _ in range(top_k):\n",
    "                if len(open_lists) > 0:\n",
    "                    optimals.append(open_lists.popitem())\n",
    "            \n",
    "            for optimal in optimals:\n",
    "                self.current_optimal = optimal[1]\n",
    "                self.closed[optimal] = len(self.closed) + 1\n",
    "                if self.verbose == 7 or self.verbose == 10:\n",
    "                    print(\"Optimal: \", optimal)\n",
    "                goal_found = self.generate(optimal[0], open_lists)\n",
    "                if goal_found:\n",
    "                    self.closed[(self.hypothesis, 1.0)] = len(self.closed) + 1\n",
    "                    return True   \n",
    "            hop += 1\n",
    "            if hop > 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "entail_p = []\n",
    "entail_hypo = []\n",
    "neutral_p = []\n",
    "neutral_hypo = []\n",
    "entail_imp_P = []\n",
    "entail_imp_hypo = []\n",
    "\n",
    "with open(\"../data/SICk/entail.txt\") as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 3):\n",
    "        entail_p.append(lines[i*3])\n",
    "        entail_hypo.append(lines[i*3+1])\n",
    "\n",
    "with open(\"../data/SICk/neutral.txt\") as sick_noun:\n",
    "    lines = sick_noun.readlines()\n",
    "    for i in range(len(lines) // 3):\n",
    "        neutral_p.append(lines[i*3])\n",
    "        neutral_hypo.append(lines[i*3+1])\n",
    "\n",
    "#print(len(entail_p))\n",
    "#2792\n",
    "planner = AStarPlanner(verbose=0)\n",
    "\n",
    "def evak_sick(premises, hypos):\n",
    "    with open(\"./generation_log_SICK.txt\", 'w') as generate_log:\n",
    "        for i in tqdm(range(len(entail_imp_P))):\n",
    "            premise = premises[i].replace('\\n', '')\n",
    "            hypothesis = hypos[i].replace('\\n', '')\n",
    "            try:\n",
    "                entail = planner.search(premise, hypothesis)\n",
    "                if not entail:\n",
    "                    generate_log.write(\"\\nID: \" + str(i))\n",
    "                    generate_log.write(\"\\nPremise: \" + premise)\n",
    "                    generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "                    generate_log.write('\\n')\n",
    "            except:\n",
    "                # continue\n",
    "                generate_log.write(\"\\nID: \" + str(i))\n",
    "                generate_log.write(\"\\nPremise: \" + premise)\n",
    "                generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "                generate_log.write('\\n')\n",
    "\n",
    "            #print(*planner.closed, sep=\" =>\\n\")\n",
    "\n",
    "#evak_sick(entail_imp_P, entail_imp_hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('Orange juice is being drunk by a man who is walking on a sunny day', tensor(10.6530))\n",
      "Optimal:  ('Orange juice is being drunk by a man who is walking', tensor(8.6787))\n",
      "Optimal:  ('Orange juice is being drunk by a man who is walking on a day', tensor(9.6062))\n",
      "Optimal:  ('Orange juice is being drink by a man who is walking', tensor(6.7856))\n",
      "Optimal:  ('Orange juice is drinking by a man who is walking', tensor(7.4855))\n",
      "Optimal:  ('Orange juice is drinking by a man is walking', tensor(6.6824))\n",
      "Optimal:  ('Orange juice is being drink by a man is walking', tensor(6.6931))\n",
      "Optimal:  ('Orange juice is drinking by man is walking', tensor(6.3404))\n",
      "Optimal:  ('Orange juice is drinking by some man is walking', tensor(6.6191))\n",
      "Optimal:  ('orange juice is drinking by man is walking', tensor(6.3404))\n",
      "Optimal:  ('Orange juice is drinking by man is walking', tensor(6.3404))\n",
      "('Orange juice is being drunk by a man who is walking on a sunny day', tensor(10.6530)) =>\n",
      "('Orange juice is being drunk by a man who is walking', tensor(8.6787)) =>\n",
      "('Orange juice is being drunk by a man who is walking on a day', tensor(9.6062)) =>\n",
      "('Orange juice is being drink by a man who is walking', tensor(6.7856)) =>\n",
      "('Orange juice is drinking by a man who is walking', tensor(7.4855)) =>\n",
      "('Orange juice is drinking by a man is walking', tensor(6.6824)) =>\n",
      "('Orange juice is being drink by a man is walking', tensor(6.6931)) =>\n",
      "('Orange juice is drinking by man is walking', tensor(6.3404)) =>\n",
      "('Orange juice is drinking by some man is walking', tensor(6.6191)) =>\n",
      "('orange juice is drinking by man is walking', tensor(6.3404)) =>\n",
      "('Orange juice is drinking by man is walking', tensor(6.3404))\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "planner = AStarPlanner(verbose=7)\n",
    "entail = planner.search(\"Orange juice is being drunk by a man who is walking on a sunny day\", \"A man is drinking orange juice and walking\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A little girl is playing a grand piano on stage', tensor(11.0662))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(7.3023))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(7.4375))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(6.5751))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(6.5751))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(6.5751))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(6.5751))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "('A little girl is playing a grand piano on stage', tensor(11.0662)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(7.3023)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(7.4375)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(6.5751)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(6.8940)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(6.5751)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(6.8940)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(6.5751)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(6.8940)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(6.5751)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "planner = AStarPlanner(verbose=7)\n",
    "entail = planner.search(entail_p[347], entail_hypo[347])\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal:  ('Two people wearing snowsuits are on the ground making snow angels', tensor(0.9347))\n",
      "============================\n",
      "('Two people wearing snowsuits are on the ground making snow angels', tensor(0.9347)) =>\n",
      "('Two people in snowsuits are lying in the snow and making snow angels', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"Two people wearing snowsuits are on the ground making snow angels\",\n",
    "        \"Two people in snowsuits are lying in the snow and making snow angels\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "('Tambourines are being played by a group of children', tensor(0.9701)) =>\n",
      "('A group of children is playing tambourines', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"Tambourines are being played by a group of children\", \n",
    "             \"A group of children is playing tambourines\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A family is watching a little boy who is hitting a baseball', tensor(0.6364)) =>\n",
      "('a little boy who is hitting a baseball', tensor(0.8425)) =>\n",
      "('A child is watching a little boy who is hitting a baseball', tensor(0.7762)) =>\n",
      "('A child is hitting a baseball', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A child is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You know that some life changing actions must be taken when grandma reacts with the sad emoji', tensor(0.9259)) =>\n",
      "('You know that some life actions must be taken when grandma reacts with the sad emoji', tensor(0.9658)) =>\n",
      "('You know that some changing actions must be taken when grandma reacts with the sad emoji', tensor(0.9301)) =>\n",
      "('You know that some actions must be taken when grandma reacts with the sad emoji', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"You know that some life changing actions must be taken when grandma reacts with the sad emoji\", \"You know that some actions must be taken when grandma reacts with the sad emoji\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A black and a white dog are joyfully running on the grass', tensor(0.5195)) =>\n",
      "('A black and a white dog are running on the grass', tensor(0.9441)) =>\n",
      "('one black and a white dog are joyfully running on the grass', tensor(0.5451)) =>\n",
      "('A dog, which has a black coat, and a white dog are running on the grass', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A black and a white dog are joyfully running on the grass\", \n",
    "             \"A dog, which has a black coat, and a white dog are running on the grass\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A group of boys are playing with a ball, in front of a large door made of wood', tensor(0.6323)) =>\n",
      "('A group of boys are playing in front of a large door made of wood', tensor(0.7649)) =>\n",
      "('A group of children are playing in front of a large door made of wood', tensor(0.8844)) =>\n",
      "('A group of boys are playing in front of a large door', tensor(0.8521)) =>\n",
      "('The children are playing in front of a large door', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A group of boys are playing with a ball, in front of a large door made of wood\", \n",
    "             \"The children are playing in front of a large door\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A man with a red helmet is riding a blue motorcycle down the road', tensor(0.6541)) =>\n",
      "('A man is riding a blue motorcycle down the road', tensor(0.7365)) =>\n",
      "('A man is riding a motorbike down the road', tensor(0.9152)) =>\n",
      "('A man is is riding a motorbike a motorbike down the road', tensor(0.9281)) =>\n",
      "('A man is is riding motorbike a motorbike down the road', tensor(0.9314)) =>\n",
      "('man is is riding a motorbike a motorbike down the road', tensor(0.9309)) =>\n",
      "('A motorcyclist is riding a motorbike along a roadway', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A man with a red helmet is riding a blue motorcycle down the road\", \n",
    "             \"A motorcyclist is riding a motorbike along a roadway\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal:  ('Two little boys are playing outside with a soccer ball on the green grass', tensor(0.8294))\n",
      "============================\n",
      "['are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'on the green grass => on the green grass', 'a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => with a soccer ball on the green grass']\n",
      "Two ['some', 'an', 'one']\n",
      "['Two => some', 'Two => an', 'Two => one', 'the => a', 'the => an', 'the => one', 'a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('an little boys are playing outside with a soccer ball on the green grass', tensor(0.8907))\n",
      "============================\n",
      "['are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'on the green grass => on the green grass', 'a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => with a soccer ball on the green grass']\n",
      "['an => a', 'an => some', 'an => one', 'the => a', 'the => an', 'the => one', 'a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('little boys are playing outside with a soccer ball on the green grass', tensor(0.8933))\n",
      "============================\n",
      "['are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'on the green grass => on the green grass', 'a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => with a soccer ball on the green grass']\n",
      "['the => a', 'the => an', 'the => one', 'a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('little boys are playing outside with a soccer ball on green grass', tensor(0.8950))\n",
      "============================\n",
      "['are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('little boys are playing outside with soccer ball on green grass', tensor(0.8955))\n",
      "============================\n",
      "['soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "[]\n",
      "Optimal:  ('little boys are playing outside with an soccer ball on green grass', tensor(0.8944))\n",
      "============================\n",
      "['playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with an soccer ball on green grass => with a soccer ball on the green grass', 'an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with an soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['an => a', 'an => some', 'an => one']\n",
      "Optimal:  ('little boys are playing outside with soccer ball on green grass', tensor(0.8955))\n",
      "============================\n",
      "['soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "[]\n",
      "Optimal:  ('little boys are playing outside with a soccer ball on green grass', tensor(0.8950))\n",
      "============================\n",
      "['are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('little boys are playing outside with soccer ball on green grass', tensor(0.8955))\n",
      "============================\n",
      "['soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "[]\n",
      "Optimal:  ('little boys are playing outside with an soccer ball on green grass', tensor(0.8944))\n",
      "============================\n",
      "['playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with an soccer ball on green grass => with a soccer ball on the green grass', 'an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with an soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['an => a', 'an => some', 'an => one']\n",
      "Optimal:  ('little boys are playing outside with soccer ball on green grass', tensor(0.8955))\n",
      "============================\n",
      "['soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "[]\n",
      "Optimal:  ('little boys are playing outside with a soccer ball on green grass', tensor(0.8950))\n",
      "============================\n",
      "['are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['a => some', 'a => an', 'a => one']\n",
      "('Two little boys are playing outside with a soccer ball on the green grass', tensor(0.8294)) =>\n",
      "('an little boys are playing outside with a soccer ball on the green grass', tensor(0.8907)) =>\n",
      "('little boys are playing outside with a soccer ball on the green grass', tensor(0.8933)) =>\n",
      "('little boys are playing outside with a soccer ball on green grass', tensor(0.8950)) =>\n",
      "('little boys are playing outside with soccer ball on green grass', tensor(0.8955)) =>\n",
      "('little boys are playing outside with an soccer ball on green grass', tensor(0.8944)) =>\n",
      "('little boys are playing outside with soccer ball on green grass', tensor(0.8955)) =>\n",
      "('little boys are playing outside with a soccer ball on green grass', tensor(0.8950)) =>\n",
      "('little boys are playing outside with soccer ball on green grass', tensor(0.8955)) =>\n",
      "('little boys are playing outside with an soccer ball on green grass', tensor(0.8944)) =>\n",
      "('little boys are playing outside with soccer ball on green grass', tensor(0.8955)) =>\n",
      "('little boys are playing outside with a soccer ball on green grass', tensor(0.8950))\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(entail_p[52], entail_hypo[52])\n",
    "#entail = planner.search(\"A dog that has a black and white coat is trotting through shallow water\", \n",
    "#             \"A dog that has a white and black colored coat is trotting through shallow water.\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal:  ('The leading car gradually shifted to the left lane', tensor(0.9242))\n",
      "============================\n",
      "('The leading car gradually shifted to the left lane', tensor(0.9242)) =>\n",
      "('The leading car slowly shifted to the left lane', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"The leading car gradually shifted to the left lane\", \n",
    "             \"The leading car slowly shifted to the left lane\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A child is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"A deer is jumping over a fence\", \n",
    "             \"A deer is jumping a fence\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"A boy is hitting a baseball\", \n",
    "             \"A chil is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entail = planner.search(\"A brown dog is attacking another animal in front of the tall man in pants\", \n",
    "             \"A dog is attacking another animal in front of the man in pants\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A family is watching a boy who is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"You can't park in front of my house on weekends.\", \n",
    "             \"You can't park in front of my large house on weekends.\")\n",
    "\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "annotations = []\n",
    "with open(\"./generation_log_upward.txt\", 'w') as generate_log:\n",
    "    phrasalGenerator = PhrasalGenerator()\n",
    "    pipeline = PolarizationPipeline(verbose=0)\n",
    "    for i in tqdm(range(0, 500)):\n",
    "        premise = MED_none[i].replace('\\n', '')\n",
    "        hypothesis = MED_none_hypo[i].replace('\\n', '')\n",
    "        premise = phrasalGenerator.preprocess(premise)\n",
    "        hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "        tokenized = tokenizer(premise).sentences[0].words\n",
    "        tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "        try:\n",
    "            h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "            h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "        except:\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            continue\n",
    "        pipeline.modify_replacement(h_tree, replaced)\n",
    "        phrases = {} \n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "\n",
    "        try:\n",
    "            annotation = pipeline.single_polarization(premise)\n",
    "        except:\n",
    "            #generate_log.write(\"\\nPremise: \" + premise)\n",
    "            #generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            continue\n",
    "    \n",
    "        phrasalGenerator.kb = phrases\n",
    "        #print(phrasalGenerator.kb)\n",
    "        phrasalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "        \n",
    "        phrasalGenerator.deptree_generate(\n",
    "            annotation['polarized_tree'], \n",
    "            annotation['annotated'], \n",
    "            tokens)\n",
    "\n",
    "        # for gen_tree in phrasalGenerator.tree_log:\n",
    "        #    leaves = gen_tree[0].sorted_leaves().popkeys()\n",
    "        #    sentence = ' '.join([x[0] for x in leaves])\n",
    "        #    print((sentence, gen_tree[1]))\n",
    "            \n",
    "        if phrasalGenerator.stop_critarion:\n",
    "            generate_log.write(\"\\nID: \" + str(i))\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            #print(\"\\nPremise: \" + premise)\n",
    "            #print(\"\\nHypothesis: \" + hypothesis)\n",
    "            #print(*phrasalGenerator.sent_log, sep=\"\\n\")\n",
    "            #generate_log.writelines(phrasalGenerator.sent_log)\n",
    "            generate_log.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail_p = []\n",
    "entail_hypo = []\n",
    "\n",
    "with open(\"generation_log_SICK_Neutral.txt\", 'r') as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        entail_p.append(lines[i*4+1])\n",
    "        entail_hypo.append(lines[i*4+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SICK_neutral_incorrect.txt\", 'w') as generate_log:\n",
    "    for i in range(len(entail_p)):\n",
    "        generate_log.write(\"ID: \" + str(i) + '\\n')\n",
    "        generate_log.write(entail_p[i])\n",
    "        generate_log.write(entail_hypo[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}