{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-MRPC were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
    "\n",
    "roberta_MRPC = \"textattack/roberta-base-MRPC\"\n",
    "bert_MRPC = \"bert-base-cased-finetuned-mrpc\"\n",
    "\n",
    "paraphraseTokenizer = AutoTokenizer.from_pretrained(roberta_MRPC)  \n",
    "paraphraseModel = AutoModelForSequenceClassification.from_pretrained(roberta_MRPC)\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_mrpc(seq1s, seq2s):\n",
    "    for i in range(len(seq1s)):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1s[i], seq2s[i], return_tensors=\"pt\")\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        print(f\"{classes[1]}: {round(paraphrase_results[1] * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is paraphrase: 82%\nis paraphrase: 100%\nis paraphrase: 100%\nis paraphrase: 92%\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"ate an apple\", \n",
    "              \"jumping over a fence\", \n",
    "              \"riding a motorbike down the road\", \n",
    "              \"all the people walk\"]\n",
    "sentences2 = [\"an apple was eaten\",  \n",
    "              \"jumping over a enclosure\", \n",
    "              \"riding a motorbike along a roadway\",\n",
    "              \"everyone walks\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is paraphrase: 99%\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"Every person who smokes two cigarettes\"]\n",
    "sentences2 = [\"Every person who smokes exactly two cigarettes\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is paraphrase: 100%\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"do not suffer from sleeplessness\"]\n",
    "sentences2 = [\"do not suffer from lack of deep sleep\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is paraphrase: 69%\n",
      "Similarity Score: 0.0823\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"a man is crying\"]\n",
    "sentences2 = [\"a man with a smile\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is paraphrase: 99%\n",
      "Similarity Score: 0.7147\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"a problem is solved\"]\n",
    "sentences2 = [\"resolved a problem\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is paraphrase: 100%\n",
      "Similarity Score: 0.7243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences1 = [\"enjoying a balloon joyride\"]\n",
    "sentences2 = [\"on a balloon joyride\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-03-14 03:09:02 INFO: Loading these models for language: en (English):\n",
      "========================================\n",
      "| Processor | Package                  |\n",
      "----------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt  |\n",
      "| pos       | ../model/en/pos/ewt.pt   |\n",
      "| lemma     | ../model/en/lemma/gum.pt |\n",
      "| depparse  | ../model/e...rse/gum.pt  |\n",
      "========================================\n",
      "\n",
      "2021-03-14 03:09:02 INFO: Use device: gpu\n",
      "2021-03-14 03:09:02 INFO: Loading: tokenize\n",
      "2021-03-14 03:09:04 INFO: Loading: pos\n",
      "2021-03-14 03:09:04 INFO: Loading: lemma\n",
      "2021-03-14 03:09:04 INFO: Loading: depparse\n",
      "2021-03-14 03:09:05 INFO: Done loading processors!\n",
      "2021-03-14 03:09:05 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt |\n",
      "=======================================\n",
      "\n",
      "2021-03-14 03:09:05 INFO: Use device: cpu\n",
      "2021-03-14 03:09:05 INFO: Loading: tokenize\n",
      "2021-03-14 03:09:05 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from Udep2Mono.polarization import PolarizationPipeline\n",
    "from chunker import Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntacticVariator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunker = Chunker()\n",
    "        self.paraphraseTokenizer = paraphraseTokenizer\n",
    "        self.paraphraseModel = paraphraseModel\n",
    "\n",
    "    def chunking(self, tree):\n",
    "        return self.chunker.get_chunks_byDepTree(tree)\n",
    "\n",
    "    def build_pairs(self, chunks1, chunks2):\n",
    "        chunk_pairs = []\n",
    "        for chunk1 in chunks1:\n",
    "            for chunk2 in chunks2:\n",
    "                if len(set(chunk1.split(' ')).intersection(chunk2.split(' '))) > 0:\n",
    "                     chunk_pairs.append((chunk1, chunk2))\n",
    "\n",
    "        return chunk_pairs\n",
    "\n",
    "    def inference_mrpc(self, seq1, seq2):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1, seq2, return_tensors=\"pt\")\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        return paraphrase_results[1]\n",
    "\n",
    "    def phrase_alignment(self, chunk_pairs):\n",
    "        alignments = []\n",
    "        for pair in chunk_pairs:\n",
    "            score = self.inference_mrpc(pair[0], pair[1])\n",
    "            if score > 0.85:\n",
    "                alignments.append(pair)\n",
    "\n",
    "        return alignments\n",
    "\n",
    "    def variate(self, sentence, p_tree, h_tree):\n",
    "        p_chunks = self.chunking(p_tree)\n",
    "        h_chunks = self.chunking(h_tree)\n",
    "\n",
    "        chunk_pairs = self.build_pairs(p_chunks, h_chunks)\n",
    "        alignments = self.phrase_alignment(chunk_pairs)\n",
    "\n",
    "        #print(chunk_pairs)\n",
    "        print(*alignments, sep='\\n')\n",
    "\n",
    "        var_sentence = copy(sentence)\n",
    "        variates = set()\n",
    "        for align in alignments:\n",
    "            var_sentence = var_sentence.replace(align[0], align[1])\n",
    "            variates.add(var_sentence)\n",
    "            #print(sentence + \" => \" + var_sentence)\n",
    "\n",
    "        return variates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('a baseball', 'a baseball')\n('hitting a baseball', 'hitting a baseball')\n('who hitting a baseball', 'hitting a baseball')\n('A boy who hitting a baseball', 'hitting a baseball')\n{'A boy who is hitting a baseball'}\n"
     ]
    }
   ],
   "source": [
    "syntacticVariator = SyntacticVariator()\n",
    "pipeline = PolarizationPipeline(verbose=1)\n",
    "from copy import copy\n",
    "\n",
    "P = \"A boy who is hitting a baseball\"\n",
    "H = \"A boy is hitting a baseball\"\n",
    "p_tree = pipeline.single_polarization(P)[\"polarized_tree\"]\n",
    "h_tree = pipeline.single_polarization(H)[\"polarized_tree\"]\n",
    "\n",
    "new_P = syntacticVariator.variate(P, p_tree, h_tree)\n",
    "\n",
    "print(new_P)\n",
    "#print(P + \" => \" + new_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['with a warm smile',\n",
       " 'A man with a warm smile',\n",
       " 'a lecture',\n",
       " 'giving a lecture']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "P = \"A man with a warm smile is giving a lecture\"\n",
    "p_tree = pipeline.single_polarization(P)[\"polarized_tree\"]\n",
    "chunker.get_chunks_byDepTree(p_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['smiling warmly',\n",
       " 'smiling warmly',\n",
       " 'A man smiling warmly',\n",
       " 'a lecture',\n",
       " 'giving a lecture']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "H = \"A man smiling warmly is giving a lecture\"\n",
    "h_tree = pipeline.single_polarization(H)[\"polarized_tree\"]\n",
    "chunker.get_chunks_byDepTree(h_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is paraphrase: 100%\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"A man smiling warmly\"]\n",
    "sentences2 = [\"A man with a warm smile\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is paraphrase: 1%\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"giving a lecture\"]\n",
    "sentences2 = [\"A man with a warm smile\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['of the office',\n",
       " 'in front of the office',\n",
       " 'sitting in front of the office',\n",
       " 'sitting in front of the office',\n",
       " 'a women sitting in front of the office']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "P = \"There is a women sitting in front of the office\"\n",
    "p_tree = pipeline.single_polarization(P)[\"polarized_tree\"]\n",
    "chunker.get_chunks_byDepTree(p_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['of the office',\n",
       " 'in front of the office',\n",
       " 'a seated women in front of the office']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "H = \"There is a seated women in front of the office\"\n",
    "h_tree = pipeline.single_polarization(H)[\"polarized_tree\"]\n",
    "chunker.get_chunks_byDepTree(h_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is paraphrase: 99%\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"Every person who smokes exactly two cigarettes\"]\n",
    "sentences2 = [\"Every person who smokes two cigarettes\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Udep2Mono/polarity_gold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}