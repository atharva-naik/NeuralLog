{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-MRPC were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
    "\n",
    "roberta_MRPC = \"textattack/roberta-base-MRPC\"\n",
    "bert_MRPC = \"bert-base-cased-finetuned-mrpc\"\n",
    "\n",
    "paraphraseTokenizer = AutoTokenizer.from_pretrained(roberta_MRPC)  \n",
    "paraphraseModel = AutoModelForSequenceClassification.from_pretrained(roberta_MRPC)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentenceTransformer = SentenceTransformer(\"roberta-large-nli-stsb-mean-tokens\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(s1, s2):\n",
    "    num_sim = 0\n",
    "    seq1 = s1.split(\" \")\n",
    "    for w in seq1:\n",
    "        if w in s2:\n",
    "            num_sim += 1\n",
    "    return num_sim / len(seq1)\n",
    "\n",
    "def inference_mrpc(seq1s, seq2s):\n",
    "    for i in range(len(seq1s)):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1s[i], seq2s[i], return_tensors=\"pt\")\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        print(f\"{classes[1]}: {round(paraphrase_results[1] * 100)}%\")\n",
    "\n",
    "def inference_sts(seq1s, seq2s):\n",
    "    embeddings1 = sentenceTransformer.encode(seq1s, convert_to_tensor=True)\n",
    "    embeddings2 = sentenceTransformer.encode(seq2s, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    for i in range(len(seq1s)):\n",
    "        print(\"Similarity Score: {:.4f}\".format(cosine_scores[i][i])) \n",
    "        # word_similarity(seq1s[i], seq2s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is paraphrase: 82%\n",
      "is paraphrase: 100%\n",
      "is paraphrase: 100%\n",
      "is paraphrase: 92%\n",
      "Similarity Score: 0.8932\n",
      "Similarity Score: 0.8080\n",
      "Similarity Score: 0.9863\n",
      "Similarity Score: 0.9202\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"ate an apple\", \n",
    "              \"jumping over a fence\", \n",
    "              \"riding a motorbike down the road\", \n",
    "              \"all the people walk\"]\n",
    "sentences2 = [\"an apple was eaten\",  \n",
    "              \"jumping over a enclosure\", \n",
    "              \"riding a motorbike along a roadway\",\n",
    "              \"everyone walks\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is paraphrase: 99%\n",
      "Similarity Score: 0.7890\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"there is a seated women\"]\n",
    "sentences2 = [\"there is a women sitting\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is paraphrase: 99%\n",
      "Similarity Score: 0.9588\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"a red apple\"]\n",
    "sentences2 = [\"an apple which is red\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is paraphrase: 69%\n",
      "Similarity Score: 0.0823\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"a man is crying\"]\n",
    "sentences2 = [\"a man with a smile\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is paraphrase: 99%\n",
      "Similarity Score: 0.7147\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\"a problem is solved\"]\n",
    "sentences2 = [\"resolved a problem\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is paraphrase: 100%\n",
      "Similarity Score: 0.7243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences1 = [\"enjoying a balloon joyride\"]\n",
    "sentences2 = [\"on a balloon joyride\"]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-03-08 05:22:52 INFO: Loading these models for language: en (English):\n",
      "========================================\n",
      "| Processor | Package                  |\n",
      "----------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt  |\n",
      "| pos       | ../model/en/pos/ewt.pt   |\n",
      "| lemma     | ../model/en/lemma/gum.pt |\n",
      "| depparse  | ../model/e...rse/gum.pt  |\n",
      "========================================\n",
      "\n",
      "2021-03-08 05:22:52 INFO: Use device: gpu\n",
      "2021-03-08 05:22:52 INFO: Loading: tokenize\n",
      "2021-03-08 05:22:54 INFO: Loading: pos\n",
      "2021-03-08 05:22:54 INFO: Loading: lemma\n",
      "2021-03-08 05:22:54 INFO: Loading: depparse\n",
      "2021-03-08 05:22:55 INFO: Done loading processors!\n",
      "2021-03-08 05:22:55 INFO: Loading these models for language: en (English):\n",
      "========================================\n",
      "| Processor | Package                  |\n",
      "----------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt  |\n",
      "| pos       | ../model/en/pos/ewt.pt   |\n",
      "| lemma     | ../model/en/lemma/ewt.pt |\n",
      "| depparse  | ../model/e...rse/ewt.pt  |\n",
      "========================================\n",
      "\n",
      "2021-03-08 05:22:55 INFO: Use device: gpu\n",
      "2021-03-08 05:22:55 INFO: Loading: tokenize\n",
      "2021-03-08 05:22:55 INFO: Loading: pos\n",
      "2021-03-08 05:22:55 INFO: Loading: lemma\n",
      "2021-03-08 05:22:55 INFO: Loading: depparse\n",
      "2021-03-08 05:22:56 INFO: Done loading processors!\n",
      "2021-03-08 05:22:56 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt |\n",
      "=======================================\n",
      "\n",
      "2021-03-08 05:22:56 INFO: Use device: gpu\n",
      "2021-03-08 05:22:56 INFO: Loading: tokenize\n",
      "2021-03-08 05:22:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from Udep2Mono.polarization import PolarizationPipeline\n",
    "from chunker import Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nsubj↑  [PRP  We↑]  [advmod↑  [RB  not↑]  [aux↓  [VBD  did↓]  [obj↓  [NNS  guests↓]  [VB  expect↓]]]]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'val'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-419c6ff00aae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_chunks_byDepTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\UdepLog\\src\\chunker.py\u001b[0m in \u001b[0;36mget_chunks_byDepTree\u001b[1;34m(self, tree)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_chunks_byDepTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mpipe1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         \u001b[0mg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmono2Graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\UdepLog\\src\\chunker.py\u001b[0m in \u001b[0;36mmono2Graph\u001b[1;34m(self, sent_info)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPairCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmono2Graph_recur\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"verb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\UdepLog\\src\\chunker.py\u001b[0m in \u001b[0;36mmono2Graph_recur\u001b[1;34m(self, sent_tree, G, mods, pos, counter)\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msent_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodifiers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"acl\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                     \u001b[0mpipeTemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\UdepLog\\src\\chunker.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msent_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodifiers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"acl\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                     \u001b[0mpipeTemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'val'"
     ]
    }
   ],
   "source": [
    "chunker = Chunker()\n",
    "pipeline = PolarizationPipeline(verbose=1)\n",
    "\n",
    "tree = pipeline.single_polarization(\"We did not expect guests\")[\"polarized_tree\"]\n",
    "result_tree = pipeline.postprocess(tree)\n",
    "print(result_tree)\n",
    "chunker.get_chunks_byDepTree(result_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}