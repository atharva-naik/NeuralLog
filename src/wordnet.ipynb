{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordnet import *\n",
    "\n",
    "def assign_all_relations_wordnet(k):\n",
    "    assign_relations([k.nouns], k, pos=\"NN\", depth=10)\n",
    "    assign_relations([k.subsecAdj], k, pos=\"JJ\", depth=10)\n",
    "    assign_relations([k.verbs], k, pos=\"VB\", depth=10)\n",
    "    assign_relations([k.advs], k, pos=\"RB\", depth=10)\n",
    "\n",
    "\n",
    "def assign_relations(word_lists, k, pos, depth):\n",
    "    \"\"\" note that k.nouns = { wholeStr: {type1: node, type2: node} }\n",
    "     so we need to make sure the two compared nodes are of the same type \"\"\"\n",
    "    full_dict = {}\n",
    "    for word_list in word_lists:\n",
    "        full_dict.update(word_list)\n",
    "    words = sorted(full_dict.keys())\n",
    "    for idx_pair in itertools.combinations(range(len(full_dict)), 2):\n",
    "        # print(idx_pair)\n",
    "        word1_wholeStr, word2_wholeStr = words[idx_pair[0]], words[idx_pair[1]]\n",
    "        # a dict: {type1: node1, type2: node2}\n",
    "        node1_dict = full_dict[word1_wholeStr]\n",
    "        node2_dict = full_dict[word2_wholeStr]\n",
    "\n",
    "        # first find the relation between word1_wholeStr and word2_wholeStr\n",
    "        word1_lower, word2_lower = word1_wholeStr.lower(), word2_wholeStr.lower()\n",
    "        rel = find_relation(word1_lower, word2_lower, pos, depth)\n",
    "        if rel:\n",
    "            add_relation_wordnet(node1_dict, node2_dict, k, rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.frags = {}\n",
    "        self.numPairs = 0\n",
    "        self.tokens = {\n",
    "            'nouns': {},\n",
    "            'subjectAdj': {},\n",
    "            'RCs': {},\n",
    "            'VPs': {},\n",
    "            'NPs': {},\n",
    "            'PPN': {},\n",
    "            'PPV': {},\n",
    "            'advs': {},\n",
    "            'verbs': {},\n",
    "            'CDs': {}\n",
    "        }\n",
    "        self.pairs_added = []\n",
    "\n",
    "    def update_modifier(self):\n",
    "        self.modifier_NP()\n",
    "\n",
    "    def modifier_NP(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nrefused + 'VERB' => 'Did not VERB'\\nrefuses + 'VERB' => 'Does not VERB'\\nrefuse + 'VERB' => 'Not VERB'\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\"\"\"\n",
    "refused + 'VERB' => 'Did not VERB'\n",
    "refuses + 'VERB' => 'Does not VERB'\n",
    "refuse + 'VERB' => 'Not VERB'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | gum       |\n",
      "| pos       | gum       |\n",
      "| lemma     | gum       |\n",
      "| depparse  | gum       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "INFO:stanza:Use device: gpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Loading: depparse\n",
      "INFO:stanza:Loading: ner\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from Udep2Mono.binarization import BinaryDependencyTree\n",
    "from Udep2Mono import polarization\n",
    "from Udep2Mono.util import btreeToList\n",
    "from copy import deepcopy\n",
    "\n",
    "class TreeFactory:\n",
    "    def __init__(self):\n",
    "        self.tree_builder = {\n",
    "            \"amod\": self.build_amod\n",
    "        }\n",
    "        \n",
    "    def buildTree(self, rel, word, wid):\n",
    "        return self.tree_builder[rel](word, wid)\n",
    "\n",
    "    def build_amod(self, noun, wid):\n",
    "        left = BinaryDependencyTree(\"ADJ\", \"N\", \"N\", 1024, wid=wid-(wid-1)/2, npos=\"JJ\")\n",
    "        right = BinaryDependencyTree(noun, \"N\", \"N\", 1024, wid=wid, npos=\"NN\")\n",
    "        return BinaryDependencyTree(\"amod\", left, right, 1025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "from database import *\n",
    "from pattern.en import pluralize, singularize\n",
    "\n",
    "class LexicalGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.length = 0\n",
    "        self.tree_factory = TreeFactory()\n",
    "        self.treeLog = []\n",
    "        self.polarLog = []\n",
    "        self.lexial_generation = {\n",
    "             \"acl\": self.generate_acl_relcl,\n",
    "             \"acl:relcl\": self.generate_acl_relcl,\n",
    "             \"advcl\": self.generate_acl_relcl,\n",
    "             \"cc:preconj\": self.generate_det,\n",
    "             \"det\": self.generate_det,\n",
    "             \"det:predet\": self.generate_det,\n",
    "        }\n",
    "        '''\"advmod\": self.generate_advmod,\n",
    "            \"advmod:count\": self.generate_advmod,\n",
    "            \"amod\": self.generate_amod,\n",
    "            \"appos\": self.generate_inherite,\n",
    "            \"aux\": self.generate_aux,\n",
    "            \"aux:pass\": self.generate_aux,\n",
    "            \"case\": self.generate_case,\n",
    "            \"cc\": self.generate_cc,\n",
    "           \n",
    "            \"ccomp\": self.generate_ccomp,\n",
    "            \"compound\": self.generate_inherite,\n",
    "            \"compound:prt\": self.generate_inherite,\n",
    "            \"conj\": self.generate_inherite,\n",
    "            \"cop\": self.generate_inherite,\n",
    "            \"csubj\": self.generate_nsubj,\n",
    "            \"csubj:pass\": self.generate_nsubj,\n",
    "            \"dep\": self.generate_dep,\n",
    "            \n",
    "            \"discourse\": self.generate_discourse,\n",
    "            \"expl\": self.generate_expl,\n",
    "            \"fixed\": self.generate_inherite,\n",
    "            \"flat\": self.generate_inherite,\n",
    "            \"goeswith\": self.generate_inherite,\n",
    "            \"iobj\": self.generate_inherite,\n",
    "            \"mark\": self.generate_inherite,\n",
    "            \"nmod\": self.generate_nmod,\n",
    "            \"nmod:npmod\": self.generate_nmod,\n",
    "            \"nmod:tmod\": self.generate_nmod,\n",
    "            \"nmod:poss\": self.generate_nmod_poss,\n",
    "            \"nsubj\": self.generate_nsubj,\n",
    "            \"nsubj:pass\": self.generate_nsubj,\n",
    "            \"nummod\": self.generate_nummod,\n",
    "            \"obj\": self.generate_obj,\n",
    "            \"obl\": self.generate_obj,\n",
    "            \"obl:npmod\": self.generate_oblnpmod,\n",
    "            \"obl:tmod\": self.generate_inherite,\n",
    "            \"parataxis\": self.generate_inherite,\n",
    "            \"xcomp\": self.generate_obj,'''\n",
    "\n",
    "    def deptree_generate(self, length, tree):\n",
    "        self.deptree = tree\n",
    "        self.length = length\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if tree.val in self.lexial_generation.keys():\n",
    "            self.lexial_generation[tree.val](tree)\n",
    "\n",
    "    def save_tree(self, tree=None):\n",
    "        if tree is not None:\n",
    "            generated, _, _, _ = btreeToList(tree, self.length, {}, 0)\n",
    "        else:\n",
    "            generated, _, _, _ = btreeToList(self.deptree, self.length, {}, 0)\n",
    "        generated = '[%s]' % ', '.join(map(str, generated)).replace(\"'\", \"\")\n",
    "        generated = generated.replace(\",\", \"\")\n",
    "        print(\"New tree: \", generated)\n",
    "\n",
    "        if tree is not None:\n",
    "            return deepcopy(self.deptree)\n",
    "        else:\n",
    "            return deepcopy(self.deptree)\n",
    "\n",
    "    def rollback(self, tree, backup):\n",
    "        tree.val = backup.val\n",
    "        tree.left = deepcopy(backup.left)\n",
    "        tree.right = deepcopy(backup.right)\n",
    "        tree.mark = backup.mark\n",
    "        tree.npos = backup.npos\n",
    "        tree.id = backup.id\n",
    "\n",
    "    def generate_acl_relcl(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "        hyper = [] \n",
    "        hypo = [] \n",
    "        syn =[] \n",
    "\n",
    "        if right.npos != None:\n",
    "            hyper, hypo, syn, ant = get_word_sets(singularize(right.val), right.npos.lower())\n",
    "\n",
    "            if right.mark == \"+\":\n",
    "                tree.val = right.val\n",
    "                tree.mark = right.mark\n",
    "                tree.npos = right.npos\n",
    "                tree.id = right.id\n",
    "                tree.left = \"N\"\n",
    "                tree.right = \"N\"\n",
    "                self.treeLog.append(self.save_tree())\n",
    "                self.rollback(tree, backup)\n",
    "                \n",
    "                print(hyper)\n",
    "                for word in hyper:\n",
    "                    tree.right.val = word\n",
    "                    self.treeLog.append(self.save_tree())\n",
    "                self.rollback(tree, backup)\n",
    "\n",
    "            if right.mark == \"-\":\n",
    "                amod_tree = self.tree_factory.buildTree(\"amod\", right.val, right.id)\n",
    "                tree.right = amod_tree\n",
    "                self.treeLog.append(self.save_tree())\n",
    "                self.rollback(tree, backup) \n",
    "\n",
    "                print(hypo)\n",
    "                for word in hypo:\n",
    "                    tree.right.val = word\n",
    "                    self.treeLog.append(self.save_tree())\n",
    "                self.rollback(tree, backup)\n",
    "        else:\n",
    "            self.generate(left)\n",
    "\n",
    "    def generate_det(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "        #hyper, hypo, syn, ant = get_word_sets(right.val, right.npos.lower())\n",
    "        kb = quantifier.find({\"word\": left.val})[0]\n",
    "        self.generate(right)\n",
    "\n",
    "        if left.mark == \"+\":\n",
    "            for word in kb[\"<\"]:\n",
    "                tree.left.val = word\n",
    "                self.treeLog.append(self.save_tree())\n",
    "            self.rollback(tree, backup)\n",
    "        \n",
    "        if left.mark == \"-\":\n",
    "            for word in kb[\">\"]:\n",
    "                tree.left.val = word\n",
    "                self.treeLog.append(self.save_tree())\n",
    "            self.rollback(tree, backup)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "all↑ dogs↓ who↓ follow↓ orders↓\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [amod [JJ ADJ] [NN dogs]]]]\n",
      "{'corgi', 'a Cocker Spaniel', 'Mexican hairless', 'hunt', 'Great Pyrenees', 'pooch', 'Leonberg', 'hound', 'basenji', 'Vienna sausage', 'quest', 'carriage dog', 'A poodle', 'pug-dog', 'tree', 'poodle', 'barker', 'Welsh corgi', 'toy', 'coach dog', 'toy dog', 'a Great Dane', 'a spaniel', 'spitz', 'an Irish Setter', 'a rottweiler', 'a golden retriever', 'dalmatian', 'bow-wow', 'mongrel', 'Labrador Retriever', 'hunting dog', 'lapdog', 'trace', 'Belgian griffon', 'pug', 'cur', 'griffon', 'Newfoundland', 'doggy', 'working dog', 'run down', 'puppy', 'Brussels griffon', 'poodle dog', 'Catahoula leopard dog', 'Newfoundland dog', 'doggie', 'perisher', 'an Afgan Hound', 'Joy', 'beagle', 'mutt'}\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS corgi↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS a Cocker Spaniel↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Mexican hairless↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS hunt↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Great Pyrenees↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS pooch↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Leonberg↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS hound↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS basenji↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Vienna sausage↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS quest↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS carriage dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS A poodle↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS pug-dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS tree↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS poodle↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS barker↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Welsh corgi↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS toy↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS coach dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS toy dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS a Great Dane↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS a spaniel↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS spitz↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS an Irish Setter↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS a rottweiler↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS a golden retriever↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS dalmatian↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS bow-wow↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS mongrel↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Labrador Retriever↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS hunting dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS lapdog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS trace↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Belgian griffon↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS pug↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS cur↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS griffon↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Newfoundland↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS doggy↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS working dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS run down↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS puppy↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Brussels griffon↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS poodle dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Catahoula leopard dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Newfoundland dog↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS doggie↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS perisher↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS an Afgan Hound↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS Joy↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS beagle↓]]]\n",
      "New tree:  [det↓ [DT all↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS mutt↓]]]\n",
      "New tree:  [det↓ [DT most↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS dogs↓]]]\n",
      "New tree:  [det↓ [DT many↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS dogs↓]]]\n",
      "New tree:  [det↓ [DT several↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS dogs↓]]]\n",
      "New tree:  [det↓ [DT some↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS dogs↓]]]\n",
      "New tree:  [det↓ [DT a↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS dogs↓]]]\n",
      "New tree:  [det↓ [DT an↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS dogs↓]]]\n",
      "New tree:  [det↓ [DT one↑] [acl:relcl↓ [nsubj↓ [WP who↓] [obj↓ [NNS orders↓] [VBP follow↓]]] [NNS dogs↓]]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"All flowers which \"]\n",
    "annotations, _ = polarization.run_polarize_pipeline(\n",
    "    sentences, verbose=2, parser=\"stanza\")\n",
    "\n",
    "lexicalGenerator = LexicalGenerator()\n",
    "for annotation in annotations:\n",
    "    annotated, original, polarized, postags, polarized_tree = annotation\n",
    "    print(annotated)\n",
    "    lexicalGenerator.deptree_generate(len(original), polarized_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import conjugate, lemma, lexeme, PAST, SG, PRESENT\n",
    "\n",
    "class ImplicativeGenerator:\n",
    "    def __init__(self, length, kb, tree):\n",
    "        self.kb = kb\n",
    "        self.treeLog = []\n",
    "        self.polarLog = []\n",
    "        self.deptree = tree\n",
    "        self.length = length\n",
    "\n",
    "    def find_verbs(self, postags):\n",
    "        verbs = []\n",
    "        for word in postags:\n",
    "            if 'VB' in postags[word][1]:\n",
    "                verbs.append((word, postags[word][0]))\n",
    "        return verbs\n",
    "    \n",
    "    def fix_tense(self, verb, pos):\n",
    "        if pos == \"VBD\":\n",
    "            return conjugate(verb=verb, tense=PAST, person=1)\n",
    "        elif pos == \"VBZ\":\n",
    "            return conjugate(verb=verb, tense=PRESENT, person=3)\n",
    "        else:\n",
    "            return verb\n",
    "\n",
    "    def search(self):\n",
    "        #verbs = self.find_verbs(postags)\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    \n",
    "    def save_tree(self, tree=None):\n",
    "        if tree is not None:\n",
    "            generated, _, _, _ = btreeToList(tree, self.length, {}, 0)\n",
    "        else:\n",
    "            generated, _, _, _ = btreeToList(self.deptree, self.length, {}, 0)\n",
    "        generated = '[%s]' % ', '.join(map(str, generated)).replace(\"'\", \"\")\n",
    "        generated = generated.replace(\",\", \"\")\n",
    "        print(\"New tree: \", generated)\n",
    "\n",
    "        if tree is not None:\n",
    "            return deepcopy(self.deptree)\n",
    "        else:\n",
    "            return deepcopy(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if tree.val in [\"ccomp\", \"xcomp\"]:\n",
    "            backup = deepcopy(tree)\n",
    "\n",
    "            verb = conjugate(verb=tree.right.val, tense=PRESENT, person=1)\n",
    "            pos = tree.right.npos\n",
    "\n",
    "            impl_signs = self.kb.find({\"Verb\": verb})\n",
    "            if impl_signs:\n",
    "                sign = impl_signs[0]['Signature'].split('/')\n",
    "                if sign[0] == \"+\" and sign[1] == \"+\":\n",
    "                    self.treeLog.append(self.save_tree(tree.left.right))\n",
    "                elif sign[0] == \"+\" and sign[1] == \"-\":\n",
    "                    tree.val = tree.left.right.val\n",
    "                    tree.mark = tree.left.right.mark\n",
    "                    tree.id = tree.left.right.id\n",
    "                    tree.right = tree.left.right.right\n",
    "                    tree.right.val = self.fix_tense(tree.right.val, pos)\n",
    "                    tree.left = tree.left.right.left\n",
    "                    \n",
    "                    self.treeLog.append(self.save_tree())\n",
    "\n",
    "                    tree.val = backup.val\n",
    "                    tree.mark = backup.mark\n",
    "                    tree.id = backup.id\n",
    "                    tree.left = deepcopy(backup.left)\n",
    "                    tree.right = deepcopy(backup.right)\n",
    "        else:\n",
    "            if tree.left != \"N\":\n",
    "                self.generate(tree.left)\n",
    "\n",
    "            if tree.right != \"N\":\n",
    "                self.generate(tree.right)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.45it/s]\n",
      "New tree:  [nsubj↑ [PRP i↑] [obj↑ [nmod:poss↑ [PRP$ my↑] [NN homework↑]] [VB finished↑]]]\n",
      "New tree:  [nsubj↑ [det= [DT this=] [NN apple=]] [cop↑ [VBZ is↑] [JJ good↑]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from Udep2Mono.dependency_parse import dependencyParse\n",
    "#tree, postags, words = dependencyParse(\"All dogs eat food\", parser=\"stanza\")[0]\n",
    "\n",
    "sentences = [\"I managed to finish my homework\", \n",
    "             \"I recognized that this apple is good\",\n",
    "             \"I realized that my homework is hard\"]\n",
    "annotations, _ = polarization.run_polarize_pipeline(\n",
    "    sentences, verbose=2, parser=\"stanza\")\n",
    "print()\n",
    "for annotation in annotations:\n",
    "    annotated, original, polarized, postags, polarized_tree = annotation\n",
    "    impgenerator = ImplicativeGenerator(len(original), db.implicative, polarized_tree)\n",
    "    impgenerator.search()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "paraphraseTokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-cased-finetuned-mrpc\",cache_dir ='../model/')\n",
    "paraphraseModel = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased-finetuned-mrpc\", cache_dir ='../model/',)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentenceTransformer = SentenceTransformer(\"roberta-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "def inference_mrpc(seq1s, seq2s):\n",
    "    for i in range(len(seq1s)):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1s[i], seq2s[i], return_tensors=\"pt\")\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        print(f\"{classes[1]}: {round(paraphrase_results[1] * 100)}%\")\n",
    "\n",
    "def inference_sts(sentences1, sentences2):\n",
    "    embeddings1 = sentenceTransformer.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = sentenceTransformer.encode(sentences2, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    for i in range(len(sentences1)):\n",
    "        print(\"Similarity Score: {:.4f}\".format(cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is paraphrase: 94%\n",
      "is paraphrase: 94%\n",
      "is paraphrase: 94%\n",
      "is paraphrase: 93%\n",
      "Score: 0.8999\n",
      "Score: 0.9313\n",
      "Score: 0.9685\n",
      "Score: 0.9409\n"
     ]
    }
   ],
   "source": [
    "seq0 = \"I caused him to submit his resignation\"\n",
    "seq1 = \"I caused the submission of his resignation\"\n",
    "seq2 = \"I caused him to submit the resignation\"\n",
    "seq3 = \"I caused the submission of his resignation\"\n",
    "seq4 = \"I caused the submission of the resignation\"\n",
    "seq5 = \"I caused the submission of his resignation\"\n",
    "seq6 = \"cut up an apple\"\n",
    "seq7 = \"cut an apple into piece\"\n",
    "\n",
    "sentences1 = [seq0, seq2, seq4, seq6]\n",
    "sentences2 = [seq1, seq3, seq5, seq7]\n",
    "\n",
    "inference_mrpc(sentences1, sentences2)\n",
    "inference_sts(sentences1, sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}