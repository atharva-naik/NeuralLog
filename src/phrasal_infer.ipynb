{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-03-15 18:21:05 INFO: Loading these models for language: en (English):\n",
      "========================================\n",
      "| Processor | Package                  |\n",
      "----------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt  |\n",
      "| pos       | ../model/en/pos/ewt.pt   |\n",
      "| lemma     | ../model/en/lemma/gum.pt |\n",
      "| depparse  | ../model/e...rse/gum.pt  |\n",
      "========================================\n",
      "\n",
      "2021-03-15 18:21:05 INFO: Use device: gpu\n",
      "2021-03-15 18:21:05 INFO: Loading: tokenize\n",
      "2021-03-15 18:21:06 INFO: Loading: pos\n",
      "2021-03-15 18:21:07 INFO: Loading: lemma\n",
      "2021-03-15 18:21:07 INFO: Loading: depparse\n",
      "2021-03-15 18:21:07 INFO: Done loading processors!\n",
      "2021-03-15 18:21:07 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt |\n",
      "=======================================\n",
      "\n",
      "2021-03-15 18:21:07 INFO: Use device: cpu\n",
      "2021-03-15 18:21:07 INFO: Loading: tokenize\n",
      "2021-03-15 18:21:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from Udep2Mono.util import btree2list\n",
    "from Udep2Mono.dependency_parse import tokenizer\n",
    "from Udep2Mono.dependency_parse import dependency_parse\n",
    "from Udep2Mono.binarization import BinaryDependencyTree\n",
    "from Udep2Mono.polarization import PolarizationPipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentenceTransformer = SentenceTransformer(\"roberta-large-nli-stsb-mean-tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw import TreeWidget\n",
    "from nltk.draw.util import CanvasFrame\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def jupyter_draw_nltk_tree(tree):\n",
    "    cf = CanvasFrame()\n",
    "    tc = TreeWidget(cf.canvas(), tree)\n",
    "    tc['node_font'] = 'arial 14 bold'\n",
    "    tc['leaf_font'] = 'arial 14'\n",
    "    tc['node_color'] = '#005990'\n",
    "    tc['leaf_color'] = '#3F8F57'\n",
    "    tc['line_color'] = '#175252'\n",
    "    cf.add_widget(tc, 20, 20)\n",
    "    os.system('rm -rf ../data/tree.png')\n",
    "    os.system('rm -rf ../data/tree.ps')\n",
    "    cf.print_to_file('../data/tree.ps')\n",
    "    cf.destroy()\n",
    "    os.system('convert ../data/tree.ps ../data/tree.png')\n",
    "    display(Image(filename='../data/tree.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_sts(seq1s, seq2s, dist=False):\n",
    "    embeddings1 = sentenceTransformer.encode(seq1s, convert_to_tensor=True)\n",
    "    embeddings2 = sentenceTransformer.encode(seq2s, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    distance = torch.dist(embeddings1, embeddings2)\n",
    "    if dist:\n",
    "        return distance\n",
    "    return cosine_scores[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-MRPC were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "roberta_MRPC = \"textattack/roberta-base-MRPC\"\n",
    "bert_MRPC = \"bert-base-cased-finetuned-mrpc\"\n",
    "\n",
    "paraphraseTokenizer = AutoTokenizer.from_pretrained(roberta_MRPC)  \n",
    "paraphraseModel = AutoModelForSequenceClassification.from_pretrained(roberta_MRPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunker import Chunker\n",
    "\n",
    "class SyntacticVariator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunker = Chunker()\n",
    "        self.paraphraseTokenizer = paraphraseTokenizer\n",
    "        self.paraphraseModel = paraphraseModel\n",
    "\n",
    "    def chunking(self, tree):\n",
    "        return self.chunker.get_chunks_byDepTree(tree)\n",
    "\n",
    "    def build_pairs(self, chunks1, chunks2):\n",
    "        chunk_pairs = []\n",
    "        for chunk1 in chunks1:\n",
    "            for chunk2 in chunks2:\n",
    "                if len(set(chunk1.split(' ')).intersection(chunk2.split(' '))) > 0:\n",
    "                     chunk_pairs.append((chunk1, chunk2))\n",
    "\n",
    "        return chunk_pairs\n",
    "\n",
    "    def inference_mrpc(self, seq1, seq2):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1, seq2, return_tensors=\"pt\")\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        return paraphrase_results[1]\n",
    "\n",
    "    def phrase_alignment(self, chunk_pairs):\n",
    "        alignments = []\n",
    "        for pair in chunk_pairs:\n",
    "            score = self.inference_mrpc(pair[0], pair[1])\n",
    "            if score > 0.85:\n",
    "                alignments.append(pair)\n",
    "\n",
    "        return alignments\n",
    "\n",
    "    def variate(self, P, H, p_tree, h_tree):\n",
    "        p_chunks = self.chunking(p_tree)\n",
    "        h_chunks = self.chunking(h_tree)\n",
    "\n",
    "        p_chunks.append(P)\n",
    "        h_chunks.append(H)\n",
    "\n",
    "        chunk_pairs = self.build_pairs(p_chunks, h_chunks)\n",
    "        alignments = self.phrase_alignment(chunk_pairs)\n",
    "\n",
    "        variates = set()\n",
    "        for align in alignments:\n",
    "            var_sentence = P.replace(align[0], align[1])\n",
    "            variates.add(var_sentence)\n",
    "\n",
    "        return variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A man is riding down the road tensor(0.4486)\n",
      "A man is A motorcyclist is riding a motorbike along a roadway down the road tensor(0.9334)\n",
      "A man is riding a motorbike a motorcycle down the road tensor(0.9282)\n",
      "A motorcyclist is riding a motorbike along a roadway tensor(1.0000)\n",
      "A man is riding a motorbike down the road tensor(0.9153)\n",
      "A man is riding a motorcycle down the road tensor(0.8933)\n"
     ]
    }
   ],
   "source": [
    "premise = \"A man is riding a motorcycle down the road\"\n",
    "hypothesis = \"A motorcyclist is riding a motorbike along a roadway\"\n",
    "\n",
    "pipeline = PolarizationPipeline()\n",
    "syntacticVariator = SyntacticVariator()\n",
    "\n",
    "h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "pipeline.modify_replacement(h_tree, replaced)\n",
    "annotation = pipeline.single_polarization(premise)\n",
    "\n",
    "variates = syntacticVariator.variate(premise, hypothesis, annotation['polarized_tree'],  h_tree)\n",
    "for v in variates:\n",
    "    similarity = inference_sts([v], [hypothesis])\n",
    "    print(v, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "who hitting a baseball tensor(0.5833)\n",
      "A child who hitting a baseball tensor(0.9777)\n",
      "A boy who is A child who is hitting a baseball tensor(0.9021)\n",
      "A boy who is hitting a baseball tensor(0.8627)\n",
      "hitting a baseball tensor(0.6114)\n",
      "A child who is hitting a baseball tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "premise = \"A boy who is hitting a baseball\"\n",
    "hypothesis = \"A child who is hitting a baseball\"\n",
    "\n",
    "pipeline = PolarizationPipeline()\n",
    "syntacticVariator = SyntacticVariator()\n",
    "\n",
    "h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "pipeline.modify_replacement(h_tree, replaced)\n",
    "annotation = pipeline.single_polarization(premise)\n",
    "\n",
    "variates = syntacticVariator.variate(premise, hypothesis, annotation['polarized_tree'],  h_tree)\n",
    "for v in variates:\n",
    "    similarity = inference_sts([v], [hypothesis])\n",
    "    print(v, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import pluralize, singularize\n",
    "from copy import copy\n",
    "import re\n",
    "import torch\n",
    "\n",
    "class PhrasalGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.annotated = None\n",
    "        self.original = None\n",
    "        self.kb = {}\n",
    "        self.hypothesis = \"\"\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.mod_at_left = [\n",
    "            \"advmod\", \"amod\", \"advmod:count\", \n",
    "            \"acl:relcl\", \"obl\", 'obl:npmod', \"det\",\n",
    "            \"obl:tmod\", \"nmod\", \"nmod:npmod\", \n",
    "            \"nmod:poss\", \"nmod:tmod\", \"obl:npmod\",\n",
    "            \"acl\", \"advcl\", \"xcomp\", \"ccomp\", \n",
    "            'compound:ptr']\n",
    "        self.mod_at_right = [\"obj\", \"appos\"]\n",
    "        self.mod_symmetric = [\"conj\", \"compound\"]\n",
    "        self.mod_special = [\"nsubj\"]\n",
    "        self.implicative = {\n",
    "            \"watching\": 1\n",
    "        }\n",
    "        \n",
    "        '''  \n",
    "            \"cop\": self.generate_inherite, \n",
    "            \"expl\": self.generate_expl,\n",
    "            \"nummod\": self.generate_nummod,\n",
    "        '''\n",
    "\n",
    "    def deptree_generate(self, tree, annotated, original):\n",
    "        self.stop_critarion = False\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.deptree = tree.copy()\n",
    "        self.original = original  \n",
    "        self.annotated = deepcopy(annotated)\n",
    "        self.sentence = original\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if self.stop_critarion:\n",
    "            return\n",
    "        if not tree.is_tree:\n",
    "            self.generate_default(tree)\n",
    "        else:\n",
    "            generation = self.get_generation_type(tree)\n",
    "            #print(generation, tree.val)\n",
    "            generation(tree)\n",
    "\n",
    "    def get_generation_type(self, tree):\n",
    "        if tree.val in self.mod_special:\n",
    "            return self.generate_special\n",
    "\n",
    "        disjunction = False\n",
    "        if tree.val == \"conj\":\n",
    "            disjunction |= self.search_dependency('or', tree.left)\n",
    "            disjunction |= self.search_dependency('and', tree.left)\n",
    "        \n",
    "        left_mod = tree.left.mark == \"+\"\n",
    "        left_mod = left_mod or tree.left.mark == \"=\" or disjunction\n",
    "        left_mod = left_mod and tree.val in self.mod_at_left\n",
    "\n",
    "        right_mod = tree.right.mark == \"+\" or tree.right.mark == \"=\" or disjunction \n",
    "        right_mod = right_mod and tree.val in self.mod_at_right\n",
    "\n",
    "        sym_mod = tree.val in self.mod_symmetric and tree.left.mark == \"+\" and tree.right.mark == \"+\"\n",
    "\n",
    "        if left_mod:\n",
    "            return self.left_modifier_generate\n",
    "        elif right_mod:\n",
    "            return self.right_modifier_generate\n",
    "        elif sym_mod:\n",
    "            return self.symmetric_generate\n",
    "        else:\n",
    "            return self.generate_default\n",
    "\n",
    "    def generate_special(self, tree):\n",
    "        if tree.val == \"nsubj\":\n",
    "            if tree.left.val == \"who\" and tree.right.val == \"aux\":\n",
    "                self.left_modifier_generate(tree)\n",
    "\n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "\n",
    "    def delete_cc(self, tree):\n",
    "        if tree.val == \"cc\" and tree.left.val != \"but\":\n",
    "            self.delete_modifier(tree, tree.right)\n",
    "\n",
    "        if tree.is_tree:\n",
    "            self.delete_cc(tree.left)\n",
    "            self.delete_cc(tree.right)\n",
    "\n",
    "    def delete_modifier(self, tree, modifier):\n",
    "        tree.val = modifier.val\n",
    "        tree.mark = modifier.mark\n",
    "        tree.pos = modifier.pos\n",
    "        tree.id = modifier.id\n",
    "        \n",
    "        tree.is_tree = modifier.is_tree\n",
    "        tree.is_root = modifier.is_root\n",
    "\n",
    "        tree.left = modifier.left\n",
    "        tree.right = modifier.right\n",
    "\n",
    "        self.delete_cc(tree)\n",
    "        self.save_tree()\n",
    "\n",
    "    def delete_left_modifier(self, tree):\n",
    "        #print(\"Delet: \", tree.left.val)\n",
    "        self.delete_modifier(tree, tree.right)\n",
    "\n",
    "    def delete_right_modifier(self, tree):\n",
    "        #print(\"Delet: \", tree.right.val)\n",
    "        self.delete_modifier(tree, tree.left)\n",
    "\n",
    "    def rollback(self, tree, backup):\n",
    "        tree.val = backup.val\n",
    "        tree.left = deepcopy(backup.left)\n",
    "        tree.right = deepcopy(backup.right)\n",
    "        tree.mark = backup.mark\n",
    "        tree.pos = backup.pos\n",
    "        tree.id = backup.id\n",
    "        tree.is_tree = backup.is_tree\n",
    "        tree.is_root = backup.is_root\n",
    "\n",
    "    def symmetric_generate(self, tree):\n",
    "        self.right_modifier_generate(tree)\n",
    "        self.left_modifier_generate(tree)\n",
    "        #self.delete_cc(tree)\n",
    "\n",
    "    def right_modifier_generate(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "\n",
    "        self.delete_right_modifier(tree)\n",
    "        self.save_tree()\n",
    "        self.rollback(tree, backup)    \n",
    "        \n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "\n",
    "    def left_modifier_generate(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "\n",
    "        self.delete_left_modifier(tree)\n",
    "        self.save_tree()\n",
    "        self.rollback(tree, backup)   \n",
    "\n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "    \n",
    "    def return_last_leaf(self, tree):\n",
    "        max_id = 0\n",
    "        max_id_l = 0\n",
    "        max_id_r = 0\n",
    "\n",
    "        if tree.id != None:\n",
    "            max_id = int(tree.id)\n",
    "    \n",
    "        if tree.left.is_tree:\n",
    "            max_id_l = self.return_last_leaf(tree.left)\n",
    "        else:\n",
    "            max_id_l = tree.left.id\n",
    "\n",
    "        if tree.right.is_tree:\n",
    "            max_id_r = self.return_last_leaf(tree.right)\n",
    "        else:\n",
    "            max_id_r = tree.right.id\n",
    "\n",
    "        return max(max_id, max(max_id_l, max_id_r))\n",
    "\n",
    "    def return_first_leaf(self, tree):\n",
    "        min_id = 100\n",
    "        min_id_l = 100\n",
    "        min_id_r = 100\n",
    "\n",
    "        if tree.id != None:\n",
    "            min_id = int(tree.id)\n",
    "    \n",
    "        if tree.left.is_tree:\n",
    "            min_id_l = self.return_last_leaf(tree.left)\n",
    "        else:\n",
    "            min_id_l = tree.left.id\n",
    "\n",
    "        if tree.right.is_tree:\n",
    "            min_id_r = self.return_last_leaf(tree.right)\n",
    "        else:\n",
    "            min_id_r = tree.right.id\n",
    "\n",
    "        return min(min_id, min(min_id_l, min_id_r))\n",
    "\n",
    "    def add_modifier_sent(self, tree, modifier, direct=0): \n",
    "        sentence = deepcopy(self.sentence)\n",
    "        if direct == 0:\n",
    "            last_leaf = self.return_first_leaf(tree)\n",
    "            sentence.insert(last_leaf-1, modifier)\n",
    "        elif direct == 1:\n",
    "            last_leaf = self.return_last_leaf(tree)\n",
    "            sentence.insert(last_leaf, modifier)        \n",
    "\n",
    "        self.remove_adjcent_duplicate(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        sentence = sentence.replace(\" 's\", \"'s\")\n",
    "\n",
    "        if abs(len(sentence) - len(self.hypothesis)) < 15:\n",
    "            re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', sentence, flags = re.I)\n",
    "            sentence = sentence.strip() \n",
    "            \n",
    "            if sentence.lower() == self.hypothesis.lower():\n",
    "                self.stop_critarion = True\n",
    "                self.sent_log.append((sentence, 1.0))\n",
    "                return\n",
    "                \n",
    "            similarity = inference_sts([sentence], [self.hypothesis])\n",
    "            if similarity > 0.90:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "            if similarity > 0.97:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "                self.stop_critarion = True\n",
    "\n",
    "    def add_modifier_lexical(self, tree, modifier, head, word_id, direct=0):\n",
    "        if direct == 0:\n",
    "            generated = ' '. join([modifier, head])\n",
    "        else:\n",
    "            generated = ' '. join([head, modifier])\n",
    "        \n",
    "        sentence = deepcopy(self.sentence)\n",
    "        diff = 0\n",
    "        if word_id > len(sentence):\n",
    "            diff = word_id - len(sentence)\n",
    "\n",
    "        goal = word_id-1-diff\n",
    "        sentence[goal] = \"DEL\"\n",
    "        sentence[goal:goal] = generated.split(' ')\n",
    "\n",
    "        if abs(len(sentence) - len(self.hypothesis.split(' '))) < 7:\n",
    "            self.remove_adjcent_duplicate(sentence)\n",
    "            sentence = ' '.join(sentence)\n",
    "            sentence = sentence.replace(\"DEL \", \"\")\n",
    "            sentence = sentence.replace(\"DEL\", \"\")\n",
    "            sentence = sentence.replace(\"-\", \" \")\n",
    "            sentence = sentence.replace(\" 's\", \"'s\")\n",
    "            re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', sentence, flags = re.I)\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence.lower() == self.hypothesis.lower():\n",
    "                self.stop_critarion = True\n",
    "                self.sent_log.append((sentence, 1.0))\n",
    "                return\n",
    "            \n",
    "            similarity = inference_sts([sentence], [self.hypothesis])\n",
    "            if similarity > 0.9:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "            if similarity > 0.97:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "                self.stop_critarion = True\n",
    "\n",
    "    def generate_default(self, tree):\n",
    "        VP_rel = {\n",
    "            \"aux\":1, \n",
    "            \"obj\":1, \n",
    "            \"obl\":1, \n",
    "            \"xcomp\":1, \n",
    "            \"ccomp\":1,\n",
    "            \"aux:pass\":1, \n",
    "            \"obl:tmod\":1, \n",
    "            \"obl:npmod\":1\n",
    "        }\n",
    "\n",
    "        VP_mod = {\n",
    "            \"advcl\":1, \n",
    "            \"xcomp\":1, \n",
    "            \"ccomp\":1,\n",
    "            \"obj\":1, \n",
    "            \"advmod\":1, \n",
    "            \"obl\":1, \n",
    "            \"obl:tmod\":1,\n",
    "            \"obl:nmod\":1, \n",
    "            \"parataxis\":1, \n",
    "            \"conj\":1\n",
    "        }\n",
    "\n",
    "        NP_rel = {\n",
    "            \"amod\":1,\n",
    "            \"compound\":1,\n",
    "            \"det\":1,\n",
    "            \"mark\":1,\n",
    "            \"nmod:poss\":1,\n",
    "            \"flat\":1,\n",
    "            \"acl:relcl\":1,\n",
    "            \"acl\":1,\n",
    "            \"nmod\":1\n",
    "        }\n",
    "\n",
    "        NP_mod = {\n",
    "            \"amod\":1,\n",
    "            \"compound\":1,\n",
    "            \"det\":1,\n",
    "            \"mark\":1,\n",
    "            \"nmod:poss\":1,\n",
    "            \"flat\":1,\n",
    "        }\n",
    "\n",
    "        if tree.pos is not None:\n",
    "            if (\"NN\" in tree.pos or \"JJ\" in tree.pos) and tree.mark == \"-\":\n",
    "                for rel in [\"amod\", \"compound\", \"det\", \"mark\", \"nmod:poss\", \"flat\", \"conj\", \"nummod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            if phrase['head'] == tree.val:\n",
    "                                self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id)\n",
    "                for rel in [\"amod\", \"acl:relcl\", \"compound\", \"acl\", \"nmod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            if phrase['head'] == tree.val:\n",
    "                                self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id, 1)\n",
    "                \n",
    "            elif \"VB\" in tree.pos and tree.mark == \"-\":\n",
    "                for rel in [\"advmod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id)\n",
    "                            self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id, 1)\n",
    "\n",
    "        elif VP_rel.get(tree.val, 0) and tree.mark == \"-\":\n",
    "            for rel in VP_mod:\n",
    "                if rel in self.kb:\n",
    "                    for phrase in self.kb[rel]:\n",
    "                        self.add_modifier_sent(tree, phrase['mod'], direct=1)\n",
    "\n",
    "        elif NP_rel.get(tree.val, 0) and tree.mark == \"-\":\n",
    "            for rel in NP_mod:\n",
    "                if rel in self.kb:\n",
    "                    for phrase in self.kb[rel]:\n",
    "                        self.add_modifier_sent(tree, phrase['mod'], direct=0)\n",
    "\n",
    "        elif VP_rel.get(tree.val, 0) and self.implicative.get(tree.right.val):\n",
    "            self.save_tree(tree=tree.left)\n",
    "        if tree.is_tree:\n",
    "            self.generate(tree.left)\n",
    "            self.generate(tree.right)  \n",
    "\n",
    "    def save_tree(self, tree=None):\n",
    "        if tree is None:\n",
    "            leaves = self.deptree.sorted_leaves().popkeys()\n",
    "            tree_copy = self.deptree.copy()\n",
    "        else:\n",
    "            leaves = tree.sorted_leaves().popkeys()\n",
    "            tree_copy = tree.copy()\n",
    "        \n",
    "        sentence = ' '.join([x[0] for x in leaves])\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        if sentence.lower() == self.hypothesis.lower():\n",
    "            self.tree_log = []\n",
    "            self.stop_critarion = True\n",
    "            self.tree_log.append((tree_copy, sentence, 1.0))\n",
    "            return\n",
    "        \n",
    "        similarity = inference_sts([sentence], [self.hypothesis])\n",
    "        #print(sentence, similarity)\n",
    "        if similarity > 0.8:\n",
    "            self.tree_log.append((tree_copy, sentence, similarity))\n",
    "        if similarity > 0.97:\n",
    "            self.tree_log = []\n",
    "            self.tree_log.append((tree_copy, sentence, similarity))\n",
    "            self.stop_critarion = True\n",
    "    \n",
    "    def remove_adjcent_duplicate(self, string):\n",
    "        to_remove = -1\n",
    "        for i in range(len(string)-1):\n",
    "            if string[i] == string[i+1]:\n",
    "                to_remove = i\n",
    "        if to_remove > -1:\n",
    "            del string[to_remove]\n",
    "\n",
    "    def search_dependency(self, deprel, tree):\n",
    "        if tree.val == deprel:\n",
    "            return True\n",
    "        else:\n",
    "            right = tree.right\n",
    "            left = tree.left\n",
    "\n",
    "            left_found = False\n",
    "            right_found = False\n",
    "\n",
    "            if right is not None:\n",
    "                right_found = self.search_dependency(deprel, right)\n",
    "\n",
    "            if left is not None:\n",
    "                left_found = self.search_dependency(deprel, left)\n",
    "\n",
    "            return left_found or right_found\n",
    "    \n",
    "    def Diff(self, li1, li2):\n",
    "        return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))    \n",
    "    \n",
    "    def preprocess(self, sentence):\n",
    "        preprocessed = sentence.replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "        preprocessed = preprocessed.replace(\"can't\", \"can not\")\n",
    "        preprocessed = preprocessed.replace(\"couldn't\", \"could not\")\n",
    "        preprocessed = preprocessed.replace(\"don't\", \"do not\")\n",
    "        preprocessed = preprocessed.replace(\"doesn't\", \"does not\")\n",
    "        preprocessed = preprocessed.replace(\"isn't\", \"is not\")\n",
    "        preprocessed = preprocessed.replace(\"won't\", \"will not\")\n",
    "        preprocessed = preprocessed.replace(\"wasn't\", \"was not\")\n",
    "        preprocessed = preprocessed.replace(\"weren't\", \"were not\")\n",
    "        preprocessed = preprocessed.replace(\"didn't\", \"did not\")\n",
    "        preprocessed = preprocessed.replace(\"aren't\", \"are not\")\n",
    "        preprocessed = preprocessed.replace(\"it's\", \"it is\")\n",
    "        preprocessed = preprocessed.replace(\"wouldn't\", \"would not\")\n",
    "        preprocessed = preprocessed.replace(\"There's\", \"There is\")\n",
    "        return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_relation = {\n",
    "    \"NN\": [\"amod\", \"nmod\", \"acl:relcl\", \"fixed\", \"compound\", \"det\", \"nmod:poss\", \"conj\", \"nummod\"],\n",
    "    \"VB\": [\"advmod\", \"acl\", \"obl\", \"xcomp\", \"advcl\", \"obl:tmod\", \"parataxis\", \"obj\",\"ccomp\"]\n",
    "}\n",
    "\n",
    "def down_right(tree):\n",
    "    if(tree.right == None):\n",
    "        return tree\n",
    "    return down_right(tree.right)\n",
    "\n",
    "def down_left(tree):\n",
    "    if(tree.left == None):\n",
    "        return tree\n",
    "    return down_left(tree.left)\n",
    "\n",
    "def collect_modifiers(tree, sent_set, mod_type=\"NN\"):\n",
    "    leaves = []\n",
    "    if tree.is_tree:\n",
    "        if tree.val in [\"mark\", \"case\", \"compound\", \"flat\", \"nmod\"]:\n",
    "            leaves.append(\n",
    "                (list(tree.right.sorted_leaves().popkeys()),\n",
    "                down_right(tree.left).val)\n",
    "            )\n",
    "        if tree.val in modifier_relation[mod_type]:\n",
    "            leaves.append(\n",
    "                (list(tree.left.sorted_leaves().popkeys()),\n",
    "                down_right(tree.right).val)\n",
    "            )\n",
    "\n",
    "        for leave in leaves:\n",
    "            if len(leave) > 0 and len(leave) < 10:\n",
    "                head = leave[1]\n",
    "                modifier = ' '.join([x[0] for x in leave[0]])\n",
    "                if tree.val in sent_set:\n",
    "                    sent_set[tree.val].append({'head': head,'mod': modifier})\n",
    "                else:\n",
    "                    sent_set[tree.val] = [{'head': head,'mod': modifier}]\n",
    "        \n",
    "        collect_modifiers(tree.left, sent_set, mod_type)\n",
    "        collect_modifiers(tree.right, sent_set, mod_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "458\n1738\n2677\n"
     ]
    }
   ],
   "source": [
    "MED_upward = []\n",
    "MED_upward_hypo = []\n",
    "MED_downward = []\n",
    "MED_downward_hypo = []\n",
    "MED_none = []\n",
    "MED_none_hypo = []\n",
    "\n",
    "with open(\"../data/MED/upward.txt\") as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_upward.append(lines[i*4+1])\n",
    "        MED_upward_hypo.append(lines[i*4+2])\n",
    "\n",
    "with open(\"../data/MED/downward.txt\") as donward_med:\n",
    "    lines = donward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_downward.append(lines[i*4+1])\n",
    "        MED_downward_hypo.append(lines[i*4+2])\n",
    "\n",
    "with open(\"../data/MED/neutral.txt\") as donward_med:\n",
    "    lines = donward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_none.append(lines[i*4+1])\n",
    "        MED_none_hypo.append(lines[i*4+2])\n",
    "\n",
    "print(len(MED_upward))\n",
    "print(len(MED_downward))\n",
    "print(len(MED_none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n====================================\n\nInit Premise: He never lacked paper money\n\nHypothesis: He never lacked money\n{   'advmod': [{'head': 'lacked', 'mod': 'never'}],\n    'obj': [{'head': 'lacked', 'mod': 'money'}]}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nFalse\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "up = [\"He never lacked paper money.\"]\n",
    "up_h = [\"He never lacked money.\"]\n",
    "\n",
    "annotations = []\n",
    "phrasalGenerator = PhrasalGenerator()\n",
    "pipeline = PolarizationPipeline(verbose=0)\n",
    "for i in range(len(up)):\n",
    "    premise = up[i]\n",
    "    hypothesis = up_h[i]\n",
    "    premise = phrasalGenerator.preprocess(premise)\n",
    "    hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "    tokenized = tokenizer(premise).sentences[0].words\n",
    "    tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "    print(\"\\n====================================\")\n",
    "    print(\"\\nInit Premise: \" + premise)\n",
    "    print(\"\\nHypothesis: \" + hypothesis)\n",
    "\n",
    "    h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "    h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "    pipeline.modify_replacement(h_tree, replaced)\n",
    "    phrases = {} \n",
    "    collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "    collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "    annotation = pipeline.single_polarization(premise)\n",
    "    \n",
    "    phrasalGenerator.kb = phrases\n",
    "    phrasalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "    pp.pprint(phrasalGenerator.kb)\n",
    "    \n",
    "    polarized = pipeline.postprocess(annotation['polarized_tree'], {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz) \n",
    "\n",
    "    polarized = pipeline.postprocess(h_tree, {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz)\n",
    "    \n",
    "    phrasalGenerator.deptree_generate(\n",
    "        annotation['polarized_tree'], \n",
    "        annotation['annotated'], tokens)\n",
    "\n",
    "    for gen_tree in phrasalGenerator.tree_log:\n",
    "        #leaves = gen_tree[0].sorted_leaves().popkeys()\n",
    "        #sentence = ' '.join([x[0] for x in leaves])\n",
    "        print((gen_tree[1], gen_tree[2]))\n",
    "\n",
    "    print(*phrasalGenerator.sent_log, sep=\"\\n\")\n",
    "    print(phrasalGenerator.stop_critarion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 500/500 [04:19<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "annotations = []\n",
    "with open(\"./generation_log_upward.txt\", 'w') as generate_log:\n",
    "    phrasalGenerator = PhrasalGenerator()\n",
    "    pipeline = PolarizationPipeline(verbose=0)\n",
    "    for i in tqdm(range(0, 500)):\n",
    "        premise = MED_none[i].replace('\\n', '')\n",
    "        hypothesis = MED_none_hypo[i].replace('\\n', '')\n",
    "        premise = phrasalGenerator.preprocess(premise)\n",
    "        hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "        tokenized = tokenizer(premise).sentences[0].words\n",
    "        tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "        try:\n",
    "            h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "            h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "        except:\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            continue\n",
    "        pipeline.modify_replacement(h_tree, replaced)\n",
    "        phrases = {} \n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "\n",
    "        try:\n",
    "            annotation = pipeline.single_polarization(premise)\n",
    "        except:\n",
    "            #generate_log.write(\"\\nPremise: \" + premise)\n",
    "            #generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            continue\n",
    "    \n",
    "        phrasalGenerator.kb = phrases\n",
    "        #print(phrasalGenerator.kb)\n",
    "        phrasalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "        \n",
    "        phrasalGenerator.deptree_generate(\n",
    "            annotation['polarized_tree'], \n",
    "            annotation['annotated'], \n",
    "            tokens)\n",
    "\n",
    "        # for gen_tree in phrasalGenerator.tree_log:\n",
    "        #    leaves = gen_tree[0].sorted_leaves().popkeys()\n",
    "        #    sentence = ' '.join([x[0] for x in leaves])\n",
    "        #    print((sentence, gen_tree[1]))\n",
    "            \n",
    "        if phrasalGenerator.stop_critarion:\n",
    "            generate_log.write(\"\\nID: \" + str(i))\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            #print(\"\\nPremise: \" + premise)\n",
    "            #print(\"\\nHypothesis: \" + hypothesis)\n",
    "            #print(*phrasalGenerator.sent_log, sep=\"\\n\")\n",
    "            #generate_log.writelines(phrasalGenerator.sent_log)\n",
    "            generate_log.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Some red flowers need light\", \n",
    "             \"Some red and beautiful flowers need light\",\n",
    "             \"All flowers need light and water\", \n",
    "             \"No flowers need bright or warm light\",\n",
    "             \"John can sing and dance\",\n",
    "             \"John ate an apple and finished his homework\",\n",
    "             \"John finished his homework and did not eat an apple\"]\n",
    "\n",
    "upward = [\"Some students sing to celebrate their graduation\",\n",
    "          \"An Irishman won the nobel prize for literature.\",\n",
    "          \"A big poison spider was spanning a web\", \n",
    "          \"A Californian special policeman pulled a car over and spoke to the driver\",\n",
    "          \"A woman is dancing in a cage\", \n",
    "          \"A woman is dancing beautifully in a cage\", \n",
    "          \"People are riding and paddling a raft\", \n",
    "          \"Some delegates finished the survey on time\"]\n",
    "\n",
    "sick_upward = [\"A brown dog is attacking another animal in front of the tall man in pants\",\n",
    "               \"A skilled person is riding a bicycle on one wheel\",\n",
    "               \"Two children are lying in the snow and are drawing angels\"]\n",
    "\n",
    "downward = [\"No spider was spanning a web\",\n",
    "            \"No student finished homework\",\n",
    "            \"I've never flown in an airplane\"]\n",
    "\n",
    "hypothesis = [\"No poison spider was spanning a web\", \n",
    "              \"No student at school finished homework compeletly\",\n",
    "              \"I've never flown in an airplane because i'm afraid.\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}