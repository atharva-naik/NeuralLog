{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from pqdict import pqdict\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "paraphraseTokenizer = AutoTokenizer.from_pretrained(\"textattack/roberta-base-MRPC\")  \n",
    "paraphraseModel = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-MRPC\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "def inference_mrpc(seq1s, seq2s):\n",
    "    for i in range(len(seq1s)):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1s[i], seq2s[i], return_tensors=\"pt\")\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        print(f\"{classes[1]}: {round(paraphrase_results[1] * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AStarSearch:\n",
    "    def __init__(self):    \n",
    "        self.closed_forward = set()                        \n",
    "        self.closed_backward = set()                     \n",
    "        self.entailments = set()\n",
    "        self.contradictions = set()\n",
    "        self.hypothesis = \"\"\n",
    "\n",
    "        model_name = \"roberta-large-nli-stsb-mean-tokens\"\n",
    "        self.sbert = SentenceTransformer(model_name)\n",
    "\n",
    "    def word_similarity(self, s1, s2):\n",
    "        num_sim = 0\n",
    "        seq1 = s1.split(\" \")\n",
    "        for w in seq1:\n",
    "            if w in s2:\n",
    "                num_sim += 1\n",
    "        return num_sim / len(seq1)\n",
    "\n",
    "    def inference_sts(self, seqs1, seqs2):\n",
    "        embeddings1 = self.sbert.encode(seqs1, convert_to_tensor=True)\n",
    "        embeddings2 = self.sbert.encode(seqs2, convert_to_tensor=True)\n",
    "        cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "        for i in range(len(seqs1)):\n",
    "            cost1 = cosine_scores[i][i]\n",
    "            cost2 = self.word_similarity(seqs1[i], seqs2[i])\n",
    "            cost = cost1 * cost2\n",
    "            return cost\n",
    "\n",
    "    def clear(self):\n",
    "        self.closedF.clear()\n",
    "        self.closedR.clear()\n",
    "\n",
    "    def generate_motion(self, open_set, side):\n",
    "        closed = self.closed_forward if side == 0 else self.closed_backward\n",
    "        opened = open_set[side]\n",
    "        self.generate_premises()\n",
    "        for premise in self.entailments:\n",
    "            if premise in closed:\n",
    "                continue\n",
    "            cost = self.inference_sts([premise], [self.hypothesis])\n",
    "            if premise not in opened:\n",
    "                opened[premise] = cost\n",
    "            elif cost > opened[premise]:\n",
    "                opened[premise] = cost\n",
    "\n",
    "    def query(self, premises, hypothesis):\n",
    "        self.clear()\n",
    "        self.hypothesis = hypothesis\n",
    "        open_lists = [pqdict({}), pqdict({})]\n",
    "        open_lists[0][premises] = self.inference_sts([premise], [hypothesis])\n",
    "        open_lists[0][hypothesis] = self.inference_sts([hypothesis], [hypothesis])\n",
    "\n",
    "        while open_lists[0] or open_lists[1]:\n",
    "            while open_lists[0]:\n",
    "                optimal = open_lists[0].pop()\n",
    "                break\n",
    "            self.generate_motion(open_list, 0)\n",
    "            if optimal in self.closed_backward:\n",
    "                break\n",
    "            self.closed_forward.add(optimal)\n",
    "\n",
    "            while open[1]:\n",
    "                optimal = open_lists[1].pop()\n",
    "                break\n",
    "            self.generate_motion(open_list, 0)\n",
    "            if optimal in self.closed_forward:\n",
    "                break\n",
    "            self.closed_backward.add(optimal)\n",
    "\n",
    "        self.closed_forward = self.closed_forward | self.closed_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}