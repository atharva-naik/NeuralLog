{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-21 22:07:44 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | gum       |\n",
      "| pos       | gum       |\n",
      "| lemma     | gum       |\n",
      "| depparse  | gum       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-12-21 22:07:44 INFO: Use device: gpu\n",
      "2020-12-21 22:07:44 INFO: Loading: tokenize\n",
      "2020-12-21 22:07:48 INFO: Loading: pos\n",
      "2020-12-21 22:07:50 INFO: Loading: lemma\n",
      "2020-12-21 22:07:50 INFO: Loading: depparse\n",
      "2020-12-21 22:07:52 INFO: Loading: ner\n",
      "2020-12-21 22:07:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from database import *\n",
    "from pattern.en import conjugate, lemma, lexeme, PAST, SG, PRESENT\n",
    "\n",
    "from Udep2Mono.binarization import BinaryDependencyTree\n",
    "from Udep2Mono import polarization\n",
    "from Udep2Mono.util import btreeToList\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImplicativeGenerator:\n",
    "    def __init__(self, length, kb, tree):\n",
    "        self.kb = kb\n",
    "        self.treeLog = []\n",
    "        self.polarLog = []\n",
    "        self.deptree = tree\n",
    "        self.length = length\n",
    "\n",
    "    def find_verbs(self, postags):\n",
    "        verbs = []\n",
    "        for word in postags:\n",
    "            if 'VB' in postags[word][1]:\n",
    "                verbs.append((word, postags[word][0]))\n",
    "        return verbs\n",
    "    \n",
    "    def fix_tense(self, verb, pos):\n",
    "        if pos == \"VBD\":\n",
    "            return conjugate(verb=verb, tense=PAST, person=1)\n",
    "        elif pos == \"VBZ\":\n",
    "            return conjugate(verb=verb, tense=PRESENT, person=3)\n",
    "        else:\n",
    "            return verb\n",
    "\n",
    "    def search(self):\n",
    "        #verbs = self.find_verbs(postags)\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    \n",
    "    def save_tree(self, tree=None):\n",
    "        if tree is not None:\n",
    "            generated, _, _, _ = btreeToList(tree, self.length, {}, 0)\n",
    "        else:\n",
    "            generated, _, _, _ = btreeToList(self.deptree, self.length, {}, 0)\n",
    "        generated = '[%s]' % ', '.join(map(str, generated)).replace(\"'\", \"\")\n",
    "        generated = generated.replace(\",\", \"\")\n",
    "        print(\"New tree: \", generated)\n",
    "\n",
    "        if tree is not None:\n",
    "            return deepcopy(self.deptree)\n",
    "        else:\n",
    "            return deepcopy(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if tree.val in [\"ccomp\", \"xcomp\"]:\n",
    "            backup = deepcopy(tree)\n",
    "\n",
    "            verb = conjugate(verb=tree.right.val, tense=PRESENT, person=1)\n",
    "            pos = tree.right.npos\n",
    "\n",
    "            impl_signs = self.kb.find({\"Verb\": verb})\n",
    "            if impl_signs:\n",
    "                sign = impl_signs[0]['Signature'].split('/')\n",
    "                if sign[0] == \"+\" and sign[1] == \"+\":\n",
    "                    self.treeLog.append(self.save_tree(tree.left.right))\n",
    "                elif sign[0] == \"+\" and sign[1] == \"-\":\n",
    "                    tree.val = tree.left.right.val\n",
    "                    tree.mark = tree.left.right.mark\n",
    "                    tree.id = tree.left.right.id\n",
    "                    tree.right = tree.left.right.right\n",
    "                    tree.right.val = self.fix_tense(tree.right.val, pos)\n",
    "                    tree.left = tree.left.right.left\n",
    "                    \n",
    "                    self.treeLog.append(self.save_tree())\n",
    "\n",
    "                    tree.val = backup.val\n",
    "                    tree.mark = backup.mark\n",
    "                    tree.id = backup.id\n",
    "                    tree.left = deepcopy(backup.left)\n",
    "                    tree.right = deepcopy(backup.right)\n",
    "        else:\n",
    "            if tree.left != \"N\":\n",
    "                self.generate(tree.left)\n",
    "\n",
    "            if tree.right != \"N\":\n",
    "                self.generate(tree.right)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.82it/s]\n",
      "New tree:  [nsubj↑ [PRP i↑] [obj↑ [nmod:poss↑ [PRP$ my↑] [NN homework↑]] [VB finished↑]]]\n",
      "New tree:  [nsubj↑ [det= [DT this=] [NN apple=]] [cop↑ [VBZ is↑] [JJ good↑]]]\n",
      "New tree:  [nsubj↑ [nmod:poss= [PRP$ my↑] [NN homework↑]] [cop↑ [VBZ is↑] [JJ hard↑]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from Udep2Mono.dependency_parse import dependencyParse\n",
    "#tree, postags, words = dependencyParse(\"All dogs eat food\", parser=\"stanza\")[0]\n",
    "\n",
    "sentences = [\"I managed to finish my homework\", \n",
    "             \"I recognized that this apple is good\",\n",
    "             \"I realized that my homework is hard\"]\n",
    "annotations, _ = polarization.run_polarize_pipeline(\n",
    "    sentences, verbose=2, parser=\"stanza\")\n",
    "print()\n",
    "for annotation in annotations:\n",
    "    annotated, original, polarized, postags, polarized_tree = annotation\n",
    "    impgenerator = ImplicativeGenerator(len(original), db.implicative, polarized_tree)\n",
    "    impgenerator.search()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dense, Activation, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "label_dict = {\n",
    "    \"+/+\": 1,\n",
    "    \"+/-\": 2,\n",
    "    \"+/o\": 3,\n",
    "    \"-/+\": 4,\n",
    "    \"-/-\": 5,\n",
    "    \"-/o\": 6,\n",
    "    \"o/+\": 7,\n",
    "    \"o/-\": 8,\n",
    "    \"o/o\": 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_verbs(filename, label_dir):\n",
    "    with open(filename, 'r') as f:\n",
    "        with open(label_dir, 'r') as l:\n",
    "            labels = []\n",
    "            verbs = []\n",
    "            for line in tqdm(f.readlines()):\n",
    "                verb_data = line.split()\n",
    "                verb = verb_data[0]\n",
    "                verbs.append(verb)\n",
    "\n",
    "            for line in tqdm(l.readlines()):\n",
    "                lb = line.strip('\\n')\n",
    "                labels.append(label_dict[lb])\n",
    "\n",
    "            #self.chars = self.flatten_list(list(map(lambda x: list(x), verbs)))\n",
    "            #self.chars_size = len(self.chars)\n",
    "            #self.vocab = sorted(list(set(self.chars)))\n",
    "            #self.vocab_size = len(self.vocab)\n",
    "            return verbs, labels\n",
    "        \n",
    "data_dir = '../data/VERB/verb.txt'\n",
    "label_dir = '../data/VERB/signature.txt'\n",
    "\n",
    "verbs, labels = read_verbs(data_dir, label_dir)\n",
    "vocab = set(' '.join([str(i) for i in verbs]))\n",
    "vocab.add('END')\n",
    "len_vocab = len(vocab)\n",
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "maxlen=20\n",
    "\n",
    "signature = [\n",
    "     [\"+/+\", 1],\n",
    "     [\"+/-\", 2],\n",
    "     [\"+/o\", 3],\n",
    "     [\"-/+\", 4],\n",
    "     [\"-/-\", 5],\n",
    "     [\"-/o\", 6],\n",
    "     [\"o/+\", 7],\n",
    "     [\"o/-\", 8],\n",
    "     [\"o/o\", 9]]\n",
    "\n",
    "# Builds an empty line with a 1 at the index of character\n",
    "def set_flag(i):\n",
    "    tmp = np.zeros(len_vocab);\n",
    "    tmp[i] = 1\n",
    "    return list(tmp)\n",
    "\n",
    "# Truncate names and create the matrix\n",
    "def prepare_X(X):\n",
    "    new_list = []\n",
    "    trunc_train_name = [str(i)[0:maxlen] for i in X]\n",
    "\n",
    "    for i in trunc_train_name:\n",
    "        tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "        for k in range(0,maxlen - len(str(i))):\n",
    "            tmp.append(set_flag(char_index[\"END\"]))\n",
    "        new_list.append(tmp)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "def prepare_y(y):\n",
    "    one_hot = []\n",
    "    for idx in labels:\n",
    "        init = np.zeros(9)\n",
    "        init[idx-1] = 1\n",
    "        one_hot.append(init)\n",
    "    return one_hot\n",
    "\n",
    "X = np.array(prepare_X(verbs))\n",
    "y = np.array(prepare_y(labels))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(512, return_sequences=True), backward_layer=LSTM(512, return_sequences=True, go_backwards=True), input_shape=(maxlen,len_vocab)))\n",
    "model.add(Bidirectional(LSTM(512)))\n",
    "model.add(Dense(100, activity_regularizer=l2(0.002)))\n",
    "model.add(Dense(9, activity_regularizer=l2(0.002)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='accuracy', patience=5)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "reduce_lr_acc = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_delta=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 65ms/step - loss: 1.0761 - accuracy: 0.6532 - val_loss: 1.0760 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.65323, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 1.0760 - accuracy: 0.6532 - val_loss: 1.0760 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00002: accuracy did not improve from 0.65323\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 1.0760 - accuracy: 0.6532 - val_loss: 1.0760 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00003: accuracy did not improve from 0.65323\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 1.0760 - accuracy: 0.6532 - val_loss: 1.0760 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00004: accuracy did not improve from 0.65323\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 1.0760 - accuracy: 0.6532 - val_loss: 1.0760 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00005: accuracy did not improve from 0.65323\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 1.0760 - accuracy: 0.6532 - val_loss: 1.0760 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00006: accuracy did not improve from 0.65323\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "history = model.fit(\n",
    "    X, y, \n",
    "    batch_size=batch_size, \n",
    "    epochs=50, verbose=1, \n",
    "    validation_data =(X, y), \n",
    "    callbacks=[callback, mc, reduce_lr_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = [\"manage\", \"get\", \"forget\"]\n",
    "X_pred = prepare_X([e for e in new_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 2s - loss: 3.6082 - accuracy: 0.3548\n",
      "Trained model, accuracy: 35.48%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('../model/implicative.h5')\n",
    "#model.load_weights(\"../model/implicative.ckpt\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "loss, acc = model.evaluate(X, y, verbose=2)\n",
    "print(\"Trained model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 1s - loss: 3.6082 - accuracy: 0.3548\n",
      "Trained model, accuracy: 35.48%\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.load_model('../model/implicative.h5')\n",
    "loss, acc = model1.evaluate(X, y, verbose=2)\n",
    "print(\"Trained model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(new_names, prediction):\n",
    "    return_results = []\n",
    "    k = 0\n",
    "    for i in prediction:\n",
    "        if max(i) < 0.65:\n",
    "            return_results.append([new_names[k], \"N\"])\n",
    "        else:\n",
    "            return_results.append([new_names[k], signature[np.argmax(i)]])\n",
    "        k += 1\n",
    "    return return_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 1s - loss: 2.9703 - accuracy: 0.4919\n",
      "Untrained model, accuracy: 49.19%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X, y, verbose=2)\n",
    "print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['manage', ['o/o', 9]], ['get', ['o/o', 9]], ['forget', ['o/o', 9]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_pred)\n",
    "pred(new_names, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/VERB/verb.txt', 'w') as data:\n",
    "    with open('../data/VERB/signature.txt', 'w') as label:\n",
    "        verbs = set()\n",
    "        for index, row in df.iterrows():\n",
    "            verb = row['verb'] + '\\n'\n",
    "            if verb not in verbs:\n",
    "                verbs.add(verb)\n",
    "                data.write(verb)\n",
    "                label.write(row['signature'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
