{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-21 22:07:44 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | gum       |\n",
      "| pos       | gum       |\n",
      "| lemma     | gum       |\n",
      "| depparse  | gum       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-12-21 22:07:44 INFO: Use device: gpu\n",
      "2020-12-21 22:07:44 INFO: Loading: tokenize\n",
      "2020-12-21 22:07:48 INFO: Loading: pos\n",
      "2020-12-21 22:07:50 INFO: Loading: lemma\n",
      "2020-12-21 22:07:50 INFO: Loading: depparse\n",
      "2020-12-21 22:07:52 INFO: Loading: ner\n",
      "2020-12-21 22:07:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from database import *\n",
    "from pattern.en import conjugate, lemma, lexeme, PAST, SG, PRESENT\n",
    "\n",
    "from Udep2Mono.binarization import BinaryDependencyTree\n",
    "from Udep2Mono import polarization\n",
    "from Udep2Mono.util import btreeToList\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImplicativeGenerator:\n",
    "    def __init__(self, length, kb, tree):\n",
    "        self.kb = kb\n",
    "        self.treeLog = []\n",
    "        self.polarLog = []\n",
    "        self.deptree = tree\n",
    "        self.length = length\n",
    "\n",
    "    def find_verbs(self, postags):\n",
    "        verbs = []\n",
    "        for word in postags:\n",
    "            if 'VB' in postags[word][1]:\n",
    "                verbs.append((word, postags[word][0]))\n",
    "        return verbs\n",
    "    \n",
    "    def fix_tense(self, verb, pos):\n",
    "        if pos == \"VBD\":\n",
    "            return conjugate(verb=verb, tense=PAST, person=1)\n",
    "        elif pos == \"VBZ\":\n",
    "            return conjugate(verb=verb, tense=PRESENT, person=3)\n",
    "        else:\n",
    "            return verb\n",
    "\n",
    "    def search(self):\n",
    "        #verbs = self.find_verbs(postags)\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    \n",
    "    def save_tree(self, tree=None):\n",
    "        if tree is not None:\n",
    "            generated, _, _, _ = btreeToList(tree, self.length, {}, 0)\n",
    "        else:\n",
    "            generated, _, _, _ = btreeToList(self.deptree, self.length, {}, 0)\n",
    "        generated = '[%s]' % ', '.join(map(str, generated)).replace(\"'\", \"\")\n",
    "        generated = generated.replace(\",\", \"\")\n",
    "        print(\"New tree: \", generated)\n",
    "\n",
    "        if tree is not None:\n",
    "            return deepcopy(self.deptree)\n",
    "        else:\n",
    "            return deepcopy(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if tree.val in [\"ccomp\", \"xcomp\"]:\n",
    "            backup = deepcopy(tree)\n",
    "\n",
    "            verb = conjugate(verb=tree.right.val, tense=PRESENT, person=1)\n",
    "            pos = tree.right.npos\n",
    "\n",
    "            impl_signs = self.kb.find({\"Verb\": verb})\n",
    "            if impl_signs:\n",
    "                sign = impl_signs[0]['Signature'].split('/')\n",
    "                if sign[0] == \"+\" and sign[1] == \"+\":\n",
    "                    self.treeLog.append(self.save_tree(tree.left.right))\n",
    "                elif sign[0] == \"+\" and sign[1] == \"-\":\n",
    "                    tree.val = tree.left.right.val\n",
    "                    tree.mark = tree.left.right.mark\n",
    "                    tree.id = tree.left.right.id\n",
    "                    tree.right = tree.left.right.right\n",
    "                    tree.right.val = self.fix_tense(tree.right.val, pos)\n",
    "                    tree.left = tree.left.right.left\n",
    "                    \n",
    "                    self.treeLog.append(self.save_tree())\n",
    "\n",
    "                    tree.val = backup.val\n",
    "                    tree.mark = backup.mark\n",
    "                    tree.id = backup.id\n",
    "                    tree.left = deepcopy(backup.left)\n",
    "                    tree.right = deepcopy(backup.right)\n",
    "        else:\n",
    "            if tree.left != \"N\":\n",
    "                self.generate(tree.left)\n",
    "\n",
    "            if tree.right != \"N\":\n",
    "                self.generate(tree.right)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.82it/s]\n",
      "New tree:  [nsubj↑ [PRP i↑] [obj↑ [nmod:poss↑ [PRP$ my↑] [NN homework↑]] [VB finished↑]]]\n",
      "New tree:  [nsubj↑ [det= [DT this=] [NN apple=]] [cop↑ [VBZ is↑] [JJ good↑]]]\n",
      "New tree:  [nsubj↑ [nmod:poss= [PRP$ my↑] [NN homework↑]] [cop↑ [VBZ is↑] [JJ hard↑]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from Udep2Mono.dependency_parse import dependencyParse\n",
    "#tree, postags, words = dependencyParse(\"All dogs eat food\", parser=\"stanza\")[0]\n",
    "\n",
    "sentences = [\"I managed to finish my homework\", \n",
    "             \"I recognized that this apple is good\",\n",
    "             \"I realized that my homework is hard\"]\n",
    "annotations, _ = polarization.run_polarize_pipeline(\n",
    "    sentences, verbose=2, parser=\"stanza\")\n",
    "print()\n",
    "for annotation in annotations:\n",
    "    annotated, original, polarized, postags, polarized_tree = annotation\n",
    "    impgenerator = ImplicativeGenerator(len(original), db.implicative, polarized_tree)\n",
    "    impgenerator.search()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dense, Activation, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import tensorflow as tf\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "\n",
    "label_dict = {\n",
    "    \"+/+\": 1,\n",
    "    \"+/-\": 2,\n",
    "    \"+/o\": 3,\n",
    "    \"-/+\": 4,\n",
    "    \"-/-\": 5,\n",
    "    \"-/o\": 6,\n",
    "    \"o/+\": 7,\n",
    "    \"o/-\": 8,\n",
    "    \"o/o\": 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:00<00:00, 319467.87it/s]\n",
      "100%|██████████| 124/124 [00:00<00:00, 386974.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_verbs(filename, label_dir):\n",
    "    with open(filename, 'r') as f:\n",
    "        with open(label_dir, 'r') as l:\n",
    "            labels = []\n",
    "            verbs = []\n",
    "            for line in tqdm(f.readlines()):\n",
    "                verb_data = line.split()\n",
    "                verb = verb_data[0]\n",
    "                verbs.append(verb)\n",
    "\n",
    "            for line in tqdm(l.readlines()):\n",
    "                lb = line.strip('\\n')\n",
    "                labels.append(label_dict[lb])\n",
    "\n",
    "            #self.chars = self.flatten_list(list(map(lambda x: list(x), verbs)))\n",
    "            #self.chars_size = len(self.chars)\n",
    "            #self.vocab = sorted(list(set(self.chars)))\n",
    "            #self.vocab_size = len(self.vocab)\n",
    "            return verbs, labels\n",
    "        \n",
    "data_dir = '../data/VERB/verb.txt'\n",
    "label_dir = '../data/VERB/signature.txt'\n",
    "\n",
    "verbs, labels = read_verbs(data_dir, label_dir)\n",
    "vocab = set(' '.join([str(i) for i in verbs]))\n",
    "vocab.add('END')\n",
    "len_vocab = len(vocab)\n",
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "maxlen=20\n",
    "\n",
    "signature = [\n",
    "     [\"+/+\", 1],\n",
    "     [\"+/-\", 2],\n",
    "     [\"+/o\", 3],\n",
    "     [\"-/+\", 4],\n",
    "     [\"-/-\", 5],\n",
    "     [\"-/o\", 6],\n",
    "     [\"o/+\", 7],\n",
    "     [\"o/-\", 8],\n",
    "     [\"o/o\", 9]]\n",
    "\n",
    "# Builds an empty line with a 1 at the index of character\n",
    "def set_flag(i):\n",
    "    tmp = np.zeros(len_vocab);\n",
    "    tmp[i] = 1\n",
    "    return list(tmp)\n",
    "\n",
    "# Truncate names and create the matrix\n",
    "def prepare_X(X):\n",
    "    new_list = []\n",
    "    trunc_train_name = [str(i)[0:maxlen] for i in X]\n",
    "\n",
    "    for i in trunc_train_name:\n",
    "        tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "        for k in range(0,maxlen - len(str(i))):\n",
    "            tmp.append(set_flag(char_index[\"END\"]))\n",
    "        new_list.append(tmp)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "def prepare_y(y):\n",
    "    one_hot = []\n",
    "    for idx in labels:\n",
    "        init = np.zeros(9)\n",
    "        init[idx-1] = 1\n",
    "        one_hot.append(init)\n",
    "    return one_hot\n",
    "\n",
    "X = np.array(prepare_X(verbs))\n",
    "y = np.array(prepare_y(labels))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(512, return_sequences=True), backward_layer=LSTM(512, return_sequences=True, go_backwards=True), input_shape=(maxlen,len_vocab)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(512)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activity_regularizer=l2(0.002)))\n",
    "model.add(Dense(9, activity_regularizer=l2(0.002)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='accuracy', patience=5)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "reduce_lr_acc = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_delta=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 14s 150ms/step - loss: 1.6114 - accuracy: 0.6117 - val_loss: 1.3143 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.64516, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 1.3301 - accuracy: 0.6530 - val_loss: 1.2673 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00002: accuracy improved from 0.64516 to 0.65323, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 3s 110ms/step - loss: 1.2833 - accuracy: 0.6485 - val_loss: 1.1708 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00003: accuracy did not improve from 0.65323\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 3s 109ms/step - loss: 1.1186 - accuracy: 0.6737 - val_loss: 1.1395 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00004: accuracy did not improve from 0.65323\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 3s 108ms/step - loss: 1.1302 - accuracy: 0.6909 - val_loss: 1.0192 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00005: accuracy improved from 0.65323 to 0.66129, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 1.0093 - accuracy: 0.7200 - val_loss: 1.0683 - val_accuracy: 0.6613\n",
      "\n",
      "Epoch 00006: accuracy did not improve from 0.66129\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 1.0185 - accuracy: 0.6780 - val_loss: 0.8904 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00007: accuracy improved from 0.66129 to 0.70968, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 1.0321 - accuracy: 0.6460 - val_loss: 0.8804 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00008: accuracy did not improve from 0.70968\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 1.0581 - accuracy: 0.6598 - val_loss: 0.7736 - val_accuracy: 0.7339\n",
      "\n",
      "Epoch 00009: accuracy did not improve from 0.70968\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.7402 - accuracy: 0.7915 - val_loss: 0.7559 - val_accuracy: 0.8065\n",
      "\n",
      "Epoch 00010: accuracy improved from 0.70968 to 0.72581, saving model to best_model.h5\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.7335 - accuracy: 0.8368 - val_loss: 0.7510 - val_accuracy: 0.7984\n",
      "\n",
      "Epoch 00011: accuracy improved from 0.72581 to 0.77419, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 3s 108ms/step - loss: 0.8016 - accuracy: 0.7391 - val_loss: 0.5780 - val_accuracy: 0.7903\n",
      "\n",
      "Epoch 00012: accuracy did not improve from 0.77419\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.6361 - accuracy: 0.7826 - val_loss: 0.5167 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00013: accuracy did not improve from 0.77419\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 3s 104ms/step - loss: 0.5366 - accuracy: 0.8624 - val_loss: 0.4896 - val_accuracy: 0.8871\n",
      "\n",
      "Epoch 00014: accuracy improved from 0.77419 to 0.78226, saving model to best_model.h5\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 3s 105ms/step - loss: 0.4803 - accuracy: 0.8630 - val_loss: 0.3961 - val_accuracy: 0.9274\n",
      "\n",
      "Epoch 00015: accuracy improved from 0.78226 to 0.84677, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 3s 105ms/step - loss: 0.3462 - accuracy: 0.9378 - val_loss: 0.3478 - val_accuracy: 0.9516\n",
      "\n",
      "Epoch 00016: accuracy improved from 0.84677 to 0.88710, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.3408 - accuracy: 0.9632 - val_loss: 0.3177 - val_accuracy: 0.9597\n",
      "\n",
      "Epoch 00017: accuracy improved from 0.88710 to 0.91935, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.4123 - accuracy: 0.8940 - val_loss: 0.2878 - val_accuracy: 0.9516\n",
      "\n",
      "Epoch 00018: accuracy did not improve from 0.91935\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 0.3497 - accuracy: 0.9406 - val_loss: 0.2320 - val_accuracy: 0.9839\n",
      "\n",
      "Epoch 00019: accuracy improved from 0.91935 to 0.95968, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 0.2276 - accuracy: 0.9770 - val_loss: 0.2644 - val_accuracy: 0.9677\n",
      "\n",
      "Epoch 00020: accuracy improved from 0.95968 to 0.96774, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.3482 - accuracy: 0.9437 - val_loss: 0.2842 - val_accuracy: 0.9516\n",
      "\n",
      "Epoch 00021: accuracy did not improve from 0.96774\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 3s 103ms/step - loss: 0.2977 - accuracy: 0.9448 - val_loss: 0.3698 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00022: accuracy did not improve from 0.96774\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 3s 105ms/step - loss: 0.4660 - accuracy: 0.9660 - val_loss: 0.2107 - val_accuracy: 0.9839\n",
      "\n",
      "Epoch 00023: accuracy improved from 0.96774 to 0.97581, saving model to best_model.h5\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.1854 - accuracy: 0.9890 - val_loss: 0.1947 - val_accuracy: 0.9839\n",
      "\n",
      "Epoch 00024: accuracy did not improve from 0.97581\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 3s 105ms/step - loss: 0.1891 - accuracy: 0.9837 - val_loss: 0.1806 - val_accuracy: 0.9839\n",
      "\n",
      "Epoch 00025: accuracy improved from 0.97581 to 0.98387, saving model to best_model.h5\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 0.1789 - accuracy: 0.9775 - val_loss: 0.1747 - val_accuracy: 0.9919\n",
      "\n",
      "Epoch 00026: accuracy did not improve from 0.98387\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 0.1624 - accuracy: 0.9919 - val_loss: 0.1714 - val_accuracy: 0.9839\n",
      "\n",
      "Epoch 00027: accuracy improved from 0.98387 to 0.99194, saving model to best_model.h5\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 3s 105ms/step - loss: 0.1931 - accuracy: 0.9846 - val_loss: 0.1652 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: accuracy did not improve from 0.99194\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 3s 108ms/step - loss: 0.1681 - accuracy: 0.9968 - val_loss: 0.1611 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00029: accuracy did not improve from 0.99194\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.1571 - accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: accuracy improved from 0.99194 to 1.00000, saving model to best_model.h5\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.1621 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: accuracy did not improve from 1.00000\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.1578 - accuracy: 1.0000 - val_loss: 0.1513 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: accuracy did not improve from 1.00000\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.1509 - accuracy: 1.0000 - val_loss: 0.1489 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: accuracy did not improve from 1.00000\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 0.1488 - accuracy: 1.0000 - val_loss: 0.1467 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: accuracy did not improve from 1.00000\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 0.1493 - accuracy: 1.0000 - val_loss: 0.1443 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: accuracy did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "history = model.fit(\n",
    "    X, y, \n",
    "    batch_size=batch_size, \n",
    "    epochs=50, verbose=1, \n",
    "    validation_data =(X, y), \n",
    "    callbacks=[callback, mc, reduce_lr_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = [\"manage\", \"get\", \"forget\"]\n",
    "X_pred = prepare_X([e for e in new_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(new_names, prediction):\n",
    "    return_results = []\n",
    "    k = 0\n",
    "    for i in prediction:\n",
    "        #if max(i) < 0.65:\n",
    "        #    return_results.append([new_names[k], \"N\"])\n",
    "        #else:\n",
    "        return_results.append([new_names[k], signature[np.argmax(i)]])\n",
    "        k += 1\n",
    "    return return_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['manage', ['+/-', 2]], ['get', ['+/-', 2]], ['forget', ['-/+', 4]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_pred)\n",
    "pred(new_names, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/VERB/verb.txt', 'w') as data:\n",
    "    with open('../data/VERB/signature.txt', 'w') as label:\n",
    "        verbs = set()\n",
    "        for index, row in df.iterrows():\n",
    "            verb = row['verb'] + '\\n'\n",
    "            if verb not in verbs:\n",
    "                verbs.add(verb)\n",
    "                data.write(verb)\n",
    "                label.write(row['signature'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
